{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34be6d2-b1bb-4630-bc87-ca80f7da298c",
   "metadata": {},
   "source": [
    "# Compiling SBR Wind Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ec6ad-cbf8-45b3-a300-b21e987b9aa5",
   "metadata": {},
   "source": [
    "We want to compile all the wind data released from the SBR phases (currently pahse 0 and phase 1) and use that to generate Gaussian plumes.  Since wind data is only released around over passes and we want to be able to randomly sample a 5-10min contiguous duration (i.e. no big gap between data points), we keep the wind data for each overpass in a separate file.\n",
    "\n",
    "Here, we want to extract the wind datasets for an overpass and the relevant columns.  Since the data is stored in a GDrive, the simplest way of extracting the data is to download the whole folders into `orbio/methane-cv/notebooks/data/sbr_drives` and glob for the wind files.  We then load the relevant columns (wind vector components and direction) and save into new files named `<datetime of first reading>)_wind.parquet`\n",
    "\n",
    "TODO we need someway to group the continguous wind data into time ordered groups.  We can have multiple overpasses in a single day and even at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9ecf2-1a75-41b3-af33-4d5d104228a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory, TemporaryFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.azure_wrap.blob_storage_sdk_v2 import upload_dir\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f9cd8-a523-4974-bdfd-481ecaaade48",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_files = list(Path(\"./data/sbr_data\").glob(\"**/*wind+meteorological_data.csv\"))\n",
    "len(wind_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639957f9-e4c1-445f-a88c-366f0710130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_speed(u: float, v: float) -> float:\n",
    "    \"\"\"Calculate wind speed from u and v components.\"\"\"\n",
    "    return np.sqrt(np.square(u) + np.square(v))\n",
    "\n",
    "\n",
    "def wind_direction(u: float, v: float) -> float:\n",
    "    \"\"\"Calculate wind direction from u and v components.\"\"\"\n",
    "    return np.degrees(np.arctan2(u, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7bcb9-dfee-4e14-b227-a51cfc6ebe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_dir = Path(\"data/\")\n",
    "wind_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "wind_dfs = []\n",
    "for file in wind_files:\n",
    "    satellite = Path(file).parent.stem.split(\"_\")[-1]\n",
    "    df = (\n",
    "        pd.read_csv(file)\n",
    "        .assign(satellite=satellite)\n",
    "        .assign(timestamp=lambda df: pd.to_datetime(df[\"TIMESTAMP UTC\"]))\n",
    "        .assign(date=lambda df: df.timestamp.dt.date)\n",
    "        .assign(velocity_x=lambda df: df[\"Ux_Avg_10meter(m/s)\"])\n",
    "        .assign(velocity_y=lambda df: df[\"Uy_Avg_10meter(m/s)\"])\n",
    "        .assign(direction_deg=lambda df: df[\"WndDir_10meter(degrees)\"] - 180)\n",
    "        .dropna()  # some overpasses have no data but the file exists\n",
    "        .assign(\n",
    "            speed=lambda df: df.apply(lambda row: wind_speed(row.velocity_x, row.velocity_y), axis=1),\n",
    "            calculated_direction_deg=lambda df: df.apply(\n",
    "                lambda row: wind_direction(row.velocity_x, row.velocity_y), axis=1\n",
    "            ),\n",
    "        )\n",
    "    )[\n",
    "        [\n",
    "            \"satellite\",\n",
    "            \"date\",\n",
    "            \"timestamp\",\n",
    "            \"velocity_x\",\n",
    "            \"velocity_y\",\n",
    "            \"direction_deg\",\n",
    "            \"speed\",\n",
    "            \"calculated_direction_deg\",\n",
    "        ]\n",
    "    ]\n",
    "    wind_dfs.append(df)\n",
    "    # break\n",
    "\n",
    "wind_df = pd.concat(wind_dfs)\n",
    "wind_df.to_parquet(\"data/sbr_wind.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e02db2-6edf-44dd-9805-62ac44339179",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7515eff-b296-41f9-be44-a52f878cf4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wind_df.sort_values([\"date\", \"satellite\"], ascending=True).groupby([\"date\", \"satellite\"]).agg(\n",
    "    num_readings=(\"timestamp\", \"count\"),\n",
    "    speed_max=(\"speed\", \"max\"),\n",
    "    speed_min=(\"speed\", \"min\"),\n",
    "    speed_mean=(\"speed\", \"mean\"),\n",
    "    speed_std=(\"speed\", \"std\"),\n",
    "    direction_mean=(\"calculated_direction_deg\", \"mean\"),\n",
    "    direction_std=(\"calculated_direction_deg\", \"std\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c08471-f085-4626-bc29-b8326703bb7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wind_df[(wind_df[\"date\"] == datetime.strptime(\"2024-11-24\", \"%Y-%m-%d\").date()) & (wind_df[\"satellite\"] == \"LS9\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc95892-b2b1-445f-b076-ee15efabbcf1",
   "metadata": {},
   "source": [
    "## Wind Speed and Direction Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208742e-7540-4ebe-b786-c5a0b3525c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 2 * np.pi / 360\n",
    "plot_df = (\n",
    "    wind_df.calculated_direction_deg.round(0)\n",
    "    .value_counts()\n",
    "    .rename_axis(\"direction\")\n",
    "    .rename(\"counts\")\n",
    "    .reset_index()\n",
    "    .assign(counts=lambda df: df.counts / df.counts.sum())\n",
    ")\n",
    "\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "plt.figure(figsize=(15, 4))\n",
    "ax1 = plt.subplot(131)\n",
    "ax2 = plt.subplot(132, projection=\"polar\")\n",
    "ax3 = plt.subplot(133, projection=\"polar\")\n",
    "\n",
    "# wind speed\n",
    "ax1.hist(wind_df.speed, bins=360)\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_title(\"Wind Speed Density\")\n",
    "ax1.set_xlabel(\"meters per second\", fontsize=10)\n",
    "ax1.set_ylabel(\"\", fontsize=14)\n",
    "\n",
    "# reported wind direction\n",
    "plot_df = (\n",
    "    wind_df.calculated_direction_deg.round(0)\n",
    "    .value_counts()\n",
    "    .rename_axis(\"direction\")\n",
    "    .rename(\"counts\")\n",
    "    .reset_index()\n",
    "    .assign(counts=lambda df: df.counts / df.counts.sum())\n",
    ")\n",
    "ax2.set_theta_zero_location(\"N\")\n",
    "ax2.set_theta_direction(-1)\n",
    "ax2.bar(plot_df.direction, plot_df.counts, bottom=0.0, width=width)  # color=colors, width=width)\n",
    "ax2.set_yticklabels([])\n",
    "ax2.set_title(\"Calculated Wind Direction Density\")\n",
    "\n",
    "# calculated wind direction\n",
    "plot_df = (\n",
    "    wind_df.direction_deg.round(0)\n",
    "    .value_counts()\n",
    "    .rename_axis(\"direction\")\n",
    "    .rename(\"counts\")\n",
    "    .reset_index()\n",
    "    .assign(counts=lambda df: df.counts / df.counts.sum())\n",
    ")\n",
    "ax3.set_theta_zero_location(\"N\")\n",
    "ax3.set_theta_direction(-1)\n",
    "ax3.bar(plot_df.direction, plot_df.counts, bottom=0.0, width=width)  # color=colors, width=width)\n",
    "ax3.set_yticklabels([])\n",
    "ax3.set_title(\"Reported Wind Direction Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd448f-7365-4d8e-bb29-f33553a628e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "direction_error = wind_df.direction_deg - wind_df.calculated_direction_deg\n",
    "print(f\"Direction Error Mean: {direction_error.mean()}\")\n",
    "print(f\"Direction Error STD: {direction_error.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce9d9d-fdc3-4e48-b2a8-00654a335637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 2 subplots arranged horizontally\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# First subplot\n",
    "ax1.scatter(wind_df.direction_deg, wind_df.speed, alpha=0.25, s=4)\n",
    "ax1.set_title(\"Reported Wind Direction vs Wind Speed\")\n",
    "ax1.set_xlabel(\"Reported Wind Direction\")\n",
    "ax1.set_ylabel(\"Wind Speed\")\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Second subplot (identical)\n",
    "ax2.scatter(wind_df.calculated_direction_deg, wind_df.speed, alpha=0.25, s=4)\n",
    "ax2.set_title(\"Calculated Wind Direction vs Wind Speed\")\n",
    "ax2.set_xlabel(\"Calculated Wind Direction\")\n",
    "ax2.set_ylabel(\"Wind Speed\")\n",
    "ax2.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631da45-9026-432e-b82d-831e202313a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 2 subplots arranged horizontally\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.scatter(wind_df.direction_deg, wind_df.calculated_direction_deg, alpha=0.25, s=4)\n",
    "plt.title(\"Reported Wind Direction vs Calculated Wind Direction\")\n",
    "plt.xlabel(\"Reported Wind Direction\")\n",
    "plt.ylabel(\"Calculated Wind Direction\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c5e90-6d17-4b72-beed-e6b9954736c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate\n",
    "print(wind_df[[\"direction_deg\", \"calculated_direction_deg\"]].head(20).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cacce57-2c5e-469b-90f9-708e77ffb4b9",
   "metadata": {},
   "source": [
    "## Extracting Wind Data and Uploading to BlobStore\n",
    "\n",
    "Here, we want to extract the wind datasets for an overpass and the relevant columns.  Since the data is stored in a GDrive, the simplest way of extracting the data is to download the whole folders into `orbio/methane-cv/notebooks/data/sbr_drives` and glob for the wind files.  We then load the relevant columns (wind vector components and direction) and save into new files named `<datetime of first reading>)_wind.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e915f5-b9b9-49d2-8d62-8605eb499019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wind_dir = Path(\"data/wind/\")\n",
    "wind_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "max_time_gap = 10  # seconds - files with larger gaps typically has irregular sampling\n",
    "WIND_BLOB = \"sbr_wind_data\"\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    for file in wind_files:\n",
    "        satellite = Path(file).parent.stem.split(\"_\")[-1]\n",
    "        df = (\n",
    "            pd.read_csv(file)\n",
    "            .assign(satellite=satellite)\n",
    "            .assign(timestamp=lambda df: pd.to_datetime(df[\"TIMESTAMP UTC\"]))\n",
    "            .assign(speed_x=lambda df: df[\"Ux_Avg_10meter(m/s)\"])\n",
    "            .assign(speed_y=lambda df: df[\"Uy_Avg_10meter(m/s)\"])\n",
    "            .assign(direction_deg=lambda df: df[\"WndDir_10meter(degrees)\"])[\n",
    "                [\"satellite\", \"timestamp\", \"speed_x\", \"speed_y\", \"direction_deg\"]\n",
    "            ]\n",
    "            .dropna()  # some overpasses have no data but the file exists\n",
    "            .sort_values(by=\"timestamp\", ascending=True)\n",
    "        )\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(f\"'{file}' has no data -- skipping\")\n",
    "            continue\n",
    "        # skip files with time gaps > 10 as that indicates weird sampling occuring\n",
    "        if (gap := df.timestamp.diff().max()) > pd.Timedelta(seconds=max_time_gap):\n",
    "            print(f\"'{file}' has a large gap in time ({gap}) -- skipping\")\n",
    "            continue\n",
    "\n",
    "        satellite = df.satellite.iloc[0]\n",
    "        date = df.timestamp.iloc[0].date().isoformat()\n",
    "        filepath = Path(temp_dir) / f\"{satellite}_{date}.parquet\"\n",
    "        df.to_parquet(filepath)\n",
    "\n",
    "        ##############################\n",
    "        # upload to blob store\n",
    "        ##############################\n",
    "        # since the files are on local disk and we want to preserve the original file / directory structure\n",
    "        # we need to strip out the containing directory not part of the original structure\n",
    "        relative_path = file.parent.parent.parent\n",
    "        blob_path = file.relative_to(relative_path)\n",
    "        azure_blob_path = Path(blob) / blob_path\n",
    "\n",
    "        try:\n",
    "            upload_dir(filepath.as_posix(), azure_blob_path.as_posix(), recursive=False)\n",
    "        except Exception as err:\n",
    "            if err.error_code == \"ScriptExecution.WriteStreams.AlreadyExists\":\n",
    "                print(f\"{file} already exists or is not empty -- skipping\")\n",
    "            else:\n",
    "                raise err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf10df8-8314-40b7-9ef5-c16fad908ebd",
   "metadata": {},
   "source": [
    "### Check if files were uploaded OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac483cc-efb6-4a75-8f1d-5503901f452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.fsspec import AzureMachineLearningFileSystem\n",
    "\n",
    "from src.azure_wrap.blob_storage_sdk_v2 import DATASTORE_URI\n",
    "\n",
    "fs = AzureMachineLearningFileSystem(DATASTORE_URI)\n",
    "wind_files = list(fs.glob(f\"{WIND_BLOB}/**/*.parquet\"))\n",
    "\n",
    "len(wind_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb7ecf-fe97-4a47-95fc-f6d63d44443d",
   "metadata": {},
   "source": [
    "### Check for files with large time gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e66a3f-51b6-4333-8ea8-509a0e6ab803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "durations = []\n",
    "max_gaps = []\n",
    "\n",
    "for file in wind_files:\n",
    "    satellite = Path(file).parent.stem.split(\"_\")[-1]\n",
    "    df = (\n",
    "        pd.read_csv(file)\n",
    "        .assign(satellite=satellite)\n",
    "        .assign(timestamp=lambda df: pd.to_datetime(df[\"TIMESTAMP UTC\"]))\n",
    "        .assign(speed_x=lambda df: df[\"Ux_Avg_10meter(m/s)\"])\n",
    "        .assign(speed_y=lambda df: df[\"Uy_Avg_10meter(m/s)\"])\n",
    "        .assign(direction_deg=lambda df: df[\"WndDir_10meter(degrees)\"])[\n",
    "            [\"satellite\", \"timestamp\", \"speed_x\", \"speed_y\", \"direction_deg\"]\n",
    "        ]\n",
    "        .dropna()  # some overpasses have no data but the file exists\n",
    "    )\n",
    "\n",
    "    duration = df.timestamp.max() - df.timestamp.min()\n",
    "    durations.append(duration)\n",
    "    max_gap = df.timestamp.diff().max()\n",
    "    max_gaps.append(max_gap)\n",
    "    files.append(file)\n",
    "df = pd.DataFrame({\"file\": files, \"duration\": durations, \"max_gap\": max_gaps}).sort_values(by=\"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b7945-c8b5-4870-8dba-7a212d9af6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_gaps_df = df[df.max_gap > pd.Timedelta(seconds=10)].reset_index(drop=True)\n",
    "large_gaps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023fd1d-79dd-4c0e-94ed-2535dfebde12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looks like all the files have at least an hour duration\n",
    "# the minimum duration of a file with a minimal gap < 10 seconds in just over 4 hours\n",
    "df.sort_values(by=\"duration\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac63cdd-a9b4-47e8-9b61-da07b61dbf7b",
   "metadata": {},
   "source": [
    "### Inspecting Files with large gaps\n",
    "\n",
    "We will exclude files with gaps larger than 10 seconds, as that typically indicates some weird sampling happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f53eed-6e24-4c8b-8e7c-cf9b8b93f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(large_gaps_df.file)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bfdbcb-41ae-40fc-beb8-b2f6f6b38c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = (\n",
    "    pd.read_csv(\"data/sbr_data/Phase 1 (1-1-25-3-31-25)/02272025_LS8/2025-02-27_wind+meteorological_data.csv\")\n",
    "    .assign(satellite=satellite)\n",
    "    .assign(timestamp=lambda df: pd.to_datetime(df[\"TIMESTAMP UTC\"]))\n",
    "    .assign(speed_x=lambda df: df[\"Ux_Avg_10meter(m/s)\"])\n",
    "    .assign(speed_y=lambda df: df[\"Uy_Avg_10meter(m/s)\"])\n",
    "    .assign(direction_deg=lambda df: df[\"WndDir_10meter(degrees)\"])[\n",
    "        [\"satellite\", \"timestamp\", \"speed_x\", \"speed_y\", \"direction_deg\"]\n",
    "    ]\n",
    "    .dropna()  # some overpasses have no data but the file exists\n",
    ")\n",
    "df1.timestamp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e51e6-83df-4d1d-9d23-30eb1e639921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.timestamp.diff().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14732d61-d213-4f34-abed-bf8a97983176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
