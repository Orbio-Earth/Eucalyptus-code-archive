{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Baseline to DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline = \"371-baseline-reruns\"\n",
    "baseline = \"346-larger-dataset-reruns\"\n",
    "experiment = \"DDP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query for Runs\n",
    "\n",
    "we've tagged each job we want to compare with `\"experiment\": \"DDP\"` to make searching simpler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query mlflow for runs (jobs)\n",
    "baseline_df = mlflow.search_runs(\n",
    "    experiment_names=[baseline],\n",
    "    filter_string=\"tags.comparison='DDP'\",\n",
    ").assign(experiment_name=baseline)\n",
    "\n",
    "experiment_df = mlflow.search_runs(\n",
    "    experiment_names=[experiment],\n",
    "    filter_string=\"tags.comparison='DDP'\",\n",
    ").assign(experiment_name=experiment)\n",
    "\n",
    "df = pd.concat([baseline_df, experiment_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Metrics Files\n",
    "\n",
    "We need to download each `overall_metrics.json` for each experiment and load it into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "metrics_list = []\n",
    "for run_id in df.run_id:\n",
    "    artifact_path = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"overall_metrics.json\")\n",
    "\n",
    "    with open(artifact_path) as f:\n",
    "        metrics = json.load(f)\n",
    "        metrics_df = pd.DataFrame(metrics, index=[run_id])\n",
    "        metrics_list.append(metrics_df)\n",
    "\n",
    "metrics_df = pd.concat(metrics_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(metrics_df, how=\"left\", left_on=\"run_id\", right_index=True).assign(\n",
    "    duration=lambda df: (df.end_time - df.start_time),\n",
    "    minutes=lambda df: (df.end_time - df.start_time).astype(int) / (1e9 * 60),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons\n",
    "\n",
    "We will be comparing the following metrics between the baseline and the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    [\n",
    "        \"experiment_name\",\n",
    "        \"duration\",\n",
    "        \"f1_score\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"false_positive_rate\",\n",
    "        \"false_negative_rate\",\n",
    "        \"signal2noise_ratio\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Training Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that baseline sample mean duration is 'greater' than the experiment sample mean\n",
    "# for now, we ignore multiple comparison corrections because there's only a single comparison :)\n",
    "# multiple comparison test: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dunnett.html\n",
    "sample_a = df[df.experiment_name == baseline].minutes\n",
    "sample_b = df[df.experiment_name == experiment].minutes\n",
    "stats_duration = ttest_ind(sample_a, sample_b, equal_var=False, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_duration)\n",
    "print(stats_duration.confidence_interval())\n",
    "# The confidence interval does not include zero, so we can conclude that\n",
    "# sample mean of the baseline is greater than the sample mean of the experiment\n",
    "avg_difference = sample_a.mean() - sample_b.mean()\n",
    "avg_percent = (sample_a.mean() - sample_b.mean()) / sample_a.mean() * 100\n",
    "print(f\"The experiment is {avg_difference:0.2f} ({avg_percent:0.2f}%) minutes faster on average.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that baseline sample mean F1 Score is 'greater' than the experiment sample mean\n",
    "# for now, we ignore multiple comparison corrections because there's only a single comparison :)\n",
    "# multiple comparison test: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dunnett.html\n",
    "sample_a = df[df.experiment_name == baseline].f1_score\n",
    "sample_b = df[df.experiment_name == experiment].f1_score\n",
    "stats_f1_score = ttest_ind(sample_a, sample_b, equal_var=False, alternative=\"two-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_f1_score)\n",
    "print(stats_f1_score.confidence_interval())\n",
    "# The confidence interval does include zero, so we cannot reject the null hypothesis\n",
    "# and conclude the sample mean of the baseline is no different than the sample mean of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that baseline sample mean Precision is 'greater' than the experiment sample mean\n",
    "# for now, we ignore multiple comparison corrections because there's only a single comparison :)\n",
    "# multiple comparison test: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dunnett.html\n",
    "sample_a = df[df.experiment_name == baseline].precision\n",
    "sample_b = df[df.experiment_name == experiment].precision\n",
    "stats_precision = ttest_ind(sample_a, sample_b, equal_var=False, alternative=\"two-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_precision)\n",
    "print(stats_precision.confidence_interval())\n",
    "# The confidence interval does include zero, so we cannot reject the null hypothesis\n",
    "# and conclude the sample mean of the baseline precision is no different than the sample mean of the experiment precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that baseline sample mean Recall is 'greater' than the experiment sample mean\n",
    "# for now, we ignore multiple comparison corrections because there's only a single comparison :)\n",
    "# multiple comparison test: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dunnett.html\n",
    "sample_a = df[df.experiment_name == baseline].recall\n",
    "sample_b = df[df.experiment_name == experiment].recall\n",
    "stats_recall = ttest_ind(sample_a, sample_b, equal_var=False, alternative=\"two-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_recall)\n",
    "print(stats_recall.confidence_interval())\n",
    "# The confidence interval does include zero, so we cannot reject the null hypothesis\n",
    "# and conclude the sample mean of the baseline recall is no different than the sample mean of the experiment recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that baseline sample mean FPR is 'greater' than the experiment sample mean\n",
    "# for now, we ignore multiple comparison corrections because there's only a single comparison :)\n",
    "# multiple comparison test: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dunnett.html\n",
    "sample_a = df[df.experiment_name == baseline].false_positive_rate\n",
    "sample_b = df[df.experiment_name == experiment].false_positive_rate\n",
    "stats_fpr = ttest_ind(sample_a, sample_b, equal_var=False, alternative=\"two-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_fpr)\n",
    "print(stats_fpr.confidence_interval())\n",
    "# The confidence interval does include zero, so we cannot reject the null hypothesis\n",
    "# and conclude the sample mean of the baseline FPR is no different than the sample mean of the experiment FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. False Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that baseline sample mean FNR is 'greater' than the experiment sample mean\n",
    "# for now, we ignore multiple comparison corrections because there's only a single comparison :)\n",
    "# multiple comparison test: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dunnett.html\n",
    "sample_a = df[df.experiment_name == baseline].false_negative_rate\n",
    "sample_b = df[df.experiment_name == experiment].false_negative_rate\n",
    "stats_fnr = ttest_ind(sample_a, sample_b, equal_var=False, alternative=\"two-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_fnr)\n",
    "print(stats_fnr.confidence_interval())\n",
    "# The confidence interval does include zero, so we cannot reject the null hypothesis\n",
    "# and conclude the sample mean of the baseline FNR is no different than the sample mean of the experiment FNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Signal2Noise Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that baseline sample mean Signal2Noise Ratio is 'greater' than the experiment sample mean\n",
    "# for now, we ignore multiple comparison corrections because there's only a single comparison :)\n",
    "# multiple comparison test: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dunnett.html\n",
    "sample_a = df[df.experiment_name == baseline].signal2noise_ratio\n",
    "sample_b = df[df.experiment_name == experiment].signal2noise_ratio\n",
    "stats_snr = ttest_ind(sample_a, sample_b, equal_var=False, alternative=\"two-sided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_snr)\n",
    "print(stats_snr.confidence_interval())\n",
    "# The confidence interval does not include zero, so we reject the null hypothesis\n",
    "# and conclude the sample mean of the baseline SNR is different than the sample mean of the experiment SNR\n",
    "avg_difference = sample_a.mean() - sample_b.mean()\n",
    "print(f\"The experiment has a larger SNR by {avg_difference:0.3f} on average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install tabulate -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = df[\n",
    "    [\n",
    "        \"run_id\",\n",
    "        \"experiment_name\",\n",
    "        \"minutes\",\n",
    "        \"total_samples\",\n",
    "        \"total_pixels\",\n",
    "        \"total_positives\",\n",
    "        \"total_negatives\",\n",
    "        \"mean_squared_error\",\n",
    "        \"true_positives\",\n",
    "        \"true_negatives\",\n",
    "        \"false_positives\",\n",
    "        \"false_negatives\",\n",
    "        \"true_positive_rate\",\n",
    "        \"true_negative_rate\",\n",
    "        \"false_positive_rate\",\n",
    "        \"false_negative_rate\",\n",
    "        \"recall\",\n",
    "        \"precision\",\n",
    "        \"f1_score\",\n",
    "        \"signal2noise_ratio\",\n",
    "    ]\n",
    "].set_index(\"run_id\")\n",
    "print(report_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": [\n",
    "            \"duration\",\n",
    "            \"f1_score\",\n",
    "            \"precision\",\n",
    "            \"recall\",\n",
    "            \"false_positive_rate\",\n",
    "            \"false_negative_rate\",\n",
    "            \"signal2noise_ratio\",\n",
    "        ],\n",
    "        \"test\": [\n",
    "            \"one-sided t-test\",\n",
    "            \"two-sided t-test\",\n",
    "            \"two-sided t-test\",\n",
    "            \"two-sided t-test\",\n",
    "            \"two-sided t-test\",\n",
    "            \"two-sided t-test\",\n",
    "            \"two-sided t-test\",\n",
    "        ],\n",
    "        \"CI_lower\": [\n",
    "            stats_duration.confidence_interval().low,\n",
    "            stats_f1_score.confidence_interval().low,\n",
    "            stats_precision.confidence_interval().low,\n",
    "            stats_recall.confidence_interval().low,\n",
    "            stats_fpr.confidence_interval().low,\n",
    "            stats_fnr.confidence_interval().low,\n",
    "            stats_snr.confidence_interval().low,\n",
    "        ],\n",
    "        \"CI_upper\": [\n",
    "            stats_duration.confidence_interval().high,\n",
    "            stats_f1_score.confidence_interval().high,\n",
    "            stats_precision.confidence_interval().high,\n",
    "            stats_recall.confidence_interval().high,\n",
    "            stats_fpr.confidence_interval().high,\n",
    "            stats_fnr.confidence_interval().high,\n",
    "            stats_snr.confidence_interval().high,\n",
    "        ],\n",
    "        \"conclusion\": [\n",
    "            \"The experiment is 162.04 minutes (23.68%) faster on average\",\n",
    "            \"no difference\",\n",
    "            \"no difference\",\n",
    "            \"no difference\",\n",
    "            \"no difference\",\n",
    "            \"no difference\",\n",
    "            \"The experiment has a larger SNR by 0.504 on average\",\n",
    "        ],\n",
    "    }\n",
    ").set_index(\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "methane-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
