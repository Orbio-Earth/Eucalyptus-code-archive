{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfe4d10-c99a-44a0-b9cf-5ba4aa40923c",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8650427-5968-4835-90da-a7900a94ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import collections\n",
    "import datetime\n",
    "import hashlib\n",
    "import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac\n",
    "import rasterio\n",
    "import shapely\n",
    "import tqdm\n",
    "from pystac_client import Client\n",
    "from shapely import wkt\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 250)\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from src.azure_wrap.ml_client_utils import (\n",
    "    initialize_blob_service_client,\n",
    "    initialize_ml_client,\n",
    ")\n",
    "\n",
    "ml_client = initialize_ml_client()\n",
    "abs_client = initialize_blob_service_client(ml_client)\n",
    "\n",
    "\n",
    "def x_intersects_any_in_y(x: gpd.GeoSeries, y: gpd.GeoSeries):\n",
    "    \"\"\"Uses geopandas' spatial index to figure out, for each geometry in x,\n",
    "    whether it intersects with *any* geometry in y.\n",
    "\n",
    "    Same as `x.intersects(y.union_all())` but much faster.\n",
    "    \"\"\"\n",
    "    if len(y) == 0:\n",
    "        return np.zeros(len(x), dtype=bool)\n",
    "    matches, distances = y.sindex.nearest(x, return_all=False, return_distance=True)\n",
    "    return distances == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9f564-daf5-4959-8785-24c45cd38aed",
   "metadata": {},
   "source": [
    "# Get MGRS catalogue overlapping O&G producing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d1044-c744-4f51-b971-4ffd2ee887c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get World Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd181b5-c383-4148-8b5e-5053b21cf112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get country boundaries\n",
    "# !curl -L -o data/ne_110m_admin_0_countries.zip https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\n",
    "# !unzip -u data/ne_110m_admin_0_countries.zip -d data/ne_110m_admin_0_countries\n",
    "# !rm data/ne_110m_admin_0_countries.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bbb4b6-a8ad-4c93-a5ea-39c5cd209ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(\"data/ne_110m_admin_0_countries/\")\n",
    "world.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccfbf5-1870-4809-aef6-2e527326690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to map countries in the world dataframeto the EI regions\n",
    "# This won't be pretty.\n",
    "# Note: moved Turkey from Europe to Middle East\n",
    "EI_countries_in_regions = {  # copy-pasted from spreadsheet\n",
    "    \"USA\": [\"United States of America\"],\n",
    "    \"North America wo US\": [\"Canada\", \"Mexico\"],\n",
    "    \"Europe\": [\n",
    "        \"Denmark\",\n",
    "        \"Germany\",\n",
    "        \"Italy\",\n",
    "        \"Netherlands\",\n",
    "        \"Norway\",\n",
    "        \"Poland\",\n",
    "        \"Romania\",\n",
    "        \"Ukraine\",\n",
    "        \"United Kingdom\",\n",
    "        \"Georgia\",\n",
    "    ],\n",
    "    \"CIS\": [\n",
    "        \"Armenia\",\n",
    "        \"Azerbaijan\",\n",
    "        \"Belarus\",\n",
    "        \"Kazakhstan\",\n",
    "        \"Kyrgyzstan\",\n",
    "        \"Moldova\",\n",
    "        \"Russia\",\n",
    "        \"Tajikistan\",\n",
    "        \"Turkmenistan\",\n",
    "        \"Uzbekistan\",\n",
    "    ],\n",
    "    \"Middle East\": [\n",
    "        \"Bahrain\",\n",
    "        \"Iran\",\n",
    "        \"Iraq\",\n",
    "        \"Israel\",\n",
    "        \"Kuwait\",\n",
    "        \"Oman\",\n",
    "        \"Qatar\",\n",
    "        \"Saudi Arabia\",\n",
    "        \"Syria\",\n",
    "        \"United Arab Emirates\",\n",
    "        \"Jordan\",\n",
    "        \"Lebanon\",\n",
    "        \"Turkey\",\n",
    "    ],\n",
    "    \"Asia\": [\n",
    "        \"Bangladesh\",\n",
    "        \"Brunei\",\n",
    "        \"China\",\n",
    "        \"India\",\n",
    "        \"Indonesia\",\n",
    "        \"Malaysia\",\n",
    "        \"Myanmar\",\n",
    "        \"Pakistan\",\n",
    "        \"Thailand\",\n",
    "        \"Vietnam\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "world[\"EI_region\"] = world[\"CONTINENT\"].map(\n",
    "    {\n",
    "        \"Africa\": \"Africa\",\n",
    "        \"Asia\": \"Asia\",\n",
    "        \"Europe\": \"Europe\",\n",
    "        \"North America\": \"North America wo US\",\n",
    "        \"South America\": \"South and Central America\",\n",
    "        \"Oceania\": \"Australia\",\n",
    "        \"Seven seas (open ocean)\": \"\",\n",
    "        \"Antarctica\": \"\",\n",
    "    }\n",
    ")\n",
    "\n",
    "world.loc[world[\"SUBREGION\"] == \"Central America\", \"EI_region\"] = \"South and Central America\"\n",
    "world.loc[world[\"REGION_WB\"] == \"Middle East & North Africa\", \"EI_region\"] = \"Middle East\"\n",
    "world.loc[world[\"CONTINENT\"] == \"Africa\", \"EI_region\"] = \"Africa\"\n",
    "world.loc[world[\"SUBREGION\"] == \"Australia and New Zealand\", \"EI_region\"] = \"Australia\"\n",
    "\n",
    "EI_country_to_region_map = {\n",
    "    country: region for (region, country_list) in EI_countries_in_regions.items() for country in country_list\n",
    "}\n",
    "# check all EI countries are present in the world dataframe\n",
    "world_countries_set = set(world.SOVEREIGNT)\n",
    "for country in EI_country_to_region_map:\n",
    "    if country == \"Bahrain\":\n",
    "        # Bahrain is missing but it's tiny,\n",
    "        # so let's ignore that\n",
    "        continue\n",
    "    world_countries_set.remove(country)  # this raises a KeyError if not present\n",
    "EI_mapped_countries = world.SOVEREIGNT.map(EI_country_to_region_map)\n",
    "world[\"EI_region\"] = world.EI_region.where(EI_mapped_countries.isnull(), EI_mapped_countries)\n",
    "\n",
    "world.EI_region.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffabce-cfd4-4d51-ae3f-1aee13dc83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "world.plot(\"EI_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8109660-b527-4d08-bdc8-3d0d5aff094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EI_geometries = world.dissolve(\"EI_region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66595ede-6561-4300-b654-b4bb9734972a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MGRS tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d7379-2007-4632-a5f4-125ffe4ab62f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !curl -L -o data/S2A_OPER_GIP_TILPAR_MPC__20151209T095117_V20150622T000000_21000101T000000_B00.kml https://hls.gsfc.nasa.gov/wp-content/uploads/2016/03/S2A_OPER_GIP_TILPAR_MPC__20151209T095117_V20150622T000000_21000101T000000_B00.kml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ac80a-8ca9-4f75-ad9b-f6e6e7214859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgrs_gdf = gpd.read_file(\"data/S2A_OPER_GIP_TILPAR_MPC__20151209T095117_V20150622T000000_21000101T000000_B00.kml\", columns=[\"Name\"])\n",
    "mgrs_gdf = gpd.read_file(\n",
    "    \"data/S2A_OPER_GIP_TILPAR_MPC__20151209T095117_V20150622T000000_21000101T000000_B00.kml\", engine=\"pyogrio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b9826-fcf6-4807-8c29-517bf6464393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason each MGRS is a GeometryCollection containing polygons and points\n",
    "# let's get rid of the stupid point\n",
    "mgrs_gdf = mgrs_gdf.dissolve(\"Name\")  # performs a union on the GeometryCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263f63b-7477-4530-94b5-26ab068517ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the international date line is a pain in the ass and I don't want to deal with it\n",
    "mgrs_gdf = mgrs_gdf[~(mgrs_gdf.geometry.bounds[\"minx\"] == -180)]\n",
    "# remove tiles outside of +-60deg latitude\n",
    "mgrs_gdf = mgrs_gdf[(mgrs_gdf.geometry.bounds.maxy < 60) & (mgrs_gdf.geometry.bounds.miny > -60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043eb494-6554-40a1-bd7d-85f51a054926",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22590a2d-710c-445b-89a1-8f91a95d61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgrs_gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858d0f7-e034-48b2-9518-e498187a78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EI_geometries.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2dba3-c7bc-4b3f-9666-419e07e6a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_centroid = gpd.GeoDataFrame(geometry=mgrs_gdf.geometry.centroid)\n",
    "joined = gpd.sjoin(mgrs_centroid, EI_geometries.reset_index(), how=\"left\", predicate=\"within\")\n",
    "mgrs_gdf[\"EI_region\"] = joined[[\"EI_region\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d5fbe-c8e7-4d59-a33b-0d671fe32f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gpd.sjoin(aux_gdf, mgrs_gdf, how=\"left\", predicate=\"intersects\")[\"Name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f82ac9-54e0-424b-bb22-1ed8dec524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove tiles in our Aux/SBR set\n",
    "# aux_latlongs = pd.read_csv(\"../src/data/ancillary/ground_truth_plumes.csv\")\n",
    "\n",
    "# from shapely.geometry import Point\n",
    "\n",
    "# aux_latlongs[\"geometry\"] = aux_latlongs.apply(lambda row: Point(row[\"lon\"], row[\"lat\"]), axis=1)\n",
    "# aux_gdf = gpd.GeoDataFrame(aux_latlongs, geometry=\"geometry\", crs=\"EPSG:4326\")  # CRS for lat/lon\n",
    "# aux_mgrs = gpd.sjoin(aux_gdf, mgrs_gdf, how=\"left\", predicate=\"intersects\")\n",
    "# # aux_mgrs_list = aux_mgrs.index_right.unique().tolist()  # 8 MGRS\n",
    "# aux_mgrs_list = aux_mgrs[\"Name\"].unique().tolist()  # 8 MGRS\n",
    "\n",
    "# mgrs_gdf = mgrs_gdf[~mgrs_gdf.index.isin(aux_mgrs_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4ae01-cdf6-46d0-845a-d957f27757c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tiles with no region\n",
    "mgrs_has_region = mgrs_gdf.EI_region.str.len() > 0\n",
    "mgrs_gdf = mgrs_gdf[mgrs_has_region]\n",
    "mgrs_gdf.shape  # number of MGRS granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ecc80-f719-4431-b961-5851e727b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_gdf.plot(\"EI_region\")\n",
    "plt.title(\"MGRS tiles coloured by EI region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f8295-8189-4717-bd37-ce321995b341",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classifying MGRS scenes into O&G producing\n",
    "\n",
    "- Here we just define O&G producing as REDACTED fields.\n",
    "- See Appendix on how to load additional producing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9143c1b-15a7-44c3-a1f6-bed9a6368489",
   "metadata": {},
   "outputs": [],
   "source": [
    "producing_union = # Union of all producing areas, TODO: Insert as we can't share the one we were using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996649a-d507-48f2-ae3f-d14d2ee58a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "producing_by_region = EI_geometries.intersection(producing_union)\n",
    "producing_by_region.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6cdf25-58dd-4318-8a6c-4f730a098a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_intersects_producing = x_intersects_any_in_y(mgrs_gdf.geometry, producing_by_region.geometry)\n",
    "\n",
    "# add that as a column in data frame\n",
    "mgrs_gdf[\"OG_producing\"] = mgrs_intersects_producing\n",
    "mgrs_gdf[\"OG_producing\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aac2ad-5d90-483d-92c9-1c5c874fb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_gdf_overlapping = mgrs_gdf[mgrs_gdf[\"OG_producing\"] == True].copy()\n",
    "mgrs_gdf_overlapping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a7161-f009-4450-833c-109cb4bacea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_gdf_overlapping[\"overlapping_area\"] = mgrs_gdf_overlapping.intersection(producing_union).area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d19b9f1-34d4-424c-9fc1-cc479bc2308b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mgrs_gdf_overlapping.drop(\"description\", axis=1).explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26479bb-6e5a-442e-ad57-034997cf4b06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Get a catalogue of S2 IDs following a target region distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f043d-7c84-4aec-a3f0-18fd85f2289c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get target distribution per region\n",
    "\n",
    "- After feedback from Rob & Jack, we've come up with this split for our training tiles: Use 34% US, 24% ME, 12% Africa, 10% CIS, 5% NA without USA, 5% SMA, 5% Asia, 2.5% Australia, 2.5% EU\n",
    "    - **Note:** This is still somewhat arbitrary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56fdcb-6ee2-455e-8587-82818c011b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tiles = 3095\n",
    "num_tiles_by_region = {\n",
    "    \"USA\": int(np.round(total_tiles * 0.34)),\n",
    "    \"Middle East\": int(np.round(total_tiles * 0.24)),\n",
    "    \"Africa\": int(np.round(total_tiles * 0.12)),\n",
    "    \"CIS\": int(np.round(total_tiles * 0.10)),\n",
    "    \"North America wo US\": int(np.round(total_tiles * 0.05)),\n",
    "    \"South and Central America\": int(np.round(total_tiles * 0.05)),\n",
    "    \"Asia\": int(np.round(total_tiles * 0.05)),\n",
    "    \"Australia\": int(np.round(total_tiles * 0.025)),\n",
    "    \"Europe\": int(np.round(total_tiles * 0.025)),\n",
    "}\n",
    "num_tiles_by_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec582dd7-823a-4286-9d39-8e218cad5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgrs_gdf[mgrs_gdf[\"OG_producing\"]==True].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521a2e1-4e3a-4771-9002-5151733ed73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mgrs_gdf[mgrs_gdf[\"OG_producing\"] == True].shape)\n",
    "mgrs_gdf[mgrs_gdf[\"OG_producing\"] == True].EI_region.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d7399-3de7-4836-b220-b09ebbcea8ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Selecting producing MGRS\n",
    "- For each region, if we need N tiles and have M available MGRS tiles:\n",
    "    - If N < M: Sample N tiles randomly\n",
    "    - If N > M: Take all M tiles, then sample (N-M) additional tiles from the same pool, repeating as long as needed\n",
    "- The sampling is made reproducible by:\n",
    "    - Using a deterministic seed generated from the region name + base_seed + step\n",
    "    - Step increases for each repeated sampling within a region\n",
    "    - Base seed increments for each new region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a7d42-a71e-46b4-bc87-7923135a388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a deterministic seed using hashlib: Thanks chatgpt\n",
    "# For the same mgrs_gdf, num_tiles_by_region, and total_tiles, we will select the same\n",
    "# MGRS for each region\n",
    "def generate_seed(region: str, base_seed: int, step: int = 0) -> int:\n",
    "    region_bytes = region.encode(\"utf-8\")  # Convert region name to bytes\n",
    "    hash_object = hashlib.md5(region_bytes)  # Create an MD5 hash object\n",
    "    region_hash = int(hash_object.hexdigest(), 16)  # Convert hash to an integer\n",
    "    return (region_hash + base_seed + step) % (2**32)  # Keep seed within 32-bit range\n",
    "\n",
    "\n",
    "mgrs_selected = []\n",
    "seed_count = 3334\n",
    "for region, num in num_tiles_by_region.items():\n",
    "    mgrs_prod = mgrs_gdf[(mgrs_gdf.EI_region == region) & (mgrs_gdf[\"OG_producing\"] == True)].copy()\n",
    "    print(f\"{region:25}: We have {len(mgrs_prod):3} and want {num:3}\")\n",
    "    if num < len(mgrs_prod):\n",
    "        print(f\"  Sampling {num}\")\n",
    "        # Use a unique seed for the initial sampling\n",
    "        seed = generate_seed(region, seed_count, step=0)\n",
    "        mgrs_selected.append(mgrs_prod.sample(num, random_state=seed))\n",
    "    else:\n",
    "        print(f\"  Taking all {len(mgrs_prod)} IDs once\")\n",
    "        mgrs_selected.append(mgrs_prod)\n",
    "        num = num - len(mgrs_prod)\n",
    "        if num < len(mgrs_prod):\n",
    "            print(f\"  Sampling additional {num}\")\n",
    "            # Use a different unique seed for additional sampling\n",
    "            seed = generate_seed(region, seed_count, step=1)\n",
    "            mgrs_selected.append(mgrs_prod.sample(num, random_state=seed))\n",
    "        else:\n",
    "            print(f\"  Taking all {len(mgrs_prod)} IDs once\")\n",
    "            mgrs_selected.append(mgrs_prod)\n",
    "            num = num - len(mgrs_prod)\n",
    "            if num < len(mgrs_prod):\n",
    "                print(f\"  Sampling additional {num}\")\n",
    "                # Use yet another unique seed for further sampling\n",
    "                seed = generate_seed(region, seed_count, step=2)\n",
    "                mgrs_selected.append(mgrs_prod.sample(num, random_state=seed))\n",
    "            else:\n",
    "                print(f\"  Taking all {len(mgrs_prod)} IDs once\")\n",
    "                mgrs_selected.append(mgrs_prod)\n",
    "                num = num - len(mgrs_prod)\n",
    "                if num < len(mgrs_prod):\n",
    "                    print(f\"  Sampling additional {num}\")\n",
    "                    # Use yet another unique seed for further sampling\n",
    "                    seed = generate_seed(region, seed_count, step=3)\n",
    "                    mgrs_selected.append(mgrs_prod.sample(num, random_state=seed))\n",
    "                else:\n",
    "                    print(f\"  Taking all {len(mgrs_prod)} IDs once\")\n",
    "                    mgrs_selected.append(mgrs_prod)\n",
    "                    num = num - len(mgrs_prod)\n",
    "                    if num < len(mgrs_prod):\n",
    "                        print(f\"  Sampling additional {num}\")\n",
    "                        # Use yet another unique seed for further sampling\n",
    "                        seed = generate_seed(region, seed_count, step=3)\n",
    "                        mgrs_selected.append(mgrs_prod.sample(num, random_state=seed))\n",
    "\n",
    "    seed_count += 1  # Increment base seed for next region\n",
    "mgrs_selected = pd.concat(mgrs_selected)\n",
    "mgrs_selected = mgrs_selected.reset_index()\n",
    "assert len(mgrs_selected) == total_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c9bc8-1559-4097-8369-2f046e3980b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mgrs_selected.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbaea09-92d8-4990-800a-8298b967c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 20))\n",
    "# ax = plt.gca()\n",
    "# world.plot(\"EI_region\", ax=ax, alpha=0.1)\n",
    "# mgrs_selected.plot(ax=ax, alpha=0.5)\n",
    "# plt.title(\"Selected tiles in O&G producing areas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf6889-407e-45aa-876b-f6f6f27c65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique MGRS?\n",
    "mgrs_selected[\"Name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598edfa-1aef-497f-871c-61538ae31d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of samples for a given MGRS\n",
    "mgrs_selected[\"Name\"].value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a79cd-89de-4618-999e-8182ab169944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mgrs_selected = mgrs_selected.drop(\n",
    "    [\n",
    "        \"description\",\n",
    "        \"timestamp\",\n",
    "        \"begin\",\n",
    "        \"end\",\n",
    "        \"altitudeMode\",\n",
    "        \"tessellate\",\n",
    "        \"extrude\",\n",
    "        \"visibility\",\n",
    "        \"drawOrder\",\n",
    "        \"icon\",\n",
    "        \"snippet\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd24e8-e528-4ab5-8ac8-93e7c37f50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save master MGRS set. This will later be split into 80% train/10% val/10% test\n",
    "master_filename = \"2025_02_11_MGRS_within_OG_revamp_3095_master_set\"\n",
    "mgrs_selected.to_file(f\"../src/data/tiles/s2/csv_files/{master_filename}.geojson\", driver=\"GeoJSON\")\n",
    "mgrs_selected.to_csv(f\"../src/data/tiles/s2/csv_files/{master_filename}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e13ce-66c4-4ca9-9bca-5e5b896fce70",
   "metadata": {},
   "source": [
    "## Selecting dates at random using the STAC catalog\n",
    "- From `Selecting producing MGRS` we have selected our MGRS tiles and determined the number of samples we want from each. This info is stored in `num_samples[mgrs]` (see sampling fn, `sample_dates_for_mgrs`)\n",
    "- We enforce temporal diversity using different year ranges:\n",
    "    - 1 sample: 2016-2024\n",
    "    - 2 samples: 2016-2020, then 2021-2024\n",
    "    - 3 samples: 2016-2019, then 2020-2022, then 2023-2024\n",
    "    - ...\n",
    "- **Core Sampling Logic:**\n",
    "    - Each MGRS tile gets a deterministic seed: `get_seed_from_string(f\"{mgrs}_{mgrs_counter[mgrs]}\")`. This ensures we select the same dates each time\n",
    "    - We set initial quality thresholds of 95% max nodata, 98% max clouds\n",
    "    - If selected image has high nodata/clouds, we sample additional image with stricter thresholds (40% each)\n",
    "    - We have an additional check that we don't select 2023 dates for MGRS in our detection threshold \"test\" set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079fc48e-5a07-4842-a757-7ab891a4e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AWS Sentinel-2 data\n",
    "api_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "import planetary_computer\n",
    "\n",
    "client = Client.open(\n",
    "    api_url,\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "collection = \"sentinel-2-l2a\"\n",
    "\n",
    "# Load in current \"test set\" aka Detection Threshold tiles\n",
    "det_thresh_mgrs = pd.read_csv(\"../src/data/tiles/s2/csv_files/fpr_areas.csv\")\n",
    "det_thresh_test_set_mgrs = det_thresh_mgrs.mgrs.unique().tolist()\n",
    "\n",
    "\n",
    "### Sampling Logic Helper Functions ###\n",
    "def get_seed_from_string(s: str) -> int:\n",
    "    \"\"\"Create reproducible seed from string using hashlib.\"\"\"\n",
    "    return int(hashlib.sha256(s.encode()).hexdigest(), 16) % (2**32)\n",
    "\n",
    "\n",
    "def random_date(\n",
    "    start: datetime.datetime, end: datetime.datetime, rng: np.random.Generator, mgrs: str\n",
    ") -> datetime.datetime:\n",
    "    while True:\n",
    "        time_delta = int((end - start).total_seconds())\n",
    "        random_seconds = int(rng.integers(0, time_delta))\n",
    "        date = start + datetime.timedelta(seconds=random_seconds)\n",
    "\n",
    "        # For test set MGRS, keep sampling until we get a non-2023 date\n",
    "        if mgrs in det_thresh_test_set_mgrs and date.year == 2023:\n",
    "            print(\"Got a test set MGRS, trying to find non 2023 date...\")\n",
    "            continue\n",
    "        return date\n",
    "\n",
    "\n",
    "def random_item_for_MGRS(\n",
    "    mgrs: str, rng: np.random.Generator, max_cloud_cover: float = 60, start_year: int = 2016, end_year: int = 2024\n",
    "):\n",
    "    start_time = datetime.datetime(start_year, 1, 1)\n",
    "    if start_year == 2016:\n",
    "        # Start later to avoid not finding reference images\n",
    "        end_range_start = datetime.datetime(start_year, 5, 1)\n",
    "        end_range_end = datetime.datetime(end_year, 12, 31)\n",
    "    else:\n",
    "        end_range_start = datetime.datetime(start_year, 1, 1)\n",
    "        end_range_end = datetime.datetime(end_year, 12, 31)\n",
    "\n",
    "    end_time = random_date(end_range_start, end_range_end, rng, mgrs)\n",
    "\n",
    "    search = client.search(\n",
    "        collections=[collection],\n",
    "        max_items=1,\n",
    "        datetime=f\"{start_time.isoformat()}/{end_time.isoformat()}\",\n",
    "        query={\"s2:mgrs_tile\": {\"eq\": mgrs}, \"eo:cloud_cover\": {\"lt\": max_cloud_cover}},\n",
    "    )\n",
    "    return next(search.items())\n",
    "\n",
    "\n",
    "def item_area(item: pystac.Item) -> float:\n",
    "    polygon = shapely.geometry.shape(item.geometry)\n",
    "    return polygon.area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73492205-bdce-443f-92ba-1be7b3cbe8c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## within producing regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e16ee0-efa6-44f9-96a1-6b6ec7a915bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in selected MGRS tiles (within producing regions)\n",
    "mgrs_selected = gpd.read_file(\"../src/data/tiles/s2/csv_files/2025_02_11_MGRS_within_OG_revamp_3095_master_set.geojson\")\n",
    "mgrs_selected = mgrs_selected.sort_values(\"Name\")\n",
    "mgrs_we_want = mgrs_selected[\"Name\"].value_counts().to_dict()\n",
    "mgrs_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248d53f-86fd-40d2-9a67-2c805b1b0089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_dates_for_mgrs(\n",
    "    mgrs_selected_df: pd.DataFrame, num_samples: dict[str, int], debug: bool = True, verbose: bool = True\n",
    "):\n",
    "    # Output lists\n",
    "    selected_prod_items = []\n",
    "    nodata_percs = []\n",
    "    additional_id_due_to_high_nodata_clouds = []\n",
    "\n",
    "    def default_one():\n",
    "        return 1\n",
    "\n",
    "    mgrs_counter = collections.defaultdict(default_one)\n",
    "    additional_id_counter = collections.defaultdict(int)\n",
    "    already_sampled_dates = collections.defaultdict(set)\n",
    "    already_sampled_cloud_covers = collections.defaultdict(list)\n",
    "    already_sampled_nodatas = collections.defaultdict(list)\n",
    "\n",
    "    for idx, row in tqdm.tqdm(mgrs_selected_df.iterrows(), total=len(mgrs_selected_df)):\n",
    "        if debug:\n",
    "            if idx == 30:\n",
    "                break\n",
    "        # Ensure that for any given MGRS tile, it will always select the same date,\n",
    "        # so the output is reproducible\n",
    "        mgrs = row[\"Name\"]\n",
    "        mgrs_area = row[\"geometry\"].area\n",
    "\n",
    "        # Create reproducible RNG for this MGRS tile and counter combination\n",
    "        seed = get_seed_from_string(f\"{mgrs}_{mgrs_counter[mgrs]}\")\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Determine start and end year for temporal diversity\n",
    "        if num_samples[mgrs] == 1:\n",
    "            start_year = 2016\n",
    "            end_year = 2024\n",
    "        # If we want 2 IDs per tile, take one 2016-2021, one 2022-2024\n",
    "        elif num_samples[mgrs] == 2:\n",
    "            if mgrs_counter[mgrs] == 1:\n",
    "                start_year = 2016\n",
    "                end_year = 2020\n",
    "            else:\n",
    "                start_year = 2021\n",
    "                end_year = 2024\n",
    "        # If we want 3 IDs per tile, take one 2016-2019, one 2020-2022, one 2023-2024\n",
    "        elif num_samples[mgrs] == 3:\n",
    "            if mgrs_counter[mgrs] == 1:\n",
    "                start_year = 2016\n",
    "                end_year = 2019\n",
    "            elif mgrs_counter[mgrs] == 2:\n",
    "                start_year = 2020\n",
    "                end_year = 2022\n",
    "            else:\n",
    "                start_year = 2023\n",
    "                end_year = 2024\n",
    "        # If we want 4 IDs per tile, take one 2016-2018, one 2019-2021, one 2022-2023, one 2024-2025\n",
    "        elif num_samples[mgrs] == 4:\n",
    "            if mgrs_counter[mgrs] == 1:\n",
    "                start_year = 2016\n",
    "                end_year = 2018\n",
    "            elif mgrs_counter[mgrs] == 2:\n",
    "                start_year = 2019\n",
    "                end_year = 2021\n",
    "            elif mgrs_counter[mgrs] == 3:\n",
    "                start_year = 2022\n",
    "                end_year = 2023\n",
    "            else:\n",
    "                start_year = 2024\n",
    "                end_year = 2025\n",
    "        # If we want 5 IDs per tile, take one 2016-2017, one 2018-2019, one 2020-2021, one 2022-2023, one 2024-2025\n",
    "        elif num_samples[mgrs] == 5:\n",
    "            if mgrs_counter[mgrs] == 1:\n",
    "                start_year = 2016\n",
    "                end_year = 2017\n",
    "            elif mgrs_counter[mgrs] == 2:\n",
    "                start_year = 2018\n",
    "                end_year = 2019\n",
    "            elif mgrs_counter[mgrs] == 3:\n",
    "                start_year = 2020\n",
    "                end_year = 2021\n",
    "            elif mgrs_counter[mgrs] == 4:\n",
    "                start_year = 2022\n",
    "                end_year = 2023\n",
    "            else:\n",
    "                start_year = 2023\n",
    "                end_year = 2024\n",
    "        print(\n",
    "            f\"{mgrs}: We have {mgrs_counter[mgrs] - 1}/{num_samples[mgrs]} --> Sample randomly from {start_year}-01-01 to {end_year}-12-31\"\n",
    "        )\n",
    "\n",
    "        # Initialize nodata and cloud cover thresholds\n",
    "        max_nodata_px_perc = 95\n",
    "        max_cloud_cover = 98\n",
    "        for _ in range(100):  # 100 retries\n",
    "            while True:\n",
    "                try:\n",
    "                    # Attempt to sample a random item\n",
    "                    item = random_item_for_MGRS(\n",
    "                        mgrs, rng=rng, max_cloud_cover=max_cloud_cover, start_year=start_year, end_year=end_year\n",
    "                    )\n",
    "                    id_ = item.to_dict()[\"properties\"][\"s2:product_uri\"].replace(\".SAFE\", \"\")\n",
    "                    if item.datetime in already_sampled_dates[mgrs]:\n",
    "                        print(f\"{mgrs} - {id_} --> DONT USE (we have this date already)\")\n",
    "                        continue\n",
    "                    break\n",
    "                except Exception:\n",
    "                    if debug:\n",
    "                        print(\"No data found between start and end date. Try again\")\n",
    "                    pass\n",
    "\n",
    "            # Calculate nodata percentage and cloud cover\n",
    "            nodata_perc = abs(100 - 100 * item_area(item) / mgrs_area)\n",
    "            s2_date = id_.split(\"_\")[2]\n",
    "            s2_date = f\"{s2_date[:4]}-{s2_date[4:6]}-{s2_date[6:8]}\"\n",
    "            cloud_perc = item.ext.eo.cloud_cover\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"{mgrs} (#{mgrs_counter[mgrs]}) on {s2_date} has ({nodata_perc:6.1f}% nodata) and {cloud_perc:6.1f}% clouds ({id_})\",\n",
    "                    end=\"\",\n",
    "                )\n",
    "\n",
    "            if nodata_perc > max_nodata_px_perc:\n",
    "                print(\" --> DONT USE (too much nodata)\")\n",
    "                continue\n",
    "            if item.datetime in already_sampled_dates[mgrs]:\n",
    "                print(\" --> DONT USE (we have this date already)\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\" --> USE\")\n",
    "                selected_prod_items.append(item)\n",
    "                nodata_percs.append(nodata_perc)\n",
    "                additional_id_due_to_high_nodata_clouds.append(False)\n",
    "                already_sampled_dates[mgrs].add(item.datetime)\n",
    "                already_sampled_cloud_covers[mgrs].append(cloud_perc)\n",
    "                already_sampled_nodatas[mgrs].append(nodata_perc)\n",
    "\n",
    "                # sample another ID depending on how much nodata/clouds we got\n",
    "                # Example: 90% nodata + 50% clouds ==> 95% of pixels are obscured ==> 95% to sample another ID with nodata max 40%/clouds max 40%\n",
    "                random_ = rng.random() * 100\n",
    "                if debug:\n",
    "                    print(f\"random: {random_:.2f} vs {nodata_perc + (1 - nodata_perc/100) * cloud_perc:.2f}\")\n",
    "                if random_ < nodata_perc + (1 - nodata_perc / 100) * cloud_perc:\n",
    "                    max_nodata_px_perc = 40\n",
    "                    max_cloud_cover = 40\n",
    "                    while True:\n",
    "                        try:\n",
    "                            item = random_item_for_MGRS(\n",
    "                                mgrs, rng=rng, max_cloud_cover=max_cloud_cover, start_year=start_year, end_year=end_year\n",
    "                            )\n",
    "                            id_ = item.to_dict()[\"properties\"][\"s2:product_uri\"].replace(\".SAFE\", \"\")\n",
    "                            if item.datetime in already_sampled_dates[mgrs]:\n",
    "                                print(f\"{mgrs} - {id_} --> DONT USE (we have this date already)\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                nodata_perc = abs(100 - 100 * item_area(item) / mgrs_area)\n",
    "                                s2_date = id_.split(\"_\")[2]\n",
    "                                s2_date = f\"{s2_date[:4]}-{s2_date[4:6]}-{s2_date[6:8]}\"\n",
    "                                cloud_perc = item.ext.eo.cloud_cover\n",
    "                                if verbose:\n",
    "                                    print(\n",
    "                                        f\"  {mgrs} (#{mgrs_counter[mgrs]}) on {s2_date} has ({nodata_perc:6.1f}% nodata) and {cloud_perc:6.1f}% clouds ({id_})\",\n",
    "                                        end=\"\",\n",
    "                                    )\n",
    "                                if nodata_perc > max_nodata_px_perc:\n",
    "                                    print(\" --> DONT USE (too much nodata)\")\n",
    "                                    continue\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            if debug:\n",
    "                                print(\"No data found between start and end date. Try again\")\n",
    "                            pass\n",
    "                    print(\" --> USE\")\n",
    "                    additional_id_counter[mgrs] += 1\n",
    "                    selected_prod_items.append(item)\n",
    "                    nodata_percs.append(nodata_perc)\n",
    "                    additional_id_due_to_high_nodata_clouds.append(True)\n",
    "                    already_sampled_dates[mgrs].add(item.datetime)\n",
    "                    already_sampled_cloud_covers[mgrs].append(cloud_perc)\n",
    "                    already_sampled_nodatas[mgrs].append(nodata_perc)\n",
    "\n",
    "                mgrs_counter[mgrs] += 1\n",
    "                if num_samples[mgrs] == mgrs_counter[mgrs] - 1:\n",
    "                    print(\n",
    "                        f\"{mgrs}: We sampled {num_samples[mgrs]} and {additional_id_counter[mgrs]} additional IDs due to \"\n",
    "                        f\"high nodata/cloud cover ({sorted([k.date().isoformat() for k in already_sampled_dates[mgrs]])})\"\n",
    "                    )\n",
    "                    print(\"#\" * 150)\n",
    "            break\n",
    "        else:\n",
    "            raise ValueError(\"couldn't find big enough tile after 100 tries\")\n",
    "    return selected_prod_items, nodata_percs, additional_id_due_to_high_nodata_clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e37432-6d9b-45ff-8593-34f7beec8143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_prod_items, nodata_percs, additional_id_due_to_high_nodata_clouds = sample_dates_for_mgrs(\n",
    "    mgrs_selected, mgrs_we_want, debug=False, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417527a7-ed2f-4f80-adc8-eb054ebc9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to df\n",
    "selected_prod_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"mgrs\": item.properties[\"s2:mgrs_tile\"].zfill(\n",
    "                5\n",
    "            ),  # item.ext.grid.code.split(\"-\")[1].zfill(5),  # 'MGRS-9UXA' -> '09UXA'\n",
    "            \"date\": item.datetime.date().isoformat(),\n",
    "            \"datetime\": item.datetime.isoformat(),\n",
    "            \"ID\": item.to_dict()[\"properties\"][\"s2:product_uri\"].replace(\".SAFE\", \"\"),\n",
    "            \"cloud_percent\": item.ext.eo.cloud_cover,\n",
    "        }\n",
    "        for item in selected_prod_items\n",
    "    ]\n",
    ").sort_values([\"mgrs\", \"date\"])\n",
    "selected_prod_df[\"selected_due_to_high_nodata_clouds\"] = additional_id_due_to_high_nodata_clouds\n",
    "selected_prod_df[\"nodata_perc\"] = [abs(np.round(k, 3)) for k in nodata_percs]\n",
    "print(selected_prod_df.shape)\n",
    "selected_prod_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4a0ab-df6f-4ead-bd3d-5c152b077392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final samples as csv\n",
    "final_df_filename = f\"2025_02_12_MGRS_with_IDs_within_OG_{len(selected_prod_df)}\"\n",
    "selected_prod_df.to_csv(\n",
    "    f\"../src/data/tiles/s2/csv_files/{final_df_filename}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9a21f-8cdb-4b14-8801-f9cfe60dd12f",
   "metadata": {},
   "source": [
    "### Final df Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceadcc1-9450-4023-b656-ed114194cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: In our final MGRS-date pairs, MGRS that also belong to the det thresh csv\n",
    "# should have non 2023 dates\n",
    "selected_prod_df[selected_prod_df.mgrs.isin(det_thresh_test_set_mgrs)].date.apply(\n",
    "    lambda d: pd.to_datetime(d).year\n",
    ").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b128a80-576f-47fd-919a-623390199caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_prod_df[\"selected_due_to_high_nodata_clouds\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d44b3-cfaa-4f65-bcd8-7fa896031e10",
   "metadata": {},
   "source": [
    "## Add more IDs for Hassi/Marcellus/Permian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f731130-0155-4c7a-9575-8d40ccef39c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mgrs_selected = pd.read_csv(\"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_with_metadata.csv\")\n",
    "\n",
    "if \"EI_region\" not in mgrs_selected.columns:\n",
    "    # Join with mgrs_gdf to get region col\n",
    "    mgrs_gdf_reset = mgrs_gdf.reset_index()\n",
    "    mgrs_selected = pd.merge(\n",
    "        mgrs_selected, mgrs_gdf[[\"EI_region\", \"geometry\"]], left_on=\"mgrs\", right_index=True, how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8df04-9f98-4187-b847-bfd0d548a04b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "region_geoms = [\n",
    "    gpd.read_file(\"../src/data/ancillary/hassi.geojson\").geometry.iloc[0].buffer(1.5).simplify(0.1),\n",
    "    gpd.read_file(\"../src/data/ancillary/marcellus.geojson\").geometry.iloc[0],\n",
    "    gpd.read_file(\"../src/data/ancillary/permian.geojson\").geometry.iloc[0],\n",
    "]\n",
    "regions = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"region\": [\n",
    "            \"hassi\",\n",
    "            \"marcellus\",\n",
    "            \"permian\",\n",
    "        ]\n",
    "    },\n",
    "    geometry=region_geoms,\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "regions.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb514278-6adc-4ade-9330-163d88e18289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate overlap with hassi/marcellus/permian\n",
    "mgrs_selected[\"geometry\"] = mgrs_selected[\"geometry\"].apply(wkt.loads)\n",
    "mgrs_selected = gpd.GeoDataFrame(mgrs_selected, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "mgrs_selected[\"inside_val_regions\"] = x_intersects_any_in_y(mgrs_selected.geometry, regions.geometry)\n",
    "mgrs_selected[\"inside_hassi\"] = x_intersects_any_in_y(\n",
    "    mgrs_selected.geometry, regions[regions[\"region\"] == \"hassi\"].geometry\n",
    ")\n",
    "mgrs_selected[\"inside_marcellus\"] = x_intersects_any_in_y(\n",
    "    mgrs_selected.geometry, regions[regions[\"region\"] == \"marcellus\"].geometry\n",
    ")\n",
    "mgrs_selected[\"inside_permian\"] = x_intersects_any_in_y(\n",
    "    mgrs_selected.geometry, regions[regions[\"region\"] == \"permian\"].geometry\n",
    ")\n",
    "\n",
    "mgrs_gdf[\"inside_val_regions\"] = x_intersects_any_in_y(mgrs_gdf.geometry, regions.geometry)\n",
    "mgrs_gdf[\"inside_hassi\"] = x_intersects_any_in_y(mgrs_gdf.geometry, regions[regions[\"region\"] == \"hassi\"].geometry)\n",
    "mgrs_gdf[\"inside_marcellus\"] = x_intersects_any_in_y(\n",
    "    mgrs_gdf.geometry, regions[regions[\"region\"] == \"marcellus\"].geometry\n",
    ")\n",
    "mgrs_gdf[\"inside_permian\"] = x_intersects_any_in_y(mgrs_gdf.geometry, regions[regions[\"region\"] == \"permian\"].geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035da46-b09e-4dc9-90f0-f7fb1358b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_selected[[\"inside_hassi\", \"inside_marcellus\", \"inside_permian\", \"inside_val_regions\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9d038-0aba-434e-83ee-aec3288ebf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids = mgrs_selected[mgrs_selected[\"inside_val_regions\"] == True].index.tolist()\n",
    "len(val_ids), len(set(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7a60e-5f92-4bd8-bf65-5484176a5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_gdf[[\"inside_hassi\", \"inside_marcellus\", \"inside_permian\", \"inside_val_regions\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe293f-3fab-479b-b6ac-264345bf9b79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_ids = mgrs_gdf[mgrs_gdf[\"inside_val_regions\"] == True].index.tolist()\n",
    "len(val_ids), len(set(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b857b3a-7d64-40e1-8cdc-16ad40725315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "we_want_ids_per_mgrs = 9\n",
    "already_sampled_dates = collections.defaultdict(list)\n",
    "mgrs_additional = []\n",
    "cols = [\n",
    "    \"Name\",\n",
    "    \"geometry\",\n",
    "    \"EI_region\",\n",
    "    \"OG_producing\",\n",
    "    \"inside_val_regions\",\n",
    "    \"inside_hassi\",\n",
    "    \"inside_marcellus\",\n",
    "    \"inside_permian\",\n",
    "]\n",
    "for mgrs in mgrs_gdf[mgrs_gdf[\"inside_hassi\"] == True].index.tolist():\n",
    "    already_sampled_dates[mgrs] = mgrs_selected[mgrs_selected[\"mgrs\"] == mgrs][\"datetime\"].tolist()\n",
    "    print(f\"{mgrs}: {len(already_sampled_dates[mgrs])=}\")\n",
    "    how_many = we_want_ids_per_mgrs - len(already_sampled_dates[mgrs])\n",
    "    if how_many > 0:\n",
    "        for k in range(how_many):\n",
    "            mgrs_additional.append(mgrs_gdf[mgrs_gdf.index == mgrs].reset_index()[cols])\n",
    "for idx, row in (\n",
    "    mgrs_selected[mgrs_selected[\"inside_marcellus\"] == True][\"mgrs\"].value_counts().reset_index().iterrows()\n",
    "):\n",
    "    mgrs = row[\"mgrs\"]\n",
    "    already_sampled_dates[mgrs] = mgrs_selected[mgrs_selected[\"mgrs\"] == mgrs][\"datetime\"].tolist()\n",
    "    print(f\"{mgrs}: {len(already_sampled_dates[mgrs])=}\")\n",
    "    how_many = we_want_ids_per_mgrs - len(already_sampled_dates[mgrs])\n",
    "    if how_many > 0:\n",
    "        for k in range(how_many):\n",
    "            mgrs_additional.append(mgrs_gdf[mgrs_gdf.index == mgrs].reset_index()[cols])\n",
    "for idx, row in mgrs_selected[mgrs_selected[\"inside_permian\"] == True][\"mgrs\"].value_counts().reset_index().iterrows():\n",
    "    mgrs = row[\"mgrs\"]\n",
    "    already_sampled_dates[mgrs] = mgrs_selected[mgrs_selected[\"mgrs\"] == mgrs][\"datetime\"].tolist()\n",
    "    print(f\"{mgrs}: {len(already_sampled_dates[mgrs])=}\")\n",
    "    how_many = we_want_ids_per_mgrs - len(already_sampled_dates[mgrs])\n",
    "    if how_many > 0:\n",
    "        for k in range(how_many):\n",
    "            mgrs_additional.append(mgrs_gdf[mgrs_gdf.index == mgrs].reset_index()[cols])\n",
    "mgrs_additional = pd.concat(mgrs_additional, ignore_index=True)\n",
    "mgrs_additional.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23316937-4e67-4b42-97da-2ef8f242164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_additional[[\"inside_hassi\", \"inside_marcellus\", \"inside_permian\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56185be-7096-42ce-86c3-5d068a72355a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Sampling Logic Helper Functions ###\n",
    "def get_seed_from_string(s: str) -> int:\n",
    "    \"\"\"Create reproducible seed from string using hashlib.\"\"\"\n",
    "    return 1 + int(hashlib.sha256(s.encode()).hexdigest(), 16) % (2**32)\n",
    "\n",
    "\n",
    "def sample_dates_for_mgrs(mgrs_selected_df: pd.DataFrame, debug: bool = True, verbose: bool = True):\n",
    "    # Output lists\n",
    "    selected_prod_items = []\n",
    "    nodata_percs = []\n",
    "    additional_id_due_to_high_nodata_clouds = []\n",
    "\n",
    "    def default_one():\n",
    "        return 1\n",
    "\n",
    "    mgrs_counter = collections.defaultdict(default_one)\n",
    "    additional_id_counter = collections.defaultdict(int)\n",
    "    already_sampled_cloud_covers = collections.defaultdict(list)\n",
    "    already_sampled_nodatas = collections.defaultdict(list)\n",
    "    start_year = 2016\n",
    "    end_year = 2025\n",
    "\n",
    "    for idx, row in tqdm.tqdm(mgrs_selected_df.iterrows(), total=len(mgrs_selected_df)):\n",
    "        # Ensure that for any given MGRS tile, it will always select the same date,\n",
    "        # so the output is reproducible\n",
    "        mgrs = row[\"Name\"]\n",
    "        mgrs_area = row[\"geometry\"].area\n",
    "\n",
    "        # Create reproducible RNG for this MGRS tile and counter combination\n",
    "        seed = get_seed_from_string(f\"{mgrs}_{mgrs_counter[mgrs]}\")\n",
    "        rng = np.random.default_rng(seed + idx)\n",
    "\n",
    "        print(f\"{mgrs}: Sample randomly from {start_year}-05-01 to {end_year}-12-31\")\n",
    "\n",
    "        # Initialize nodata and cloud cover thresholds\n",
    "        max_nodata_px_perc = 95\n",
    "        max_cloud_cover = 98\n",
    "        for _ in range(100):  # 100 retries\n",
    "            while True:\n",
    "                try:\n",
    "                    # Attempt to sample a random item\n",
    "                    item = random_item_for_MGRS(\n",
    "                        mgrs, rng=rng, max_cloud_cover=max_cloud_cover, start_year=start_year, end_year=end_year\n",
    "                    )\n",
    "                    id_ = item.to_dict()[\"properties\"][\"s2:product_uri\"].replace(\".SAFE\", \"\")\n",
    "                    if item.datetime in already_sampled_dates[mgrs]:\n",
    "                        print(f\"{mgrs} - {id_} --> DONT USE (we have this date already)\")\n",
    "                        print(already_sampled_dates[mgrs])\n",
    "                        continue\n",
    "                    break\n",
    "                except Exception as err:\n",
    "                    print(err)\n",
    "                    import traceback\n",
    "\n",
    "                    print(traceback.print_exception(None, err, err.__traceback__))\n",
    "\n",
    "            # Calculate nodata percentage and cloud cover\n",
    "            nodata_perc = abs(100 - 100 * item_area(item) / mgrs_area)\n",
    "            s2_date = id_.split(\"_\")[2]\n",
    "            s2_date = f\"{s2_date[:4]}-{s2_date[4:6]}-{s2_date[6:8]}\"\n",
    "            cloud_perc = item.ext.eo.cloud_cover\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"{mgrs} on {s2_date} has ({nodata_perc:6.1f}% nodata) and {cloud_perc:6.1f}% clouds ({id_})\",\n",
    "                    end=\"\",\n",
    "                )\n",
    "            if nodata_perc > max_nodata_px_perc:\n",
    "                print(\" --> DONT USE (too much nodata)\")\n",
    "                continue\n",
    "            if item.datetime in already_sampled_dates[mgrs]:\n",
    "                print(\" --> DONT USE (we have this date already)\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\" --> USE\")\n",
    "                selected_prod_items.append(item)\n",
    "                nodata_percs.append(nodata_perc)\n",
    "                additional_id_due_to_high_nodata_clouds.append(False)\n",
    "                already_sampled_dates[mgrs].append(item.datetime)\n",
    "                already_sampled_cloud_covers[mgrs].append(cloud_perc)\n",
    "                already_sampled_nodatas[mgrs].append(nodata_perc)\n",
    "\n",
    "                # sample another ID depending on how much nodata/clouds we got\n",
    "                # Example: 90% nodata + 50% clouds ==> 95% of pixels are obscured ==> 95% to sample another ID with nodata max 40%/clouds max 40%\n",
    "                random_ = rng.random() * 100\n",
    "                if debug:\n",
    "                    print(f\"random: {random_:.2f} vs {nodata_perc + (1 - nodata_perc/100) * cloud_perc:.2f}\")\n",
    "                if random_ < nodata_perc + (1 - nodata_perc / 100) * cloud_perc:\n",
    "                    max_nodata_px_perc = 40\n",
    "                    max_cloud_cover = 40\n",
    "                    while True:\n",
    "                        try:\n",
    "                            item = random_item_for_MGRS(\n",
    "                                mgrs, rng=rng, max_cloud_cover=max_cloud_cover, start_year=start_year, end_year=end_year\n",
    "                            )\n",
    "                            id_ = item.to_dict()[\"properties\"][\"s2:product_uri\"].replace(\".SAFE\", \"\")\n",
    "                            if item.datetime in already_sampled_dates[mgrs]:\n",
    "                                print(f\"{mgrs} - {id_} --> DONT USE (we have this date already)\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                nodata_perc = abs(100 - 100 * item_area(item) / mgrs_area)\n",
    "                                s2_date = id_.split(\"_\")[2]\n",
    "                                s2_date = f\"{s2_date[:4]}-{s2_date[4:6]}-{s2_date[6:8]}\"\n",
    "                                cloud_perc = item.ext.eo.cloud_cover\n",
    "                                if verbose:\n",
    "                                    print(\n",
    "                                        f\"  {mgrs} on {s2_date} has ({nodata_perc:6.1f}% nodata) and {cloud_perc:6.1f}% clouds ({id_})\",\n",
    "                                        end=\"\",\n",
    "                                    )\n",
    "                                if nodata_perc > max_nodata_px_perc:\n",
    "                                    print(\" --> DONT USE (too much nodata)\")\n",
    "                                    continue\n",
    "                            break\n",
    "                        except Exception as err:\n",
    "                            print(err)\n",
    "                            import traceback\n",
    "\n",
    "                            print(traceback.print_exception(None, err, err.__traceback__))\n",
    "                    print(\" --> USE\")\n",
    "                    additional_id_counter[mgrs] += 1\n",
    "                    selected_prod_items.append(item)\n",
    "                    nodata_percs.append(nodata_perc)\n",
    "                    additional_id_due_to_high_nodata_clouds.append(True)\n",
    "                    already_sampled_dates[mgrs].append(item.datetime)\n",
    "                    already_sampled_cloud_covers[mgrs].append(cloud_perc)\n",
    "                    already_sampled_nodatas[mgrs].append(nodata_perc)\n",
    "            break\n",
    "        else:\n",
    "            raise ValueError(\"couldn't find big enough tile after 100 tries\")\n",
    "    return selected_prod_items, nodata_percs, additional_id_due_to_high_nodata_clouds\n",
    "\n",
    "\n",
    "selected_mgrs_additional, nodata_percs, additional_id_due_to_high_nodata_clouds = sample_dates_for_mgrs(\n",
    "    mgrs_additional, debug=True, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bd904-0626-4490-aa8b-d7971d79f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to df\n",
    "additional_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"mgrs\": item.properties[\"s2:mgrs_tile\"].zfill(5),\n",
    "            \"date\": item.datetime.date().isoformat(),\n",
    "            \"datetime\": item.datetime.isoformat(),\n",
    "            \"ID\": item.to_dict()[\"properties\"][\"s2:product_uri\"].replace(\".SAFE\", \"\"),\n",
    "            \"cloud_percent\": item.ext.eo.cloud_cover,\n",
    "        }\n",
    "        for item in selected_mgrs_additional\n",
    "    ]\n",
    ").sort_values([\"mgrs\", \"date\"])\n",
    "additional_df[\"selected_due_to_high_nodata_clouds\"] = additional_id_due_to_high_nodata_clouds\n",
    "additional_df[\"nodata_perc\"] = [abs(np.round(k, 3)) for k in nodata_percs]\n",
    "print(additional_df.shape)\n",
    "additional_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f40aa1-1972-4afd-b56e-73b14308fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDs we have already sampled\n",
    "mgrs_selected.loc[\n",
    "    mgrs_selected[\"inside_val_regions\"] == True,\n",
    "    [\"mgrs\", \"date\", \"datetime\", \"ID\", \"cloud_percent\", \"selected_due_to_high_nodata_clouds\", \"nodata_perc\"],\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71754de9-f73d-4be8-befe-7819794fa289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new IDs to get the dataframe of hassi/marcellus/permian IDs we will use for train/val\n",
    "val_mgrs = pd.concat(\n",
    "    (\n",
    "        mgrs_selected.loc[\n",
    "            mgrs_selected[\"inside_val_regions\"] == True,\n",
    "            [\"mgrs\", \"date\", \"datetime\", \"ID\", \"cloud_percent\", \"selected_due_to_high_nodata_clouds\", \"nodata_perc\"],\n",
    "        ].copy(),\n",
    "        additional_df,\n",
    "    ),\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "val_mgrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d32d1-89f6-41ce-bf16-73e771a877e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"EI_region\" not in val_mgrs.columns:\n",
    "    # Join with mgrs_gdf to get region col\n",
    "    val_mgrs = pd.merge(val_mgrs, mgrs_gdf[[\"EI_region\", \"geometry\"]], left_on=\"mgrs\", right_index=True, how=\"left\")\n",
    "\n",
    "val_mgrs = gpd.GeoDataFrame(val_mgrs, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "val_mgrs[\"inside_val_regions\"] = x_intersects_any_in_y(val_mgrs.geometry, regions.geometry)\n",
    "val_mgrs[\"inside_hassi\"] = x_intersects_any_in_y(val_mgrs.geometry, regions[regions[\"region\"] == \"hassi\"].geometry)\n",
    "val_mgrs[\"inside_marcellus\"] = x_intersects_any_in_y(\n",
    "    val_mgrs.geometry, regions[regions[\"region\"] == \"marcellus\"].geometry\n",
    ")\n",
    "val_mgrs[\"inside_permian\"] = x_intersects_any_in_y(val_mgrs.geometry, regions[regions[\"region\"] == \"permian\"].geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd8774-2a41-4e74-9f4d-b492accfc0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mgrs[\"inside_val_regions\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f0e89-9914-46b1-8176-364afa254a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_mgrs.loc[val_mgrs[\"inside_hassi\"] == True, \"mgrs\"].nunique())\n",
    "val_mgrs.loc[val_mgrs[\"inside_hassi\"] == True, \"mgrs\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad18c7f-f5bc-4dab-9aed-05468546b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_mgrs.loc[val_mgrs[\"inside_marcellus\"] == True, \"mgrs\"].nunique())\n",
    "val_mgrs.loc[val_mgrs[\"inside_marcellus\"] == True, \"mgrs\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdceb774-7229-4dc2-a26a-876ff7ce58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_mgrs.loc[val_mgrs[\"inside_permian\"] == True, \"mgrs\"].nunique())\n",
    "val_mgrs.loc[val_mgrs[\"inside_permian\"] == True, \"mgrs\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd92907-a12c-460a-a59d-436d7dd25749",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mgrs[[\"inside_hassi\", \"inside_marcellus\", \"inside_permian\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc1595-a3a9-4c23-88e9-d689b4c7a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final samples as csv\n",
    "val_mgrs.to_csv(\n",
    "    \"../src/data/tiles/s2/csv_files/2025_02_19_MGRS_with_IDs_hassi_marc_permian_699.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316cfef-d887-47a5-a3e4-0d10575cdb4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Split into Val/Train\n",
    "- Validation = 50% of Hassi/Marcellus/Permian\n",
    "- Train = other 50% of H/M/P + all other global IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a392644-86a4-47e2-8053-ece3096171dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mgrs = pd.read_csv(\"../src/data/tiles/s2/csv_files/2025_02_19_MGRS_with_IDs_hassi_marc_permian_699.csv\")\n",
    "print(val_mgrs.shape)\n",
    "train_mgrs = pd.read_csv(\"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_with_metadata.csv\")\n",
    "print(train_mgrs.shape)\n",
    "train_mgrs = train_mgrs[~train_mgrs[\"mgrs\"].isin(val_mgrs[\"mgrs\"].tolist())]\n",
    "print(train_mgrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac919c8-f37d-4e16-9b9a-33645ea0e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATIFY NODATA/CLOUDS/% PER REGION BETWEEN TRAIN AND VAL\n",
    "best_error = 10000\n",
    "best_seed = 0\n",
    "for seed in range(0, 10000):\n",
    "    val_region_mgrs = sorted(val_mgrs[\"mgrs\"].unique())\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(val_region_mgrs)\n",
    "    df_train = val_mgrs[val_mgrs[\"mgrs\"].isin(val_region_mgrs[: int(0.5 * len(val_region_mgrs))])][\n",
    "        [\n",
    "            \"mgrs\",\n",
    "            \"date\",\n",
    "            \"cloud_percent\",\n",
    "            \"nodata_perc\",\n",
    "            \"EI_region\",\n",
    "            \"inside_hassi\",\n",
    "            \"inside_marcellus\",\n",
    "            \"inside_permian\",\n",
    "        ]\n",
    "    ].copy()\n",
    "    df_val = val_mgrs[val_mgrs[\"mgrs\"].isin(val_region_mgrs[int(0.5 * len(val_region_mgrs)) :])][\n",
    "        [\n",
    "            \"mgrs\",\n",
    "            \"date\",\n",
    "            \"cloud_percent\",\n",
    "            \"nodata_perc\",\n",
    "            \"EI_region\",\n",
    "            \"inside_hassi\",\n",
    "            \"inside_marcellus\",\n",
    "            \"inside_permian\",\n",
    "        ]\n",
    "    ].copy()\n",
    "    hassi_error = np.std([df_val[\"inside_hassi\"].mean(), df_train[\"inside_hassi\"].mean()]) / np.mean(\n",
    "        [df_val[\"inside_hassi\"].mean(), df_train[\"inside_hassi\"].mean()]\n",
    "    )\n",
    "    marcellus_error = np.std([df_val[\"inside_marcellus\"].mean(), df_train[\"inside_marcellus\"].mean()]) / np.mean(\n",
    "        [df_val[\"inside_marcellus\"].mean(), df_train[\"inside_marcellus\"].mean()]\n",
    "    )\n",
    "    permian_error = np.std([df_val[\"inside_permian\"].mean(), df_train[\"inside_permian\"].mean()]) / np.mean(\n",
    "        [df_val[\"inside_permian\"].mean(), df_train[\"inside_permian\"].mean()]\n",
    "    )\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_hassi\"] == True]\n",
    "    mean_clouds_val = df_val_[\"cloud_percent\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_hassi\"] == True]\n",
    "    mean_clouds_train = df_train_[\"cloud_percent\"].mean()\n",
    "    hassi_clouds_error = np.std([mean_clouds_val, mean_clouds_train]) / np.mean([mean_clouds_val, mean_clouds_train])\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_marcellus\"] == True]\n",
    "    mean_clouds_val = df_val_[\"cloud_percent\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_marcellus\"] == True]\n",
    "    mean_clouds_train = df_train_[\"cloud_percent\"].mean()\n",
    "    marcellus_clouds_error = np.std([mean_clouds_val, mean_clouds_train]) / np.mean(\n",
    "        [mean_clouds_val, mean_clouds_train]\n",
    "    )\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_permian\"] == True]\n",
    "    mean_clouds_val = df_val_[\"cloud_percent\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_permian\"] == True]\n",
    "    mean_clouds_train = df_train_[\"cloud_percent\"].mean()\n",
    "    permian_clouds_error = np.std([mean_clouds_val, mean_clouds_train]) / np.mean([mean_clouds_val, mean_clouds_train])\n",
    "\n",
    "    # NODATA\n",
    "    df_val_ = df_val[df_val[\"inside_hassi\"] == True]\n",
    "    mean_clouds_val = df_val_[\"nodata_perc\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_hassi\"] == True]\n",
    "    mean_clouds_train = df_train_[\"nodata_perc\"].mean()\n",
    "    # print(f'Nodata H: {mean_clouds_val:.3f} vs {mean_clouds_train:.3f}')\n",
    "    hassi_nodata_error = np.std([mean_clouds_val, mean_clouds_train]) / np.mean([mean_clouds_val, mean_clouds_train])\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_marcellus\"] == True]\n",
    "    mean_clouds_val = df_val_[\"nodata_perc\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_marcellus\"] == True]\n",
    "    mean_clouds_train = df_train_[\"nodata_perc\"].mean()\n",
    "    # print(f'Nodata M: {mean_clouds_val:.3f} vs {mean_clouds_train:.3f}')\n",
    "    marcellus_nodata_error = np.std([mean_clouds_val, mean_clouds_train]) / np.mean(\n",
    "        [mean_clouds_val, mean_clouds_train]\n",
    "    )\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_permian\"] == True]\n",
    "    mean_clouds_val = df_val_[\"nodata_perc\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_permian\"] == True]\n",
    "    mean_clouds_train = df_train_[\"nodata_perc\"].mean()\n",
    "    # print(f'Nodata P: {mean_clouds_val:.3f} vs {mean_clouds_train:.3f}')\n",
    "    permian_nodata_error = np.std([mean_clouds_val, mean_clouds_train]) / np.mean([mean_clouds_val, mean_clouds_train])\n",
    "\n",
    "    perc_mult = 4\n",
    "    total_error = (\n",
    "        perc_mult * hassi_error\n",
    "        + perc_mult * marcellus_error\n",
    "        + perc_mult * permian_error\n",
    "        + hassi_clouds_error\n",
    "        + marcellus_clouds_error\n",
    "        + permian_clouds_error\n",
    "        + hassi_nodata_error\n",
    "        + marcellus_nodata_error\n",
    "        + permian_nodata_error\n",
    "    )\n",
    "    if total_error < best_error:\n",
    "        best_error = total_error\n",
    "        best_seed = seed\n",
    "        print(\n",
    "            f\"{seed:5}: {total_error:.2f} = Hassi% {perc_mult*hassi_error:.2f} + Marcellus% {perc_mult*marcellus_error:.2f} + Permian% {perc_mult*permian_error:.2f} \"\n",
    "            f\"-- CLOUDS H {hassi_clouds_error:.2f} + M {marcellus_clouds_error:.2f} + P {permian_clouds_error:.2f}\"\n",
    "            f\"-- NODATA H {hassi_nodata_error:.2f} + M {marcellus_nodata_error:.2f} + P {permian_nodata_error:.2f}\"\n",
    "        )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff4600-c5be-4235-b90c-7a2a945f4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT STATS FOR BEST SPLIT\n",
    "best_seed = 6002\n",
    "for seed in [best_seed]:\n",
    "    print(f\"Seed {seed}\")\n",
    "    val_region_mgrs = sorted(val_mgrs[\"mgrs\"].unique())\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(val_region_mgrs)\n",
    "    df_train = val_mgrs[val_mgrs[\"mgrs\"].isin(val_region_mgrs[: int(0.5 * len(val_region_mgrs))])].copy()\n",
    "    df_val = val_mgrs[val_mgrs[\"mgrs\"].isin(val_region_mgrs[int(0.5 * len(val_region_mgrs)) :])].copy()\n",
    "    print(\n",
    "        f'Val   H: {df_val[\"inside_hassi\"].mean():.3f}, M: {df_val[\"inside_marcellus\"].mean():.3f}, P: {df_val[\"inside_permian\"].mean():.3f}'\n",
    "    )\n",
    "    print(\n",
    "        f'Train H: {df_train[\"inside_hassi\"].mean():.3f}, M: {df_train[\"inside_marcellus\"].mean():.3f}, P: {df_train[\"inside_permian\"].mean():.3f}'\n",
    "    )\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_hassi\"] == True]\n",
    "    mean_clouds_val = df_val_[\"cloud_percent\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_hassi\"] == True]\n",
    "    mean_clouds_train = df_train_[\"cloud_percent\"].mean()\n",
    "    print(f\"Clouds H: {mean_clouds_val:.3f} vs {mean_clouds_train:.3f}\")\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_marcellus\"] == True]\n",
    "    mean_clouds_val = df_val_[\"cloud_percent\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_marcellus\"] == True]\n",
    "    mean_clouds_train = df_train_[\"cloud_percent\"].mean()\n",
    "    print(f\"Clouds M: {mean_clouds_val:.3f} vs {mean_clouds_train:.3f}\")\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_permian\"] == True]\n",
    "    mean_clouds_val = df_val_[\"cloud_percent\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_permian\"] == True]\n",
    "    mean_clouds_train = df_train_[\"cloud_percent\"].mean()\n",
    "    print(f\"Clouds P: {mean_clouds_val:.3f} vs {mean_clouds_train:.3f}\")\n",
    "\n",
    "    # NODATA\n",
    "    df_val_ = df_val[df_val[\"inside_hassi\"] == True]\n",
    "    mean_clouds_val = df_val_[\"nodata_perc\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_hassi\"] == True]\n",
    "    mean_clouds_train = df_train_[\"nodata_perc\"].mean()\n",
    "    print(f\"Nodata H: {mean_clouds_val:.3f} vs {mean_clouds_train:.3f}\")\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_marcellus\"] == True]\n",
    "    mean_clouds_val = df_val_[\"nodata_perc\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_marcellus\"] == True]\n",
    "    mean_clouds_train = df_train_[\"nodata_perc\"].mean()\n",
    "    print(f\"Nodata M: {mean_clouds_val:.3f} vs {mean_clouds_train:.3f}\")\n",
    "\n",
    "    df_val_ = df_val[df_val[\"inside_permian\"] == True]\n",
    "    mean_clouds_val = df_val_[\"nodata_perc\"].mean()\n",
    "    df_train_ = df_train[df_train[\"inside_permian\"] == True]\n",
    "    mean_clouds_train = df_train_[\"nodata_perc\"].mean()\n",
    "    print(f\"Nodata P: {mean_clouds_val:.3f} vs {mean_clouds_train:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e160164b-aa10-4f37-bd44-dd31efce5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef92033-f59c-4ae2-af7a-8c598f338487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare global IDs and val region IDs selected for train to merge together\n",
    "for col in [\"OG_producing\", \"overlapping_area\", \"omni_nodata_perc\", \"omni_cloud_perc\", \"omni_cloud_shadow_perc\"]:\n",
    "    if col in train_mgrs.columns:\n",
    "        train_mgrs = train_mgrs.drop(col, axis=1)\n",
    "for col in [\"inside_val_regions\", \"inside_hassi\", \"inside_marcellus\", \"inside_permian\"]:\n",
    "    train_mgrs[col] = False\n",
    "print(train_mgrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f08ac2-697c-4ebb-a121-f08d988c0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final = pd.concat((train_mgrs, df_train), axis=0, ignore_index=True)\n",
    "df_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972ac29-ced2-4797-8a4b-f5893d33ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final[\"mgrs\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04ae9f-cc06-4ce2-be23-14af384f52aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_val.shape, df_val[\"mgrs\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030300b3-8384-49f3-93fc-019fd2aa8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final samples as csv\n",
    "df_train_final.to_csv(\n",
    "    f\"../src/data/tiles/s2/csv_files/2025_02_19_MGRS_with_IDs_within_OG_train_{len(df_train_final)}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "df_val.to_csv(\n",
    "    f\"../src/data/tiles/s2/csv_files/2025_02_19_MGRS_with_IDs_within_OG_val_{len(df_val)}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc4a2d-3dda-44d6-8c89-48964cb0479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gdf = df_train_final.copy(deep=True)\n",
    "train_gdf[\"geometry\"] = train_gdf[\"geometry\"].apply(wkt.loads)\n",
    "train_gdf = gpd.GeoDataFrame(train_gdf, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "val_gdf = df_val.copy(deep=True)\n",
    "val_gdf[\"geometry\"] = val_gdf[\"geometry\"].apply(wkt.loads)\n",
    "val_gdf = gpd.GeoDataFrame(val_gdf, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "print(val_gdf.shape)\n",
    "\n",
    "m = train_gdf.explore(color=\"green\", name=\"Train\", legend=True)\n",
    "m = val_gdf.explore(m=m, color=\"red\", name=\"Val\", legend=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d022ac2-6d40-4557-a162-0528d5ae53ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (OLD: Split into Train/Val Sets)\n",
    "- Stratify by region%, seasonal distribution, OMNI cloud, OMNI cloud shadow, nodata, producing area overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641269d-83f0-46b7-9404-31f410edc3ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Add OmniCloud cloud% and cloud shadow% metadata for each ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21253bb-64f9-4775-a8ca-307e1c859f5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mgrs_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3f628-81cc-41ae-9f12-464eac368594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Add all relevant metadata to each row in csv ###\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from src.azure_wrap.ml_client_utils import download_blob_directly, initialize_blob_service_client, initialize_ml_client\n",
    "\n",
    "ml_client = initialize_ml_client()\n",
    "abs_client = initialize_blob_service_client(ml_client)\n",
    "\n",
    "mgrs_selected = pd.read_csv(\"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_with_metadata.csv\")\n",
    "\n",
    "if \"EI_region\" not in mgrs_selected.columns:\n",
    "    # Join with mgrs_gdf to get region col\n",
    "    mgrs_gdf_reset = mgrs_gdf.reset_index()\n",
    "    mgrs_selected = pd.merge(\n",
    "        mgrs_selected, mgrs_gdf[[\"EI_region\", \"geometry\"]], left_on=\"mgrs\", right_index=True, how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c0112-3d73-4ed0-99ac-a2144c1d4ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! convert both gdf to area CRS for area to be meaningful\n",
    "mgrs_equal_area = mgrs_gdf.to_crs(\"EPSG:6933\")  # Mollweide projection\n",
    "producing_equal_area = producing_by_region.to_crs(\"EPSG:6933\")\n",
    "\n",
    "producing_union_equal_area = producing_equal_area.unary_union\n",
    "mgrs_equal_area[\"overlapping_area\"] = mgrs_equal_area.geometry.intersection(producing_union_equal_area).area\n",
    "\n",
    "# Transfer the calculated area back to original GDF\n",
    "mgrs_gdf = mgrs_gdf.assign(overlapping_area=mgrs_equal_area[\"overlapping_area\"])\n",
    "\n",
    "# Merge overlap data into mgrs_selected\n",
    "mgrs_selected = pd.merge(\n",
    "    mgrs_selected, mgrs_gdf[[\"OG_producing\", \"overlapping_area\"]], left_on=\"mgrs\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "def return_omnicloud_for_s2_id(s2_id):\n",
    "    s2_id_date = s2_id.split(\"_\")[2].split(\"T\")[0]\n",
    "    year = str(int(s2_id_date[:4]))\n",
    "    month = str(int(s2_id_date[4:6]))\n",
    "    day = str(int(s2_id_date[6:8]))\n",
    "    tile = s2_id.split(\"_\")[-2][1:]\n",
    "    tile_part1 = tile[:2]\n",
    "    tile_part2 = tile[2:3]\n",
    "    tile_part3 = tile[3:5]\n",
    "\n",
    "    prefix_parts = [\"tiles\", tile_part1, tile_part2, tile_part3, year, month, day, \"0\"]\n",
    "    l1c_prefix = \"/\".join(prefix_parts)\n",
    "\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_path = Path(temp_dir)\n",
    "            download_blob_directly(\n",
    "                blob_name=f\"{l1c_prefix}/OmniCloud.tif\",\n",
    "                # blob_name=f\"{l1c_prefix}/SCL.tif\",\n",
    "                local_download_filepath=temp_path / \"tmp.tif\",\n",
    "                blob_service_client=abs_client,\n",
    "                container_name=\"l1c-data\",\n",
    "            )\n",
    "            # print(f\"Downloading OmniCloud for {s2_id} from {l1c_prefix}/OmniCloud.tif\")\n",
    "            with rasterio.open(temp_path / \"tmp.tif\") as ds:\n",
    "                probs = ds.read()\n",
    "                # print(probs.shape)\n",
    "                print(f\"OmniCloud exists for {s2_id} at {l1c_prefix}/OmniCloud.tif\")\n",
    "                return probs\n",
    "    except Exception as err:\n",
    "        print(f\"OmniCloud is missing for {s2_id} at {l1c_prefix}/OmniCloud.tif\")\n",
    "        print(err)\n",
    "        # import traceback\n",
    "        # print(traceback.print_exception(None, err, err.__traceback__))\n",
    "\n",
    "\n",
    "def process_omnicloud(s2_id):\n",
    "    \"\"\"Process OmniCloud data for a single S2 ID and return metrics\"\"\"\n",
    "    try:\n",
    "        probs = return_omnicloud_for_s2_id(s2_id)\n",
    "        if probs is None:\n",
    "            return pd.Series({\"omni_nodata_perc\": np.nan, \"omni_cloud_perc\": np.nan, \"omni_cloud_shadow_perc\": np.nan})\n",
    "\n",
    "        probs = probs.astype(np.float32)\n",
    "        valid_px = float((probs[0] != 255).sum())\n",
    "        nodata_px = float((probs[0] == 255).sum())\n",
    "        all_px = probs.shape[1] * probs.shape[2]\n",
    "        probs[probs == 255] = np.nan\n",
    "\n",
    "        omnicloud_cloud_t = 35\n",
    "        omnicloud_shadow_t = 30\n",
    "        # Calculate cloud and shadow masks\n",
    "        clouds_omni = (probs[1, :, :] + probs[2, :, :] > omnicloud_cloud_t).astype(np.uint8)\n",
    "        shadows_omni = ((probs[3, :, :] > omnicloud_shadow_t) & (clouds_omni != 1)).astype(np.uint8)\n",
    "\n",
    "        # Calculate percentages\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"omni_nodata_perc\": 100 * nodata_px / all_px,\n",
    "                \"omni_cloud_perc\": 100 * clouds_omni.sum() / valid_px,\n",
    "                \"omni_cloud_shadow_perc\": 100 * shadows_omni.sum() / valid_px,\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {s2_id}: {e!s}\")\n",
    "        return pd.Series({\"omni_nodata_perc\": np.nan, \"omni_cloud_perc\": np.nan, \"omni_cloud_shadow_perc\": np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d53023-5330-4a71-9e59-d8b635c53930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add OMNI metrics\n",
    "omni_results = mgrs_selected[\"ID\"].apply(process_omnicloud)\n",
    "\n",
    "mgrs_selected[\"omni_nodata_perc\"] = omni_results[\"omni_nodata_perc\"]\n",
    "mgrs_selected[\"omni_cloud_perc\"] = omni_results[\"omni_cloud_perc\"]\n",
    "mgrs_selected[\"omni_cloud_shadow_perc\"] = omni_results[\"omni_cloud_shadow_perc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e62df-cb68-4d50-839d-cdf0de070898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill NaNs with regional means\n",
    "for col in [\"omni_nodata_perc\", \"omni_cloud_perc\", \"omni_cloud_shadow_perc\"]:\n",
    "    mgrs_selected[col] = mgrs_selected.groupby(\"EI_region\")[col].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "mgrs_selected.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18ffb9-a439-4709-bb54-5e4290501208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with metadata so we don't have to recompute the OMNI metrics\n",
    "mgrs_selected.to_csv(\n",
    "    \"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_with_metadata.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b8287-cef9-49e0-a10f-42886dc4fd50",
   "metadata": {},
   "source": [
    "#### Build stratification sampling logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d48865-fcaf-4311-8808-0683a0f401fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs_selected = pd.read_csv(\"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_with_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28419c0-85d3-4a81-89b2-58799c638241",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Group by EI_region and calculate baseline statistics\n",
    "mgrs_selected[\"datetime\"] = pd.to_datetime(mgrs_selected[\"datetime\"], format=\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"summer\"\n",
    "    else:\n",
    "        return \"fall\"\n",
    "\n",
    "\n",
    "def calculate_year_season_distribution(dates):\n",
    "    return dates.apply(lambda x: f\"{x.year}-{get_season(x.month)}\").value_counts(normalize=True).to_dict()\n",
    "\n",
    "\n",
    "# NOTE: use mean because median only focuses on the midpoint of the data\n",
    "# mean captures distributional info\n",
    "grouped_stats = (\n",
    "    mgrs_selected.groupby(\"EI_region\")\n",
    "    .agg(\n",
    "        overlapping_area_mean=(\"overlapping_area\", \"mean\"),\n",
    "        temporal_distribution=(\n",
    "            \"datetime\",\n",
    "            lambda x: x.apply(lambda date: get_season(date.month)).value_counts(normalize=True).to_dict(),\n",
    "        ),\n",
    "        omni_cloud_perc_mean=(\"omni_cloud_perc\", \"mean\"),\n",
    "        omni_cloud_perc_std=(\"omni_cloud_perc\", \"std\"),\n",
    "        omni_cloud_shadow_perc_mean=(\"omni_cloud_shadow_perc\", \"mean\"),\n",
    "        omni_cloud_shadow_perc_std=(\"omni_cloud_shadow_perc\", \"std\"),\n",
    "        omni_nodata_perc_mean=(\"omni_nodata_perc\", \"mean\"),\n",
    "        omni_nodata_perc_std=(\"omni_nodata_perc\", \"std\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "grouped_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689811f-3fde-4cd7-a82a-f590a64b642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distribution_metrics(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate distribution metrics for a dataset\n",
    "    Returns dict of metrics by region\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    # Calculate region proportions in split dataset\n",
    "    region_props = df[\"EI_region\"].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    # Calculate metrics for each region\n",
    "    for region in df[\"EI_region\"].unique():\n",
    "        region_df = df[df[\"EI_region\"] == region]\n",
    "\n",
    "        # Create year-season column\n",
    "        def get_season(month):\n",
    "            if month in [12, 1, 2]:\n",
    "                return \"winter\"\n",
    "            elif month in [3, 4, 5]:\n",
    "                return \"spring\"\n",
    "            elif month in [6, 7, 8]:\n",
    "                return \"summer\"\n",
    "            else:\n",
    "                return \"fall\"\n",
    "\n",
    "        # Create season distribution, only using season instead of year-season\n",
    "        temporal_dist = (\n",
    "            region_df[\"datetime\"].apply(lambda x: get_season(x.month)).value_counts(normalize=True).to_dict()\n",
    "        )\n",
    "        metrics[region] = {\n",
    "            \"proportion\": region_props[region],\n",
    "            \"temporal_dist\": temporal_dist,\n",
    "            \"mean_omni_cloud\": region_df[\"omni_cloud_perc\"].mean(),\n",
    "            \"mean_omni_cloud_shadow\": region_df[\"omni_cloud_shadow_perc\"].mean(),\n",
    "            \"mean_omni_nodata\": region_df[\"omni_nodata_perc\"].mean(),\n",
    "            \"mean_overlap_area\": region_df[\"overlapping_area\"].mean(),\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def score_split(\n",
    "    master_df: pd.DataFrame,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    metric_weights: dict,\n",
    "    verbose=False,\n",
    "    potency=1.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Score how well the split maintains distributions.\n",
    "    Lower score is better.\n",
    "    \"\"\"\n",
    "    train_metrics = calculate_distribution_metrics(train_df)\n",
    "    val_metrics = calculate_distribution_metrics(val_df)\n",
    "\n",
    "    scores = {\"proportion\": 0, \"temporal\": 0, \"cloud\": 0, \"shadow\": 0, \"nodata\": 0, \"overlap\": 0}\n",
    "\n",
    "    all_regions = set(train_metrics.keys()) | set(val_metrics.keys())\n",
    "\n",
    "    # These are region-specific thresholds for each metric to help ensure similar\n",
    "    # representation of each region across the train/val/test sets. The thresholds\n",
    "    # are informed by the baseline stats in `grouped_stats` and are iteratively\n",
    "    # adjusted to achieve the best split.\n",
    "\n",
    "    # A stricter threshold (e.g., 0.10 for mean_cloud in the USA) forces the mean\n",
    "    # cloud coverage to be nearly identical across train/val/test splits, ensuring\n",
    "    # consistent representation for regions where variability is naturally low.\n",
    "    # In contrast, a higher threshold (e.g., 0.20 for mean_cloud in default)\n",
    "    # indicates that the random splitting already achieves relatively even metrics\n",
    "    # for this region, so the threshold is more lenient\n",
    "    metrics_map = {\n",
    "        \"mean_omni_cloud\": (\n",
    "            \"cloud\",\n",
    "            {\n",
    "                \"USA\": 0.10,\n",
    "                \"Australia\": 0.12,\n",
    "                \"North America wo US\": 0.10,\n",
    "                \"CIS\": 0.12,\n",
    "                \"South and Central America\": 0.10,\n",
    "                \"Europe\": 0.15,\n",
    "                \"default\": 0.25,\n",
    "            },\n",
    "        ),\n",
    "        \"mean_overlap_area\": (\"overlap\", {\"USA\": 0.15, \"Europe\": 0.15, \"default\": 0.20}),\n",
    "        \"proportion\": (\n",
    "            \"proportion\",\n",
    "            {\"USA\": 0.08, \"Middle East\": 0.10, \"CIS\": 0.08, \"Australia\": 0.05, \"Europe\": 0.05, \"default\": 0.15},\n",
    "        ),\n",
    "        \"mean_omni_cloud_shadow\": (\"overlap\", {\"USA\": 0.15, \"Europe\": 0.15, \"default\": 0.20}),\n",
    "        \"mean_omni_nodata\": (\"overlap\", {\"USA\": 0.15, \"Europe\": 0.15, \"North America wo US\": 0.10, \"default\": 0.20}),\n",
    "    }\n",
    "\n",
    "    for region in all_regions:\n",
    "        if region not in train_metrics or region not in val_metrics:\n",
    "            scores[\"proportion\"] += 1000\n",
    "            continue\n",
    "\n",
    "        # Score region proportions. Use exponential penalty for increase weighting\n",
    "        props = [\n",
    "            train_metrics[region][\"proportion\"],\n",
    "            val_metrics[region][\"proportion\"],\n",
    "        ]\n",
    "        mean_prop = np.mean(props)\n",
    "        variation = np.std(props) / (mean_prop + 1e-6)\n",
    "        scores[\"proportion\"] += variation**potency\n",
    "\n",
    "        if verbose:\n",
    "            means = [\n",
    "                train_metrics[region][\"mean_omni_cloud\"],\n",
    "                val_metrics[region][\"mean_omni_cloud\"],\n",
    "            ]\n",
    "            print(\n",
    "                f\"{region:20}: %proportion {[np.round(k, 3) for k in props]}, {np.std(props)=:.2f}, {mean_prop=:.2f}, {variation=:.2f} **2--> {variation ** potency:.2f}\"\n",
    "            )\n",
    "\n",
    "        # Score temporal distribution\n",
    "        region_seasons = set(train_metrics[region][\"temporal_dist\"].keys()) | set(\n",
    "            val_metrics[region][\"temporal_dist\"].keys()\n",
    "        )\n",
    "\n",
    "        for period in region_seasons:\n",
    "            month_props = [\n",
    "                train_metrics[region][\"temporal_dist\"].get(period, 0),\n",
    "                val_metrics[region][\"temporal_dist\"].get(period, 0),\n",
    "            ]\n",
    "            if min(month_props) == 0:  # Penalize missing seasons\n",
    "                missing_penalty = (\n",
    "                    10 * potency * (1 / len(region_seasons))\n",
    "                )  # divide by num of avail seasons for that region\n",
    "                scores[\"temporal\"] += missing_penalty\n",
    "                continue\n",
    "\n",
    "            mean_month_props = np.mean(month_props)\n",
    "            variation = np.std(month_props) / (mean_month_props + 1e-6)  # ** potency\n",
    "            scores[\"temporal\"] += variation\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"{region:25}: Period {period:10}: Means {[np.round(k, 3) for k in month_props]}, {np.std(month_props)=:.2f}, {mean_month_props=:.2f},   --> {variation:.2f}\"\n",
    "                )\n",
    "\n",
    "        # Score other metrics\n",
    "        remaining_metrics_to_score = {\n",
    "            \"mean_omni_cloud\": \"cloud\",\n",
    "            \"mean_omni_cloud_shadow\": \"shadow\",\n",
    "            \"mean_omni_nodata\": \"nodata\",\n",
    "            \"mean_overlap_area\": \"overlap\",\n",
    "        }\n",
    "\n",
    "        for metric, score_key in remaining_metrics_to_score.items():\n",
    "            values = [train_metrics[region][metric], val_metrics[region][metric]]\n",
    "            mean_value = np.mean(values)\n",
    "            variation = np.std(values) / (mean_value + 1e-6)\n",
    "            scores[score_key] += variation\n",
    "\n",
    "    # Apply weights\n",
    "    if verbose:\n",
    "        for k, v in scores.items():\n",
    "            print(f\"{k:15} with weight {metric_weights.get(k, 1.0):.2f} = {metric_weights.get(k, 1.0) * v:.2f}\")\n",
    "\n",
    "    total_score = sum(metric_weights.get(k, 1.0) * v for k, v in scores.items())\n",
    "    return total_score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352de78-7e25-4ea2-b75a-c5f12e473f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def print_split_comparison(train_df: pd.DataFrame, val_df: pd.DataFrame):\n",
    "    \"\"\"Print comparison of distributions across splits in tabular format\"\"\"\n",
    "    train_metrics = calculate_distribution_metrics(train_df)\n",
    "    val_metrics = calculate_distribution_metrics(val_df)\n",
    "\n",
    "    # Get all unique regions across all splits\n",
    "    all_regions = set(train_metrics.keys()) | set(val_metrics.keys())\n",
    "\n",
    "    # Basic metrics table\n",
    "    print(\"\\nDistribution Metrics by Region and Split:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    basic_metrics = []\n",
    "    for region in all_regions:\n",
    "        for split_name, metrics in zip([\"Train\", \"Val\"], [train_metrics, val_metrics], strict=False):\n",
    "            basic_metrics.append(\n",
    "                [\n",
    "                    region,\n",
    "                    split_name,\n",
    "                    metrics[region][\"proportion\"],\n",
    "                    metrics[region][\"mean_omni_cloud\"],\n",
    "                    metrics[region][\"mean_omni_cloud_shadow\"],\n",
    "                    metrics[region][\"mean_omni_nodata\"],\n",
    "                    metrics[region][\"mean_overlap_area\"],\n",
    "                ]\n",
    "            )\n",
    "    print(\n",
    "        tabulate(\n",
    "            basic_metrics,\n",
    "            headers=[\"Region\", \"Split\", \"Prop\", \"OMNICloud%\", \"OMNICloudShadow%\", \"OMNINoData%\", \"OverlapArea\"],\n",
    "            floatfmt=\".3f\",\n",
    "            tablefmt=\"pipe\",  # Use pipe format for better readability\n",
    "        )\n",
    "    )\n",
    "    # Temporal distribution table\n",
    "    print(\"\\nSeasonal Distribution by Region:\")\n",
    "    print(\"-\" * 80)\n",
    "    temporal_metrics = []\n",
    "    for region in all_regions:\n",
    "        # Get all year-seasons for this region across splits\n",
    "        region_year_seasons = set(train_metrics[region][\"temporal_dist\"].keys()) | set(\n",
    "            val_metrics[region][\"temporal_dist\"].keys()\n",
    "        )\n",
    "        for split_name, metrics in zip([\"Train\", \"Val\"], [train_metrics, val_metrics], strict=False):\n",
    "            top_seasons = dict(\n",
    "                sorted(\n",
    "                    [(ys, metrics[region][\"temporal_dist\"].get(ys, 0)) for ys in region_year_seasons],\n",
    "                    key=lambda x: x[1],\n",
    "                    reverse=True,\n",
    "                )\n",
    "            )\n",
    "            # Format as \"season: XX.X%\"\n",
    "            top_seasons_str = \", \".join(f\"{ys}: {v:.1%}\" for ys, v in top_seasons.items())\n",
    "            temporal_metrics.append([region, split_name, top_seasons_str])\n",
    "    print(tabulate(temporal_metrics, headers=[\"Region\", \"Split\", \"Top Seasons (with %)\"], tablefmt=\"pipe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba6cea-a3d8-4690-9ae7-2005b94180fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform random splits and evaluate fitness\n",
    "rng = np.random.default_rng(42)\n",
    "best_score = float(\"inf\")\n",
    "best_split = None\n",
    "num_seeds = 4000\n",
    "train_frac = 0.9\n",
    "val_frac = 0.1\n",
    "\n",
    "# Get unique MGRS tiles\n",
    "unique_mgrs = mgrs_selected[\"mgrs\"].unique()\n",
    "\n",
    "metric_weights = {\n",
    "    \"proportion\": 10.0,\n",
    "    \"overlap\": 4.0,\n",
    "    \"cloud\": 5.0,\n",
    "    \"shadow\": 2.0,\n",
    "    \"temporal\": 0.1,\n",
    "    \"nodata\": 2.0,\n",
    "}\n",
    "potency = 1.75\n",
    "verbose = False\n",
    "\n",
    "for trial in range(num_seeds):\n",
    "    # Randomly assign MGRS to splits\n",
    "    rng.shuffle(unique_mgrs)\n",
    "    n_train = int(len(unique_mgrs) * train_frac)\n",
    "    n_val = int(len(unique_mgrs) * val_frac)\n",
    "\n",
    "    # Split MGRS tiles\n",
    "    train_mgrs = unique_mgrs[:n_train]\n",
    "    val_mgrs = unique_mgrs[n_train:]\n",
    "\n",
    "    # Create dataframe splits\n",
    "    train = mgrs_selected[mgrs_selected[\"mgrs\"].isin(train_mgrs)]\n",
    "    val = mgrs_selected[mgrs_selected[\"mgrs\"].isin(val_mgrs)]\n",
    "\n",
    "    # Score the split\n",
    "    score, component_scores = score_split(mgrs_selected, train, val, metric_weights, verbose=verbose, potency=potency)\n",
    "\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_split = (train, val)\n",
    "        print(f\"New best score: {best_score:.4f} at trial {trial}\")\n",
    "        print(\"  Component scores:\")\n",
    "        for metric, value in component_scores.items():\n",
    "            print(f\"\\t{metric:15s}: {value * metric_weights[metric]:.4f}\")\n",
    "        print_split_comparison(train, val)\n",
    "        score_split(mgrs_selected, train, val, metric_weights, verbose=True, potency=potency)\n",
    "\n",
    "\n",
    "# Output the best split\n",
    "train, val = best_split\n",
    "print()\n",
    "print(\"Best split achieved:\")\n",
    "print(f\"Train size: {len(train)}\")\n",
    "print(f\"Validation size: {len(val)}\")\n",
    "print_split_comparison(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffb692-62d5-4094-b272-f7ceefa2f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), train[\"mgrs\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19cf6c-6a33-4a1b-b659-ca6da09d6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val), val[\"mgrs\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337274b-01fc-4876-a3d9-87ac5601e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_train_4098.csv\", index=False)\n",
    "val.to_csv(\"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_val_449.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa1d53-f221-48e2-8845-e05c1ac93a05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (OLD: Appendix: Plots to Share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6456d3-1714-4e34-ad29-5bf1f52ff42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_train_4098.csv\")\n",
    "val = pd.read_csv(\"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_val_449.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e861e37-437b-481e-9f30-3c87f7bed72b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_gdf = train.copy(deep=True)\n",
    "train_gdf[\"geometry\"] = train_gdf[\"geometry\"].apply(wkt.loads)\n",
    "train_gdf = gpd.GeoDataFrame(train_gdf, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "train_gdf[\"fold\"] = \"train\"\n",
    "train_gdf = train_gdf.drop(columns=[\"datetime\"])\n",
    "\n",
    "val_gdf = val.copy(deep=True)\n",
    "val_gdf[\"geometry\"] = val_gdf[\"geometry\"].apply(wkt.loads)\n",
    "val_gdf = gpd.GeoDataFrame(val_gdf, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "val_gdf[\"fold\"] = \"val\"\n",
    "val_gdf = val_gdf.drop(columns=[\"datetime\"])\n",
    "\n",
    "m = train_gdf.explore(color=\"green\", name=\"Train\", legend=True)\n",
    "m = val_gdf.explore(m=m, color=\"red\", name=\"Val\", legend=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd372158-2ca3-42d9-bbee-8bedaccae2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gdf[\"fold\"] = \"train\"\n",
    "val_gdf[\"fold\"] = \"val\"\n",
    "gdf = pd.concat((train_gdf, val_gdf), ignore_index=True)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "output_path = \"../src/data/tiles/s2/csv_files/2025_02_12_MGRS_with_IDs_within_OG_4547_with_metadata.geojson\"\n",
    "gdf.to_file(output_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccc6ac-ec16-481e-8369-f2d714125522",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Appendix: Load other producing datasets\n",
    "\n",
    "- For querying our DB programtically it is easiest to use your AWS VM instead of an Azure VM since there is some connection magic needed (Diehl is the expert on this)\n",
    "- See Slide 21-25 in [Tile Selection + Reference tiles + Chip masking](https://docs.google.com/presentation/d/1p4pgQJjGQB4yssF7Wxs0aLuxp6xofJeKFjqrp1iUXSs/edit#slide=id.g3254c12f455_0_0) for details on the HIFLD/OGIM datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86674f07-fb25-4a0a-95f0-b8146b3de479",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to DB ###\n",
    "\n",
    "# db_port = 5432\n",
    "# db_name = \"orbio\"\n",
    "# db_user = \"readonly_user\"\n",
    "# db_host = \"orbio-dev-db-ro.ccfdaoc3gfph.eu-central-1.rds.amazonaws.com\"\n",
    "# from getpass import getpass\n",
    "# db_password = getpass(\"Read-only password (see https://git.orbio.earth/orbio/wiki/-/wikis/Playbooks/SQL-DBs-Access-&-Troubleshooting-(see-here-for-read-only-DBs!) ):\")\n",
    "\n",
    "# conn = sqlalchemy.create_engine(\n",
    "#     f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "# )\n",
    "\n",
    "### Query HIFLD Data ###\n",
    "\n",
    "# query = \"\"\"\n",
    "# SELECT geom\n",
    "# FROM source\n",
    "# WHERE origin_dataset = %s\n",
    "# \"\"\"\n",
    "# url = 'https://hifld-geoplatform.opendata.arcgis.com/datasets/geoplatform::oil-and-natural-gas-wells/explore?location=35.790450%2C-102.912136%2C4.22'\n",
    "\n",
    "# hifld = gpd.read_postgis(query, con=conn, params=(url,), geom_col='geom')\n",
    "\n",
    "### Query OGIM USA Data ###\n",
    "\n",
    "# query = \"\"\"\n",
    "# SELECT geom\n",
    "# FROM source\n",
    "# WHERE origin_dataset = 'ogim'\n",
    "#   AND (TYPE <> 'OIL AND NATURAL GAS PIPELINES' AND TYPE <> 'NATURAL GAS FLARING DETECTIONS')\n",
    "# \"\"\"\n",
    "\n",
    "# ogim_usa = gpd.read_postgis(query, con=conn, geom_col='geom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b01b8-0214-4ebd-ad44-46e5db2832f6",
   "metadata": {},
   "source": [
    "# Use the less cloudy validation IDs from Hassi/Marcellus/Permian for Detection Threshold\n",
    "- Later: Use new chip selection and use also tiles with more nodata/clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28614a4c-db12-4aa0-9648-eb3bcc141330",
   "metadata": {},
   "outputs": [],
   "source": [
    "MGRS_DATE_DF = pd.read_csv(\"../src/data/tiles/s2/csv_files/fpr_areas.csv\")\n",
    "val_df = pd.read_csv(\"../src/data/tiles/s2/csv_files/2025_02_19_MGRS_with_IDs_within_OG_val_348.csv\")\n",
    "print(val_df.shape)\n",
    "val_df = val_df[(val_df[\"cloud_percent\"] < 20) & (val_df[\"nodata_perc\"] < 2)]\n",
    "MGRS_DATE_DF.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f512b-295d-4934-ba8c-2b7e08d3ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[\"region\"] = \"hassi\"\n",
    "val_df.loc[val_df[\"inside_marcellus\"] == True, \"region\"] = \"marcellus\"\n",
    "val_df.loc[val_df[\"inside_permian\"] == True, \"region\"] = \"permian\"\n",
    "val_df[\"region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b648d6-6eba-46a2-8962-238c9f4e456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[[\"mgrs\", \"date\", \"region\", \"cloud_percent\", \"nodata_perc\", \"geometry\"]].to_csv(\n",
    "    \"../src/data/tiles/s2/csv_files/2025_03_03_fpr_areas_122_from_h_m_p_valset.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14533cc-8b23-4419-9b6d-70c60bf79059",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(\"../src/data/tiles/s2/csv_files/2025_03_03_fpr_areas_122_from_h_m_p_valset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e5c24-1181-449f-9354-8b96a548c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[\"date\"].apply(lambda x: x[5:7]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5d02c-cacb-4bc5-8bfd-1e6fe623022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[\"date\"].apply(lambda x: x[:4]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d5c07-1788-48da-a2a8-464669603928",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.groupby([\"inside_hassi\", \"inside_marcellus\", \"inside_permian\"])[\"mgrs\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5f953-e300-4363-b35d-40e2f1f5f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.groupby([\"inside_hassi\", \"inside_marcellus\", \"inside_permian\"])[\"mgrs\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e940c10-0bcb-41a0-8863-ab6d1df0831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MGRS_DATE_DF[\"date\"].str.contains(\"2023-\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157b9b3-1cee-4e38-8946-3078941a12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MGRS_DATE_DF.groupby(\"region\")[\"mgrs\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057f4a4-05db-4076-886e-71d31a0efb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MGRS_DATE_DF[\"region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf659c-108f-47ba-a845-b2c0c447ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MGRS_DATE_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ed7ef-0350-43d7-9f7f-dce6050651b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "esri_attribution = \"esri\"\n",
    "m = gpd.read_file(\"../src/data/ancillary/marcellus_large.geojson\").explore(\n",
    "    tiles=\"Esri.WorldImagery\", attr=esri_attribution, style_kwds={\"color\": \"green\", \"fillOpacity\": 0.5}\n",
    ")\n",
    "gpd.read_file(\"../src/data/ancillary/hassi.geojson\").explore(m=m, style_kwds={\"color\": \"orange\", \"fillOpacity\": 0.5})\n",
    "gpd.read_file(\"../src/data/ancillary/permian.geojson\").explore(m=m, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.5})\n",
    "\n",
    "mgrs_gdf[mgrs_gdf.index.isin(MGRS_DATE_DF[\"mgrs\"])].explore(m=m, style_kwds={\"color\": \"blue\", \"fillOpacity\": 0.5})\n",
    "mgrs_gdf[mgrs_gdf.index.isin(val_df[\"mgrs\"])].explore(m=m, style_kwds={\"color\": \"cyan\", \"fillOpacity\": 0.5})\n",
    "mgrs_gdf[mgrs_gdf.index.isin([\"40SBH\"])].explore(m=m, style_kwds={\"color\": \"magenta\", \"fillOpacity\": 0.5})\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
