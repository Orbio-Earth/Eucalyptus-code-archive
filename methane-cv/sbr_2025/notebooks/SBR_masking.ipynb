{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from lib.models.schemas import WatershedParameters\n",
    "from lib.plume_masking import retrieval_mask_using_watershed_algo\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.segmentation import watershed\n",
    "\n",
    "from sbr_2025.utils.plotting import (\n",
    "    Colorbar,\n",
    "    plot_rgb_ratio,\n",
    ")\n",
    "from sbr_2025.utils.quantification import (\n",
    "    calculate_circle_distance_quantification,\n",
    "    calculate_major_axis_quantification,\n",
    "    calculate_sqrtA_quantification,\n",
    "    find_central_plume,\n",
    "    quantification_interval,\n",
    ")\n",
    "from src.azure_wrap.ml_client_utils import (\n",
    "    initialize_blob_service_client,\n",
    ")\n",
    "from src.inference.inference_functions import (\n",
    "    crop_main_data,\n",
    "    crop_reference_data,\n",
    "    fetch_sentinel2_items_for_point,\n",
    "    generate_predictions,\n",
    ")\n",
    "from src.inference.inference_target_location import (\n",
    "    add_retrieval_to_pred,\n",
    ")\n",
    "from src.plotting.plotting_functions import grid16\n",
    "from src.training.loss_functions import TwoPartLoss\n",
    "from src.utils import PACKAGE_ROOT\n",
    "from src.utils.parameters import SatelliteID\n",
    "from src.utils.quantification_utils import calc_wind_direction\n",
    "from src.utils.utils import initialize_clients, load_model_and_concatenator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client, _, _, s3_client = initialize_clients(False)\n",
    "abs_client = initialize_blob_service_client(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = 32.82175, -111.78581  # release point for SBRs\n",
    "\n",
    "model_identifier = \"models:/torchgeo_pwr_unet/1226\"\n",
    "\n",
    "# can we predict on larger chips?\n",
    "crop_size = 128\n",
    "num_snapshots = 6\n",
    "center_buffer = 10  # number of pixels to search from center\n",
    "\n",
    "# Date range of SBR Phase 1\n",
    "start_date = \"2025-01-01\"\n",
    "end_date = \"2025-03-31\"\n",
    "\n",
    "start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(end_date, \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model, band_concatenator, train_params = load_model_and_concatenator(model_identifier, \"cpu\", SatelliteID.S2)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "lossFn = TwoPartLoss(train_params[\"binary_threshold\"], train_params[\"MSE_multiplier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Overpass Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these dates have KNOWN releases during Phase 0\n",
    "phase0_release_dates = [\n",
    "    \"2024-11-14\",\n",
    "    \"2024-11-19\",\n",
    "    \"2024-11-22\",\n",
    "    \"2024-12-02\",\n",
    "    \"2024-12-04\",\n",
    "    \"2024-12-07\",\n",
    "    \"2024-12-09\",\n",
    "    \"2024-12-17\",\n",
    "    \"2024-12-19\",\n",
    "    \"2024-12-22\",\n",
    "    \"2024-12-29\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudy_dates = [\"2025-01-06\", \"2025-01-08\", \"2024-12-14\", \"2024-11-29\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# choose a single date to do inference\n",
    "target_date = phase0_release_dates[-1]\n",
    "target_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "\n",
    "items = fetch_sentinel2_items_for_point(\n",
    "    lat=lat, lon=lon, query_datetime=target_date, crop_size=crop_size, sbr_notebook=True\n",
    ")\n",
    "\n",
    "main_data = crop_main_data(items, abs_client, s3_client, lat, lon, crop_size)\n",
    "main_item = main_data[\"tile_item\"]\n",
    "\n",
    "# load all the reference data once, and we'll reuse it for every target date\n",
    "num_snapshots = 24\n",
    "reference_data = crop_reference_data(\n",
    "    items, main_data, abs_client, s3_client, lat, lon, crop_size, required_num_previous_snapshots=num_snapshots\n",
    ")\n",
    "\n",
    "print(\"\\n---: Summary\")\n",
    "print(f\"Main tile for {target_date}: USE {main_item.id}\")\n",
    "for reference in reference_data:\n",
    "    tile_id = reference[\"tile_item\"].id\n",
    "    reference_date = reference[\"tile_item\"].time.date().isoformat()\n",
    "    print(f\"    Reference tile on {reference_date}: USE {tile_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items:\n",
    "    print(f\"ID: {item.id}\")\n",
    "    print(f\"Time: {item.time}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## I. RGBs & Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_items = [main_data, *reference_data]\n",
    "\n",
    "for i, data in enumerate(data_items):\n",
    "    date = data[\"tile_item\"].time.date().isoformat()\n",
    "    plot_rgb_ratio(data, Colorbar.INDIVIDUAL, i, SatelliteID.S2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Iterate over all Phase 0 release dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_data_csv_path = PACKAGE_ROOT.parent / \"src\" / \"data\" / \"ancillary\" / \"wind_data_with_era5.csv\"\n",
    "wind_data_df = pd.read_csv(wind_data_csv_path)\n",
    "wind_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Getting all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ground truth plumes CSV\n",
    "\n",
    "ground_truth_csv_path = PACKAGE_ROOT.parent / \"src\" / \"data\" / \"ancillary\" / \"ground_truth_plumes.csv\"\n",
    "ground_truth_df = pd.read_csv(ground_truth_csv_path)\n",
    "\n",
    "# Filter for SBR Phase 0 plumes (2024 Casa Grande plumes with non-zero quantification)\n",
    "phase0_plumes = ground_truth_df[\n",
    "    (ground_truth_df[\"site\"] == \"Casa Grande, AZ\")\n",
    "    & (ground_truth_df[\"date\"].str.startswith(\"2024\"))\n",
    "    & (ground_truth_df[\"quantification_kg_h\"] > 0)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(phase0_plumes)} SBR Phase 0 plumes with quantification data\")\n",
    "print(phase0_plumes[[\"date\", \"quantification_kg_h\"]])\n",
    "\n",
    "manual_reference_selections = {\n",
    "    \"2024-12-29\": (\"2024-12-24\", \"2024-12-22\"),\n",
    "}\n",
    "predictions = {}\n",
    "for _, plume_row in phase0_plumes.iterrows():\n",
    "    plume_date_str = plume_row[\"date\"]\n",
    "    plume_date = datetime.strptime(plume_date_str, \"%Y-%m-%d\")\n",
    "    ground_truth_kg_h = plume_row[\"quantification_kg_h\"]\n",
    "\n",
    "    print(f\"\\nProcessing plume from {plume_date_str} with ground truth {ground_truth_kg_h} kg/h\")\n",
    "\n",
    "    # Find the main data for this plume date\n",
    "    main_tile = None\n",
    "    for ref in reference_data:\n",
    "        ref_date = ref[\"tile_item\"].time.date().isoformat()\n",
    "        if ref_date == plume_date_str:\n",
    "            main_tile = ref\n",
    "            print(f\"Found main tile for {plume_date_str}: {ref['tile_item'].id}\")\n",
    "            break\n",
    "\n",
    "    if main_tile is None:\n",
    "        # Check if this plume date matches the main data loaded above\n",
    "        if main_data[\"tile_item\"].time.date().isoformat() == plume_date_str:\n",
    "            main_tile = main_data\n",
    "            print(f\"Found main tile in main_data for {plume_date_str}: {main_data['tile_item'].id}\")\n",
    "        else:\n",
    "            print(f\"No tile found for plume date {plume_date_str}, skipping\")\n",
    "            continue\n",
    "    # Find two reference tiles that don't have controlled releases\n",
    "    clean_refs = []\n",
    "\n",
    "    # Check if we have manual reference selections for this date\n",
    "    if plume_date_str in manual_reference_selections:\n",
    "        before_date, earlier_date = manual_reference_selections[plume_date_str]\n",
    "        print(f\"Using manually specified reference dates: {before_date} and {earlier_date}\")\n",
    "\n",
    "        # Find the reference tiles matching the manual selections\n",
    "        for ref in reference_data:\n",
    "            ref_date = ref[\"tile_item\"].time.date().isoformat()\n",
    "            if ref_date in (before_date, earlier_date):\n",
    "                clean_refs.append(ref)\n",
    "    else:\n",
    "        # Use the automatic selection logic\n",
    "        for ref in reference_data:\n",
    "            ref_date = ref[\"tile_item\"].time.date().isoformat()\n",
    "            if ref_date in cloudy_dates:\n",
    "                continue\n",
    "            if ref_date in phase0_release_dates:\n",
    "                continue\n",
    "            if ref_date >= plume_date_str:\n",
    "                continue\n",
    "            clean_refs.append(ref)\n",
    "\n",
    "    if len(clean_refs) < 2:  # noqa: PLR2004\n",
    "        print(f\"Not enough clean reference tiles found for {plume_date_str}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Sort by date (newest first) and take the two most recent\n",
    "    clean_refs.sort(key=lambda x: x[\"tile_item\"].time.date().isoformat(), reverse=True)\n",
    "    reference_chips = clean_refs[:2]\n",
    "\n",
    "    before_data = reference_chips[0]\n",
    "    earlier_data = reference_chips[1]\n",
    "\n",
    "    print(\"Using reference tiles from:\")\n",
    "    print(f\"  Before: {before_data['tile_item'].time.date().isoformat()}\")\n",
    "    print(f\"  Earlier: {earlier_data['tile_item'].time.date().isoformat()}\")\n",
    "\n",
    "    # Generate predictions\n",
    "    # try:\n",
    "    preds = generate_predictions(main_tile, reference_chips, model, device, band_concatenator, lossFn)\n",
    "    preds = add_retrieval_to_pred(preds, main_tile[\"tile_item\"])\n",
    "    # Add timestamp to predictions\n",
    "    preds[\"timestamp\"] = main_tile[\"tile_item\"].time\n",
    "    preds[\"ground_truth_kg_h\"] = ground_truth_kg_h\n",
    "\n",
    "    predictions[plume_date_str] = preds\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error processing {plume_date_str}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all binary predictions in a grid\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (date, pred) in enumerate(predictions.items()):\n",
    "    ax = axes[i]\n",
    "    binary_mask = pred[\"binary_probability\"]\n",
    "    ax.imshow(binary_mask, cmap=\"pink_r\", vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "    grid16(ax=ax)\n",
    "    ax.grid(True, which=\"both\")\n",
    "    ax.set_title(f\"Date: {date}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Functions implementing quantification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_quantification_methods(\n",
    "    plume_labels, binary_probability, conditional_retrieval, pixel_width, wind_speed, max_distance_pixels=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate methane quantification using all methods.\n",
    "\n",
    "    Args:\n",
    "        plume_labels: Labeled mask of plumes\n",
    "        binary_probability: Binary probability map\n",
    "        conditional_retrieval: Conditional retrieval values\n",
    "        pixel_width: Width of a pixel in meters\n",
    "        wind_speed: Wind speed in m/s\n",
    "        max_distance_pixels: Maximum distance in pixels for circle method\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dict: Dictionary with all quantification results\n",
    "    \"\"\"\n",
    "    # Find the central plume\n",
    "    nearest_plume_label = find_central_plume(plume_labels, max_distance_pixels, pixel_width)\n",
    "    if nearest_plume_label == 0:\n",
    "        raise ValueError(f\"No central plume found within {max_distance_pixels} pixels\")\n",
    "    central_plume_mask = plume_labels == nearest_plume_label\n",
    "\n",
    "    # Calculate rescaled retrieval\n",
    "    max_binary_probability = np.max(binary_probability[central_plume_mask])\n",
    "    rescaled_retrieval = conditional_retrieval * binary_probability / max_binary_probability\n",
    "\n",
    "    # Calculate quantification using conditional retrieval\n",
    "    # Square root area method\n",
    "    L_sqrtA_conditional, IME_sqrtA_conditional, Q_sqrtA_conditional = calculate_sqrtA_quantification(\n",
    "        central_plume_mask, conditional_retrieval, pixel_width, wind_speed\n",
    "    )\n",
    "\n",
    "    # Major axis method\n",
    "    L_major_conditional, IME_major_conditional, Q_major_conditional = calculate_major_axis_quantification(\n",
    "        central_plume_mask, conditional_retrieval, pixel_width, wind_speed\n",
    "    )\n",
    "\n",
    "    # Circle distance method\n",
    "    L_circle_conditional, IME_circle_conditional, Q_circle_conditional = calculate_circle_distance_quantification(\n",
    "        central_plume_mask,\n",
    "        conditional_retrieval,\n",
    "        pixel_width,\n",
    "        wind_speed,\n",
    "        min_distance_pixels=5,\n",
    "        max_distance_pixels=20,\n",
    "    )\n",
    "\n",
    "    # Calculate quantification using rescaled retrieval\n",
    "    # Square root area method\n",
    "    L_sqrtA_rescaled, IME_sqrtA_rescaled, Q_sqrtA_rescaled = calculate_sqrtA_quantification(\n",
    "        central_plume_mask, rescaled_retrieval, pixel_width, wind_speed\n",
    "    )\n",
    "\n",
    "    # Major axis method\n",
    "    L_major_rescaled, IME_major_rescaled, Q_major_rescaled = calculate_major_axis_quantification(\n",
    "        central_plume_mask, rescaled_retrieval, pixel_width, wind_speed\n",
    "    )\n",
    "\n",
    "    # Circle distance method\n",
    "    L_circle_rescaled, IME_circle_rescaled, Q_circle_rescaled = calculate_circle_distance_quantification(\n",
    "        central_plume_mask, rescaled_retrieval, pixel_width, wind_speed, max_distance_pixels\n",
    "    )\n",
    "\n",
    "    # Return all results\n",
    "    return {\n",
    "        # Square root area method - conditional\n",
    "        \"L_sqrtA_conditional_m\": L_sqrtA_conditional,\n",
    "        \"IME_sqrtA_conditional_mol\": IME_sqrtA_conditional,\n",
    "        \"Q_sqrtA_conditional_kg_h\": Q_sqrtA_conditional,\n",
    "        # Major axis method - conditional\n",
    "        \"L_major_conditional_m\": L_major_conditional,\n",
    "        \"IME_major_conditional_mol\": IME_major_conditional,\n",
    "        \"Q_major_conditional_kg_h\": Q_major_conditional,\n",
    "        # Circle method - conditional\n",
    "        \"L_circle_conditional_m\": L_circle_conditional,\n",
    "        \"IME_circle_conditional_mol\": IME_circle_conditional,\n",
    "        \"Q_circle_conditional_kg_h\": Q_circle_conditional,\n",
    "        # Square root area method - retrieval\n",
    "        \"L_sqrtA_rescaled_m\": L_sqrtA_rescaled,\n",
    "        \"IME_sqrtA_rescaled_mol\": IME_sqrtA_rescaled,\n",
    "        \"Q_sqrtA_rescaled_kg_h\": Q_sqrtA_rescaled,\n",
    "        # Major axis method - retrieval\n",
    "        \"L_major_rescaled_m\": L_major_rescaled,\n",
    "        \"IME_major_rescaled_mol\": IME_major_rescaled,\n",
    "        \"Q_major_rescaled_kg_h\": Q_major_rescaled,\n",
    "        # Circle method - retrieval\n",
    "        \"L_circle_rescaled_m\": L_circle_rescaled,\n",
    "        \"IME_circle_rescaled_mol\": IME_circle_rescaled,\n",
    "        \"Q_circle_rescaled_kg_h\": Q_circle_rescaled,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wind_data_from_csv(wind_data_df: pd.DataFrame, date_str: str, sensor: str = \"S2\") -> dict:\n",
    "    \"\"\"\n",
    "    Get all available wind data from the CSV dataframe for a specific date and sensor.\n",
    "\n",
    "    Args:\n",
    "        wind_data_df: DataFrame containing wind data\n",
    "        date_str: Date string in format 'YYYY-MM-DD'\n",
    "        sensor: Sensor type (default: \"S2\" for Sentinel 2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dict: Dictionary with keys 'geos_fp', 'era5', and 'ground_truth', each containing (wind_speed, wind_direction)\n",
    "              NaN values are propagated automatically if data is not available\n",
    "    \"\"\"\n",
    "    # Find the matching row in the wind data DataFrame\n",
    "    wind_row = wind_data_df[(wind_data_df[\"date\"] == date_str) & (wind_data_df[\"sensor\"] == sensor)]\n",
    "\n",
    "    if len(wind_row) == 0:\n",
    "        raise ValueError(f\"No wind data found for date={date_str}, sensor={sensor}\")\n",
    "\n",
    "    wind_data = {}\n",
    "\n",
    "    # Get GEOS-FP wind data\n",
    "    u_wind_geos = wind_row[\"geos_ux\"].values[0]\n",
    "    v_wind_geos = wind_row[\"geos_uy\"].values[0]\n",
    "    wind_speed_geos = np.sqrt(u_wind_geos**2 + v_wind_geos**2)\n",
    "    wind_direction_geos = calc_wind_direction(u_wind_geos, v_wind_geos)\n",
    "    wind_data[\"geos_fp\"] = (wind_speed_geos, wind_direction_geos)\n",
    "\n",
    "    # Get ERA5 wind data\n",
    "    u_wind_era5 = wind_row[\"era5_ux\"].values[0]\n",
    "    v_wind_era5 = wind_row[\"era5_uy\"].values[0]\n",
    "    wind_speed_era5 = np.sqrt(u_wind_era5**2 + v_wind_era5**2)\n",
    "    wind_direction_era5 = calc_wind_direction(u_wind_era5, v_wind_era5)\n",
    "    wind_data[\"era5\"] = (wind_speed_era5, wind_direction_era5)\n",
    "\n",
    "    # Get ground truth wind data\n",
    "    u_wind_gt = wind_row[\"gt_ux\"].values[0]\n",
    "    v_wind_gt = wind_row[\"gt_uy\"].values[0]\n",
    "    wind_speed_gt = np.sqrt(u_wind_gt**2 + v_wind_gt**2)\n",
    "    wind_direction_gt = calc_wind_direction(u_wind_gt, v_wind_gt)\n",
    "    wind_data[\"ground_truth\"] = (wind_speed_gt, wind_direction_gt)\n",
    "\n",
    "    return wind_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Obtaining all quantifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_pass_watershed(binary_probability, watershed_floor_threshold=0.075, initial_mask_threshold=0.5, connectivity=2):\n",
    "    \"\"\"\n",
    "    Perform watershed segmentation on a binary probability map using maximum probability points as markers.\n",
    "\n",
    "    Args:\n",
    "        binary_probability: Probability map of plume detection\n",
    "        watershed_floor_threshold: Lower threshold for watershed segmentation (default: 0.075)\n",
    "        initial_mask_threshold: Threshold for initial binary mask creation (default: 0.5)\n",
    "        connectivity: Connectivity for labeling (default: 2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Segmented mask after watershed algorithm\n",
    "    \"\"\"\n",
    "    # First pass masking: create binary mask from probability\n",
    "    predicted_mask = binary_probability > initial_mask_threshold\n",
    "\n",
    "    # Label connected regions\n",
    "    labeled_mask = label(predicted_mask, connectivity=connectivity)\n",
    "\n",
    "    # Create a marker mask with maximum probability point in each plume region\n",
    "    marker_mask = np.zeros_like(predicted_mask)\n",
    "    for region in regionprops(labeled_mask):\n",
    "        # Get coordinates for this plume region\n",
    "        coords = region.coords  # (row, col) coordinates of all points in region\n",
    "\n",
    "        # Get probability values for this region\n",
    "        region_probs = binary_probability[coords[:, 0], coords[:, 1]]\n",
    "\n",
    "        # Find position of maximum probability in this region\n",
    "        max_prob_idx = np.argmax(region_probs)\n",
    "        max_prob_position = coords[max_prob_idx]\n",
    "\n",
    "        # Mark the maximum point in the marker mask\n",
    "        marker_mask[max_prob_position[0], max_prob_position[1]] = 1\n",
    "\n",
    "    # Generate the markers for watershed\n",
    "    markers = ndi.label(marker_mask)[0]\n",
    "\n",
    "    # Run watershed with markers\n",
    "    segmented_mask = watershed(-binary_probability, markers, mask=binary_probability > watershed_floor_threshold)\n",
    "\n",
    "    return segmented_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the results processing to include all masking algorithms\n",
    "all_results = []\n",
    "masking_algorithms = [\n",
    "    (\n",
    "        \"strict_watershed\",\n",
    "        lambda bp: label(\n",
    "            retrieval_mask_using_watershed_algo(\n",
    "                WatershedParameters(\n",
    "                    marker_distance=1, marker_threshold=0.1, watershed_floor_threshold=0.1, closing_footprint_size=0\n",
    "                ),\n",
    "                bp,\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"sensitive_watershed\",\n",
    "        lambda bp: label(\n",
    "            retrieval_mask_using_watershed_algo(\n",
    "                WatershedParameters(\n",
    "                    marker_distance=1, marker_threshold=0.1, watershed_floor_threshold=0.05, closing_footprint_size=0\n",
    "                ),\n",
    "                bp,\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "    (\"two_pass_watershed\", lambda bp: two_pass_watershed(bp, 0.05, 0.1, 2)),\n",
    "]\n",
    "\n",
    "for plume_date_str, preds in predictions.items():\n",
    "    binary_probability = preds[\"binary_probability\"].numpy()\n",
    "    retrieval = preds[\"conditional_retrieval\"]\n",
    "\n",
    "    # Get wind data from all sources\n",
    "    sensing_time = preds[\"timestamp\"]\n",
    "    try:\n",
    "        wind_data = get_wind_data_from_csv(wind_data_df, sensing_time.date().isoformat())\n",
    "    except ValueError as e:\n",
    "        print(f\"Error getting wind data for {plume_date_str}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Calculate quantification for each masking algorithm\n",
    "    pixel_width = 20\n",
    "\n",
    "    for mask_name, mask_func in masking_algorithms:\n",
    "        # Apply the masking algorithm\n",
    "        try:\n",
    "            predicted_mask = mask_func(binary_probability)\n",
    "\n",
    "            # Base result dictionary\n",
    "            result = {\n",
    "                \"date\": plume_date_str,\n",
    "                \"masking_algorithm\": mask_name,\n",
    "                \"ground_truth_kg_h\": preds[\"ground_truth_kg_h\"],\n",
    "                \"before_date\": before_data[\"tile_item\"].time.date().isoformat(),\n",
    "                \"earlier_date\": earlier_data[\"tile_item\"].time.date().isoformat(),\n",
    "            }\n",
    "\n",
    "            # Store wind speeds and directions\n",
    "            for wind_source, (wind_speed, wind_direction) in wind_data.items():\n",
    "                result[f\"{wind_source}_wind_speed_m_s\"] = wind_speed\n",
    "                result[f\"{wind_source}_wind_direction_deg\"] = wind_direction\n",
    "\n",
    "            # Calculate quantification for each wind source\n",
    "            for wind_source, (wind_speed, _) in wind_data.items():\n",
    "                # Skip if wind speed is NaN\n",
    "                if pd.isna(wind_speed):\n",
    "                    print(f\"No {wind_source} wind data available for {plume_date_str}, skipping this source\")\n",
    "                    continue\n",
    "\n",
    "                quant_results = calculate_all_quantification_methods(\n",
    "                    predicted_mask, binary_probability, retrieval, pixel_width, wind_speed\n",
    "                )\n",
    "\n",
    "                # Add wind_source prefix to all quantification results\n",
    "                for key, value in quant_results.items():\n",
    "                    result[f\"{wind_source}_{key}\"] = value\n",
    "\n",
    "                print(\n",
    "                    f\"Processed {plume_date_str} with {mask_name}, {wind_source} wind data: \"\n",
    "                    f\"sqrtA: {quant_results['Q_sqrtA_conditional_kg_h']:.3f} kg/h, \"\n",
    "                    f\"circle rescaled: {quant_results['Q_circle_rescaled_kg_h']:.3f} kg/h\"\n",
    "                )\n",
    "\n",
    "            all_results.append(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error applying {mask_name} to {plume_date_str}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe with enhanced visualization\n",
    "all_results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Define methods and wind sources for analysis\n",
    "methods = [\n",
    "    (\"sqrtA+conditional\", \"Q_sqrtA_conditional_kg_h\"),\n",
    "    (\"major_axis+conditional\", \"Q_major_conditional_kg_h\"),\n",
    "    (\"circle+conditional\", \"Q_circle_conditional_kg_h\"),\n",
    "    (\"sqrtA+rescaled\", \"Q_sqrtA_rescaled_kg_h\"),\n",
    "    (\"major_axis+rescaled\", \"Q_major_rescaled_kg_h\"),\n",
    "    (\"circle+rescaled\", \"Q_circle_rescaled_kg_h\"),\n",
    "]\n",
    "\n",
    "wind_sources = [\"geos_fp\", \"era5\", \"ground_truth\"]\n",
    "mask_algos_labels = all_results_df[\"masking_algorithm\"].unique()\n",
    "\n",
    "# Calculate error metrics for all methods, wind sources, and masking algorithms\n",
    "error_metrics = {}\n",
    "for mask_algo in mask_algos_labels:\n",
    "    error_metrics[mask_algo] = {}\n",
    "    mask_df = all_results_df[all_results_df[\"masking_algorithm\"] == mask_algo].copy()\n",
    "\n",
    "    for wind_source in wind_sources:\n",
    "        error_metrics[mask_algo][wind_source] = {}\n",
    "        for method_name, column in methods:\n",
    "            full_column = f\"{wind_source}_{column}\"\n",
    "            # Calculate percentage error\n",
    "            error_pct = (mask_df[full_column] - mask_df[\"ground_truth_kg_h\"]) / mask_df[\"ground_truth_kg_h\"] * 100\n",
    "            mask_df.loc[:, f\"{wind_source}_{method_name}_error_pct\"] = error_pct\n",
    "\n",
    "            # Calculate metrics\n",
    "            mae = np.abs(error_pct).mean()\n",
    "            rmse = np.sqrt((error_pct**2).mean())\n",
    "            bias = error_pct.mean()\n",
    "\n",
    "            error_metrics[mask_algo][wind_source][method_name] = {\n",
    "                \"MAPE (%)\": mae,\n",
    "                \"RMSPE (%)\": rmse,\n",
    "                \"MPE (%)\": bias,\n",
    "            }\n",
    "\n",
    "# Display results dataframe\n",
    "print(\"\\nQuantification Results with Different Masking Algorithms:\")\n",
    "display(all_results_df)\n",
    "\n",
    "# Create a comprehensive error metrics table\n",
    "metrics_table = []\n",
    "for mask_algo in mask_algos_labels:\n",
    "    for wind_source in wind_sources:\n",
    "        # if wind_source in error_metrics[mask_algo]:\n",
    "        for method_name in [m[0] for m in methods]:\n",
    "            metrics_table.append(\n",
    "                {\n",
    "                    \"Masking Algorithm\": mask_algo,\n",
    "                    \"Wind Source\": wind_source.upper(),\n",
    "                    \"Method\": method_name,\n",
    "                    \"MAPE (%)\": error_metrics[mask_algo][wind_source][method_name][\"MAPE (%)\"],\n",
    "                    \"RMSPE (%)\": error_metrics[mask_algo][wind_source][method_name][\"RMSPE (%)\"],\n",
    "                    \"MPE (%)\": error_metrics[mask_algo][wind_source][method_name][\"MPE (%)\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_table)\n",
    "\n",
    "# Sort by masking algorithm, wind source, and MAE\n",
    "metrics_df = metrics_df.sort_values([\"Masking Algorithm\", \"Wind Source\", \"MAPE (%)\"])\n",
    "\n",
    "# Format the metrics to 2 decimal places\n",
    "for col in [\"MAPE (%)\", \"RMSPE (%)\", \"MPE (%)\"]:\n",
    "    metrics_df[col] = metrics_df[col].map(\"{:.2f}\".format)\n",
    "\n",
    "# Display the metrics table\n",
    "print(\"\\nError Metrics by Masking Algorithm, Wind Source, and Method:\")\n",
    "display(metrics_df)\n",
    "\n",
    "# Highlight the best method for each masking algorithm and wind source\n",
    "best_methods = []\n",
    "for mask_algo in mask_algos_labels:\n",
    "    for wind_source in wind_sources:\n",
    "        best_method = min(error_metrics[mask_algo][wind_source].items(), key=lambda x: x[1][\"MAPE (%)\"])\n",
    "        best_methods.append(\n",
    "            {\n",
    "                \"Masking Algorithm\": mask_algo,\n",
    "                \"Wind Source\": wind_source.upper(),\n",
    "                \"Best Method\": best_method[0],\n",
    "                \"MAPE (%)\": best_method[1][\"MAPE (%)\"],\n",
    "                \"RMSPE (%)\": best_method[1][\"RMSPE (%)\"],\n",
    "                \"MPE (%)\": best_method[1][\"MPE (%)\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "best_df = pd.DataFrame(best_methods)\n",
    "\n",
    "# Format the metrics to 2 decimal places\n",
    "for col in [\"MAPE (%)\", \"RMSPE (%)\", \"MPE (%)\"]:\n",
    "    best_df[col] = best_df[col].map(\"{:.2f}\".format)\n",
    "\n",
    "print(\"\\nBest Method by Masking Algorithm and Wind Source:\")\n",
    "display(best_df)\n",
    "\n",
    "# Create visualizations to compare masking algorithms\n",
    "# For each wind source, create a plot comparing the best method across masking algorithms\n",
    "for wind_source in wind_sources:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    markers = [\"o\", \"s\", \"^\"]\n",
    "    colors = [\"blue\", \"green\", \"red\"]\n",
    "\n",
    "    for i, mask_algo in enumerate(mask_algos_labels):\n",
    "        # Find the best method for this masking algorithm and wind source\n",
    "        best_method = min(error_metrics[mask_algo][wind_source].items(), key=lambda x: x[1][\"MAPE (%)\"])\n",
    "        best_method_name = best_method[0]\n",
    "\n",
    "        # Get the column name for this method\n",
    "        method_column = next(col for name, col in methods if name == best_method_name)\n",
    "        full_column = f\"{wind_source}_{method_column}\"\n",
    "\n",
    "        # Filter data for this masking algorithm\n",
    "        mask_df = all_results_df[all_results_df[\"masking_algorithm\"] == mask_algo]\n",
    "\n",
    "        plt.scatter(\n",
    "            mask_df[\"ground_truth_kg_h\"],\n",
    "            mask_df[full_column],\n",
    "            label=f\"{mask_algo} ({best_method_name})\",\n",
    "            marker=markers[i % len(markers)],\n",
    "            color=colors[i % len(colors)],\n",
    "            s=100,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    # Set reasonable x and y limits based on the data\n",
    "    max_gt = all_results_df[\"ground_truth_kg_h\"].max()\n",
    "    max_pred = (\n",
    "        all_results_df[[col for col in all_results_df.columns if col.startswith(f\"{wind_source}_Q_\")]].max().max()\n",
    "    )\n",
    "    max_value = max(max_gt, max_pred) * 1.1\n",
    "\n",
    "    plt.plot([0, max_value], [0, max_value], \"k--\", label=\"1:1 line\")\n",
    "    plt.xlim(0, max_gt * 1.1)\n",
    "    plt.ylim(0, max_gt * 2.0)\n",
    "    plt.xlabel(\"Ground Truth (kg/h)\", fontsize=12)\n",
    "    plt.ylabel(\"Predicted (kg/h)\", fontsize=12)\n",
    "    plt.title(f\"Best Methods by Masking Algorithm using {wind_source.upper()} Wind Data\", fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MAPE column from string to float before sorting\n",
    "metrics_df[\"MAPE (%)\"] = metrics_df[\"MAPE (%)\"].astype(float)\n",
    "# Sort the metrics dataframe by MAPE and display as markdown for easy copy-paste to GitHub/GitLab issues\n",
    "sorted_metrics_df = metrics_df.sort_values(by=\"MAPE (%)\")\n",
    "display(sorted_metrics_df)\n",
    "print(sorted_metrics_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all_results_df for 2024-12-19 and ERA5 wind source\n",
    "date_to_filter = \"2024-12-19\"\n",
    "wind_source = \"era5\"\n",
    "\n",
    "# Get all columns related to ERA5\n",
    "era5_columns = [col for col in all_results_df.columns if wind_source in col]\n",
    "\n",
    "# Create a filtered dataframe with date, masking_algorithm, ground_truth, and ERA5 columns\n",
    "filtered_df = all_results_df[all_results_df[\"date\"] == date_to_filter][\n",
    "    [\"date\", \"masking_algorithm\", \"ground_truth_kg_h\", *era5_columns]\n",
    "]\n",
    "\n",
    "# Display the filtered dataframe\n",
    "print(f\"\\nResults for {date_to_filter} with {wind_source.upper()} wind source:\")\n",
    "display(filtered_df)\n",
    "\n",
    "quant_columns = [col for col in era5_columns if \"L_\" in col]\n",
    "focused_df = all_results_df[all_results_df[\"date\"] == date_to_filter][\n",
    "    [\"date\", \"masking_algorithm\", \"ground_truth_kg_h\", *quant_columns]\n",
    "]\n",
    "\n",
    "print(f\"\\nQuantification results for {date_to_filter} with {wind_source.upper()} wind source:\")\n",
    "display(focused_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper functions for plotting ---\n",
    "\n",
    "\n",
    "def _get_plot_attributes(method_name, colors, markers):\n",
    "    \"\"\"Determine plot color and marker based on method name.\"\"\"\n",
    "    if \"conditional\" in method_name:\n",
    "        color = colors[\"conditional\"]\n",
    "    elif \"rescaled\" in method_name:\n",
    "        color = colors[\"rescaled\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scaling method in {method_name}\")\n",
    "\n",
    "    if \"sqrtA\" in method_name:\n",
    "        marker = markers[\"sqrtA\"]\n",
    "    elif \"major_axis\" in method_name or \"major\" in method_name:\n",
    "        marker = markers[\"major\"]\n",
    "    elif \"circle\" in method_name:\n",
    "        marker = markers[\"circle\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown lengthscale method in {method_name}\")\n",
    "    return color, marker\n",
    "\n",
    "\n",
    "def _plot_quantification_scatter(ax, df, wind_source, methods_to_plot, method_names, colors, markers):\n",
    "    \"\"\"Scatter plot comparing quantification methods.\"\"\"\n",
    "    max_pred_val = 0\n",
    "    for method_col, method_name in zip(methods_to_plot, method_names, strict=True):\n",
    "        full_col = f\"{wind_source}_{method_col}\"\n",
    "        if full_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        color, marker = _get_plot_attributes(method_name, colors, markers)\n",
    "        valid_preds = df[full_col].dropna()\n",
    "        if not valid_preds.empty:\n",
    "            max_pred_val = max(max_pred_val, valid_preds.max())\n",
    "\n",
    "        ax.scatter(\n",
    "            df[\"ground_truth_kg_h\"],\n",
    "            df[full_col],\n",
    "            label=method_name,\n",
    "            color=color,\n",
    "            marker=marker,\n",
    "            s=100,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    max_gt = df[\"ground_truth_kg_h\"].max() if not df[\"ground_truth_kg_h\"].empty else 0\n",
    "    max_val = max(max_gt, max_pred_val) * 1.1\n",
    "    max_val = max(max_val, 1.0)  # Ensure minimum limit\n",
    "\n",
    "    ax.plot([0, max_val], [0, max_val], \"k--\", label=\"1:1 line\")\n",
    "    ax.set_xlim(0, max(max_gt * 1.1, 1.0))\n",
    "    ax.set_ylim(0, max_val)\n",
    "    ax.set_xlabel(\"Ground Truth (kg/h)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Predicted (kg/h)\", fontsize=12)\n",
    "    ax.set_title(f\"Quantification Methods Comparison ({wind_source.upper()})\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "def _plot_error_bars(ax, df, wind_source, methods_to_plot, method_names, colors, markers):\n",
    "    \"\"\"Plot the bar chart of mean absolute percentage errors.\"\"\"\n",
    "    error_data = []\n",
    "    bar_colors = []\n",
    "    valid_method_names = []\n",
    "\n",
    "    for method_col, method_name in zip(methods_to_plot, method_names, strict=True):\n",
    "        full_col = f\"{wind_source}_{method_col}\"\n",
    "        if full_col in df.columns and not df[full_col].isna().all():\n",
    "            errors = (df[full_col] - df[\"ground_truth_kg_h\"]) / df[\"ground_truth_kg_h\"] * 100\n",
    "            errors = errors.dropna()  # Ensure we only use valid error calculations\n",
    "            if not errors.empty:\n",
    "                error_data.append(abs(errors).mean())\n",
    "                color, _ = _get_plot_attributes(method_name, colors, markers)\n",
    "                bar_colors.append(color)\n",
    "                valid_method_names.append(method_name)\n",
    "\n",
    "    if error_data:  # Only plot if there is data\n",
    "        ax.bar(valid_method_names, error_data, color=bar_colors, alpha=0.7)\n",
    "        ax.set_ylabel(\"Mean Absolute Percentage Error (%)\", fontsize=12)\n",
    "        ax.set_title(\"Error by Quantification Method\", fontsize=14)\n",
    "        plt.sca(ax)  # Set current axis for plt.xticks\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        ax.grid(True, axis=\"y\")\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No data to plot\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        ax.set_title(\"Error by Quantification Method\", fontsize=14)\n",
    "\n",
    "\n",
    "def _plot_time_series(ax, df, wind_source, methods_to_plot, method_names, colors, markers):\n",
    "    \"\"\"Plot the time series of predictions vs ground truth.\"\"\"\n",
    "    # Convert date strings to datetime for proper ordering\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values(\"datetime\")\n",
    "    dates = df[\"date\"].tolist()  # Use list of dates for ticks\n",
    "\n",
    "    # Plot ground truth\n",
    "    ax.plot(\n",
    "        dates,  # Use dates directly for x-axis\n",
    "        df[\"ground_truth_kg_h\"],\n",
    "        \"k-\",\n",
    "        marker=\"o\",\n",
    "        linewidth=2,\n",
    "        markersize=10,\n",
    "        label=\"Ground Truth\",\n",
    "    )\n",
    "\n",
    "    # Plot predictions for each method\n",
    "    for method_col, method_name in zip(methods_to_plot, method_names, strict=True):\n",
    "        full_col = f\"{wind_source}_{method_col}\"\n",
    "        if full_col in df.columns and not df[full_col].isna().all():\n",
    "            color, marker = _get_plot_attributes(method_name, colors, markers)\n",
    "            ax.plot(\n",
    "                dates,  # Use dates directly for x-axis\n",
    "                df[full_col],\n",
    "                marker=marker,\n",
    "                linestyle=\"-\",  # Ensure lines connect points\n",
    "                linewidth=1.5,\n",
    "                markersize=8,\n",
    "                label=method_name,\n",
    "                color=color,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax.set_ylabel(\"Quantification (kg/h)\", fontsize=12)\n",
    "    ax.set_title(\"Predictions vs Ground Truth Over Time\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.sca(ax)  # Set current axis for plt.xticks\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Adjust rotation for better readability\n",
    "\n",
    "\n",
    "def _calculate_and_display_stats(df, wind_source, methods_to_plot, method_names):\n",
    "    \"\"\"Calculate and display summary statistics.\"\"\"\n",
    "    stats_data = []\n",
    "    for method_col, method_name in zip(methods_to_plot, method_names, strict=True):\n",
    "        full_col = f\"{wind_source}_{method_col}\"\n",
    "        if full_col in df.columns and not df[full_col].isna().all():\n",
    "            # Calculate errors, handling potential NaNs\n",
    "            valid_indices = df[full_col].notna() & df[\"ground_truth_kg_h\"].notna()\n",
    "            preds = df.loc[valid_indices, full_col]\n",
    "            gt = df.loc[valid_indices, \"ground_truth_kg_h\"]\n",
    "\n",
    "            if not preds.empty:  # Proceed only if there are valid pairs\n",
    "                abs_errors = abs(preds - gt)\n",
    "                # Avoid division by zero or NaN result\n",
    "                epsilon = 1e-6\n",
    "                rel_errors = np.where(\n",
    "                    np.abs(gt) > epsilon,  # Use epsilon for safe division\n",
    "                    abs(preds - gt) / gt * 100,\n",
    "                    np.nan,  # Assign NaN if ground truth is near zero\n",
    "                )\n",
    "                rel_errors = rel_errors[~np.isnan(rel_errors)]  # Remove NaNs before aggregation\n",
    "\n",
    "                if len(rel_errors) > 0:  # Check if any valid relative errors exist\n",
    "                    stats_data.append(\n",
    "                        {\n",
    "                            \"Method\": method_name,\n",
    "                            \"MAE (kg/h)\": abs_errors.mean(),\n",
    "                            \"MAPE (%)\": np.mean(rel_errors),  # Use np.mean for NaN handling already done\n",
    "                            \"Min Error (%)\": np.min(rel_errors),\n",
    "                            \"Max Error (%)\": np.max(rel_errors),\n",
    "                            \"Std Dev (%)\": np.std(rel_errors),\n",
    "                        }\n",
    "                    )\n",
    "                else:  # Handle case with no valid relative errors\n",
    "                    stats_data.append(\n",
    "                        {\n",
    "                            \"Method\": method_name,\n",
    "                            \"MAE (kg/h)\": abs_errors.mean(),\n",
    "                            \"MAPE (%)\": np.nan,\n",
    "                            \"Min Error (%)\": np.nan,\n",
    "                            \"Max Error (%)\": np.nan,\n",
    "                            \"Std Dev (%)\": np.nan,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    if stats_data:\n",
    "        stats_df = pd.DataFrame(stats_data)\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        display(stats_df)\n",
    "    else:\n",
    "        print(\"\\nNo valid data to calculate statistics.\")\n",
    "\n",
    "\n",
    "# Plot results for two_pass_watershed and ERA5\n",
    "def plot_two_pass_era5_results():\n",
    "    \"\"\"\n",
    "    Plot analysis of two_pass_watershed algorithm with ERA5 wind data.\n",
    "\n",
    "    Creates a multi-panel figure showing:\n",
    "    1. Scatter plot comparing different quantification methods\n",
    "    2. Bar chart of percentage errors\n",
    "    3. Time series of predictions vs ground truth\n",
    "    4. Summary statistics table\n",
    "    \"\"\"\n",
    "    # Filter data for two_pass_watershed and ERA5\n",
    "    mask_algo = \"two_pass_watershed\"\n",
    "    wind_source = \"era5\"\n",
    "\n",
    "    # Ensure all_results_df is accessible (assuming it's defined globally in the notebook)\n",
    "    try:\n",
    "        filtered_df = all_results_df[all_results_df[\"masking_algorithm\"] == mask_algo].copy()\n",
    "    except NameError:\n",
    "        print(\"Error: all_results_df not found. Please ensure it is defined.\")\n",
    "        return\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data found for masking algorithm '{mask_algo}'. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # Define methods and plot attributes\n",
    "    methods_to_plot = [\"Q_circle_rescaled_kg_h\", \"Q_sqrtA_rescaled_kg_h\", \"Q_major_rescaled_kg_h\"]\n",
    "    method_names = [\"circle+rescaled\", \"sqrtA+rescaled\", \"major_axis+rescaled\"]\n",
    "    colors = {\"conditional\": \"blue\", \"rescaled\": \"red\"}\n",
    "    markers = {\"sqrtA\": \"^\", \"major\": \"s\", \"circle\": \"o\"}\n",
    "\n",
    "    # Create figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f\"Analysis for {mask_algo} with {wind_source.upper()} Wind Data\", fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Call helper functions to generate plots\n",
    "    _plot_quantification_scatter(axes[0], filtered_df, wind_source, methods_to_plot, method_names, colors, markers)\n",
    "    _plot_error_bars(axes[1], filtered_df, wind_source, methods_to_plot, method_names, colors, markers)\n",
    "    _plot_time_series(axes[2], filtered_df, wind_source, methods_to_plot, method_names, colors, markers)\n",
    "\n",
    "    # Calculate and display statistics (outside the plotting area)\n",
    "    _calculate_and_display_stats(filtered_df, wind_source, methods_to_plot, method_names)\n",
    "\n",
    "    # Remove the unused 4th subplot axis\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to prevent title overlap\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "plot_two_pass_era5_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations of the masks for all examples\n",
    "# Loop through all dates to visualize all three masking algorithms for each date\n",
    "for example_date, example_preds in predictions.items():\n",
    "    binary_probability = example_preds[\"binary_probability\"].numpy()\n",
    "\n",
    "    # Find the ground truth quantification for this date\n",
    "    gt_value = all_results_df[all_results_df[\"date\"] == example_date][\"ground_truth_kg_h\"].values\n",
    "    gt_str = f\"{gt_value[0]:.0f} kg/h\" if len(gt_value) > 0 else \"N/A\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "    # Plot the binary probability\n",
    "    im0 = axes[0].imshow(binary_probability, cmap=\"pink_r\", vmin=0, vmax=1)\n",
    "    axes[0].set_title(\"Binary Probability\")\n",
    "    grid16(axes[0])\n",
    "    plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "    # Plot each masking algorithm\n",
    "    for i, (mask_name, mask_func) in enumerate(masking_algorithms):\n",
    "        mask = mask_func(binary_probability)\n",
    "        im = axes[i + 1].imshow(mask, cmap=\"tab20\", interpolation=\"nearest\")\n",
    "        grid16(axes[i + 1])\n",
    "        axes[i + 1].set_title(f\"{mask_name}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Comparison of Masking Algorithms for {example_date} (Ground Truth: {gt_str})\", fontsize=16)\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization comparing the same method across different masking algorithms\n",
    "def visualize_method_across_masks(method_name, column_suffix):\n",
    "    \"\"\"\n",
    "    Visualize the performance of a specific quantification method across different masking algorithms.\n",
    "\n",
    "    Args:\n",
    "        method_name: Name of the method for the plot title\n",
    "        column_suffix: Column suffix for the method in the results dataframe\n",
    "    \"\"\"\n",
    "    wind_source = \"era5\"\n",
    "    full_column_prefix = f\"{wind_source}_{column_suffix}\"\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    markers = [\"o\", \"s\", \"^\"]\n",
    "    colors = [\"blue\", \"green\", \"red\"]\n",
    "\n",
    "    for i, mask_algo in enumerate(mask_algos_labels):\n",
    "        mask_df = all_results_df[all_results_df[\"masking_algorithm\"] == mask_algo]\n",
    "        full_column = f\"{wind_source}_{column_suffix}\"\n",
    "\n",
    "        if full_column in mask_df.columns and not mask_df[full_column].isna().all():\n",
    "            plt.scatter(\n",
    "                mask_df[\"ground_truth_kg_h\"],\n",
    "                mask_df[full_column],\n",
    "                label=f\"{mask_algo}\",\n",
    "                marker=markers[i % len(markers)],\n",
    "                color=colors[i % len(colors)],\n",
    "                s=100,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "    # Set reasonable x and y limits based on the data\n",
    "    max_gt = all_results_df[\"ground_truth_kg_h\"].max()\n",
    "    max_pred = all_results_df[[col for col in all_results_df.columns if col.startswith(full_column_prefix)]].max().max()\n",
    "    max_value = max(max_gt, max_pred) * 1.1\n",
    "\n",
    "    plt.plot([0, max_value], [0, max_value], \"k--\", label=\"1:1 line\")\n",
    "    plt.xlim(0, max_gt * 1.1)\n",
    "    plt.ylim(0, max_value)\n",
    "    plt.xlabel(\"Ground Truth (kg/h)\", fontsize=12)\n",
    "    plt.ylabel(\"Predicted (kg/h)\", fontsize=12)\n",
    "    plt.title(f\"{method_name} Method with {wind_source.upper()} Wind Data Across Masking Algorithms\", fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Compare circle+rescaled with sqrtA+conditional (a common baseline method)\n",
    "visualize_method_across_masks(\"circle+rescaled\", \"Q_circle_rescaled_kg_h\")\n",
    "visualize_method_across_masks(\"sqrtA+conditional\", \"Q_sqrtA_conditional_kg_h\")\n",
    "visualize_method_across_masks(\"sqrtA+rescaled\", \"Q_sqrtA_rescaled_kg_h\")\n",
    "visualize_method_across_masks(\"major_axis+rescaled\", \"Q_major_rescaled_kg_h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_retrievals_comparison(date_str, preds, vmax=0.1):\n",
    "    \"\"\"\n",
    "    Plot candidate retrieval methods for a given date's predictions.\n",
    "\n",
    "    Args:\n",
    "        date_str: Date string for the title\n",
    "        preds: Prediction dictionary containing the retrievals\n",
    "        vmax: Maximum value for colorbar (default: 0.1 mol/m2)\n",
    "    \"\"\"\n",
    "    binary_probability = preds[\"binary_probability\"].numpy()\n",
    "    marginal_retrieval = preds[\"marginal_retrieval\"]\n",
    "    conditional_retrieval = preds[\"conditional_retrieval\"]\n",
    "\n",
    "    cmap = \"pink_r\"\n",
    "\n",
    "    # Calculate rescaled retrieval\n",
    "    # Find the maximum binary probability in the central 20x20 pixels\n",
    "    h, w = binary_probability.shape\n",
    "    center_y, center_x = h // 2, w // 2\n",
    "    center_region = binary_probability[center_y - 10 : center_y + 10, center_x - 10 : center_x + 10]\n",
    "    max_prob = center_region.max()\n",
    "\n",
    "    # Rescale the marginal retrieval by the maximum probability\n",
    "    rescaled_retrieval = marginal_retrieval / max_prob if max_prob > 0 else marginal_retrieval.copy()\n",
    "\n",
    "    # Create the figure with 4 subplots\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "    # Plot binary probability\n",
    "    im0 = axes[0].imshow(binary_probability, cmap=\"pink_r\", vmin=0, vmax=1, interpolation=\"nearest\")\n",
    "    axes[0].set_title(\"Binary Probability\")\n",
    "    grid16(axes[0])\n",
    "    plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "    # Plot marginal retrieval\n",
    "    im1 = axes[1].imshow(marginal_retrieval, cmap=cmap, vmin=0, vmax=vmax, interpolation=\"nearest\")\n",
    "    axes[1].set_title(\"Marginal Retrieval (mol/m²)\")\n",
    "    grid16(axes[1])\n",
    "    plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "    # Plot conditional retrieval\n",
    "    im2 = axes[2].imshow(conditional_retrieval, cmap=cmap, vmin=0, vmax=vmax, interpolation=\"nearest\")\n",
    "    axes[2].set_title(\"Conditional Retrieval (mol/m²)\")\n",
    "    grid16(axes[2])\n",
    "    plt.colorbar(im2, ax=axes[2])\n",
    "\n",
    "    # Plot rescaled retrieval\n",
    "    im3 = axes[3].imshow(rescaled_retrieval, cmap=cmap, vmin=0, vmax=vmax, interpolation=\"nearest\")\n",
    "    axes[3].set_title(f\"Rescaled Retrieval (mol/m²)\\nMax Prob: {max_prob:.3f}\")\n",
    "    grid16(axes[3])\n",
    "    plt.colorbar(im3, ax=axes[3])\n",
    "\n",
    "    # Add ground truth info if available\n",
    "    gt_value = all_results_df[all_results_df[\"date\"] == date_str][\"ground_truth_kg_h\"].values\n",
    "    gt_str = f\"Ground Truth: {gt_value[0]:.0f} kg/h\" if len(gt_value) > 0 else \"\"\n",
    "\n",
    "    plt.suptitle(f\"Retrieval Comparison for {date_str} {gt_str}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot retrievals for each date\n",
    "for date_str, preds in predictions.items():\n",
    "    plot_retrievals_comparison(date_str, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Sanity checks with Gorroño plumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.validation.fpr_dt_pipeline import load_gorrono_plumes\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from src.azure_wrap.blob_storage_sdk_v2 import download_from_blob\n",
    "from src.azure_wrap.ml_client_utils import (\n",
    "    get_azureml_uri,\n",
    "    make_acceptable_uri,\n",
    ")\n",
    "\n",
    "\n",
    "def load_gorrono_plumes(ml_client) -> list[np.ndarray]:\n",
    "    \"\"\"Download Gorroño plumes to a temporary directory, convert them to mol/m² and return their paths.\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        temp_path = Path(temp_dir)\n",
    "        gorrono_plumes_uri = make_acceptable_uri(\n",
    "            str(get_azureml_uri(ml_client, \"orbio-data/methane_enhancements_molpercm2\"))\n",
    "        )\n",
    "        download_from_blob(gorrono_plumes_uri, temp_path, recursive=True)\n",
    "\n",
    "        # We need to convert the enhancements from mol/cm² to mol/m² as the radtran functions expect mol/m²\n",
    "        raw_enhancements = [np.load(temp_path / f\"{i}/methane_enhancement.npy\") * 1e4 for i in range(5)]\n",
    "\n",
    "    return raw_enhancements\n",
    "\n",
    "\n",
    "gorrono_plumes = load_gorrono_plumes(ml_client)\n",
    "gorrono_wind_speed = 3.5  # all Gorroño simulations were run with 3.5 m/s wind speed\n",
    "# according to the paper\n",
    "\n",
    "# Plot the plumes\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, plume in enumerate(gorrono_plumes):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(plume, cmap=\"pink_r\", vmin=0, vmax=0.2)\n",
    "    plt.title(f\"Plume {i+1}\\nmax: {plume.max():.2f} mol/m²\")\n",
    "    plt.xlim(200, 300)\n",
    "    plt.ylim(200, 300)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "concentration_thresholds = [0.0005, 0.001, 0.002, 0.005, 0.01]\n",
    "# def calculate_all_quantification_methods(\n",
    "#     plume_labels, binary_probability, conditional_retrieval, pixel_width, wind_speed, max_distance_pixels=10\n",
    "# ):\n",
    "all_quantifications = []\n",
    "for i, plume in enumerate(gorrono_plumes):\n",
    "    for thresh in concentration_thresholds:\n",
    "        binary_mask = plume > thresh\n",
    "        binary_probability = binary_mask.astype(float)  # 0 or 1, no uncertainty\n",
    "        conditional_retrieval = plume\n",
    "        pixel_width = 20  # Sentinel 2 resolution\n",
    "        wind_speed = gorrono_wind_speed\n",
    "        max_distance_pixels = 10\n",
    "        quantifications = calculate_all_quantification_methods(\n",
    "            binary_mask, binary_probability, conditional_retrieval, pixel_width, wind_speed, max_distance_pixels=10\n",
    "        )\n",
    "        quantifications[\"plume_index\"] = i + 1\n",
    "        quantifications[\"threshold\"] = thresh\n",
    "        all_quantifications.append(quantifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantifications_df = pd.DataFrame(all_quantifications)\n",
    "\n",
    "# Plot each plume with different thresholds\n",
    "\n",
    "\n",
    "for i in range(len(gorrono_plumes)):\n",
    "    df_sub = quantifications_df[quantifications_df[\"plume_index\"] == i + 1]\n",
    "    thresh = df_sub[\"threshold\"]\n",
    "    Q_sqrtA = df_sub[\"Q_sqrtA_conditional_kg_h\"]\n",
    "    Q_circle = df_sub[\"Q_circle_conditional_kg_h\"]\n",
    "    Q_major = df_sub[\"Q_major_conditional_kg_h\"]\n",
    "    plt.plot(thresh, Q_sqrtA, color=\"#0072B2\", marker=\"s\", linestyle=\"-\", label=\"sqrtA\" if i == 0 else \"\")\n",
    "    plt.plot(thresh, Q_circle, color=\"#D55E00\", marker=\"o\", linestyle=\"-\", label=\"circle\" if i == 0 else \"\")\n",
    "    plt.plot(thresh, Q_major, color=\"#009E73\", marker=\"^\", linestyle=\"-\", label=\"major\" if i == 0 else \"\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Concentration Threshold (mol/m²) for plume mask\")\n",
    "plt.ylabel(\"Quantification (kg/h)\")\n",
    "plt.grid(True)\n",
    "# Add a horizontal dashed line at 1000 kg/hr\n",
    "plt.axhline(y=1000, color=\"black\", linestyle=\"--\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.title(\"Quantifications of the Gorroño plumes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantifications_df[\"log_error_sqrtA\"] = np.log(quantifications_df[\"Q_sqrtA_conditional_kg_h\"] / 1000)\n",
    "quantifications_df[\"log_error_circle\"] = np.log(quantifications_df[\"Q_circle_conditional_kg_h\"] / 1000)\n",
    "quantifications_df[\"log_error_major\"] = np.log(quantifications_df[\"Q_major_conditional_kg_h\"] / 1000)\n",
    "quantifications_df.groupby(\"threshold\")[\"log_error_sqrtA\"].apply(lambda x: np.abs(x).median())\n",
    "quantifications_df.groupby(\"threshold\")[\"log_error_circle\"].apply(lambda x: np.abs(x).median())\n",
    "# Estimate the median absolute error in the logs of the major axis method\n",
    "quantifications_df.groupby(\"threshold\")[\"log_error_major\"].apply(lambda x: np.abs(x).median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_models = [\n",
    "    load_model_and_concatenator(model_id, device, SatelliteID.S2)[0]\n",
    "    for model_id in [\n",
    "        \"models:/torchgeo_pwr_unet/1395\",\n",
    "        \"models:/torchgeo_pwr_unet/1391\",\n",
    "        \"models:/torchgeo_pwr_unet/1486\",\n",
    "    ]\n",
    "]\n",
    "for model in alternative_models:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the alternative models to each date\n",
    "# code copy-pasted from the central predictions above\n",
    "# In reality, we'll also want to include a choice of reference tiles\n",
    "# in the ensemble.\n",
    "alternative_predictions = {}\n",
    "for _, plume_row in phase0_plumes.iterrows():\n",
    "    plume_date_str = plume_row[\"date\"]\n",
    "    plume_date = datetime.strptime(plume_date_str, \"%Y-%m-%d\")\n",
    "    ground_truth_kg_h = plume_row[\"quantification_kg_h\"]\n",
    "\n",
    "    print(f\"\\nProcessing plume from {plume_date_str} with ground truth {ground_truth_kg_h} kg/h\")\n",
    "\n",
    "    # Find the main data for this plume date\n",
    "    main_tile = None\n",
    "    for ref in reference_data:\n",
    "        ref_date = ref[\"tile_item\"].time.date().isoformat()\n",
    "        if ref_date == plume_date_str:\n",
    "            main_tile = ref\n",
    "            print(f\"Found main tile for {plume_date_str}: {ref['tile_item'].id}\")\n",
    "            break\n",
    "\n",
    "    if main_tile is None:\n",
    "        # Check if this plume date matches the main data loaded above\n",
    "        if main_data[\"tile_item\"].time.date().isoformat() == plume_date_str:\n",
    "            main_tile = main_data\n",
    "            print(f\"Found main tile in main_data for {plume_date_str}: {main_data['tile_item'].id}\")\n",
    "        else:\n",
    "            print(f\"No tile found for plume date {plume_date_str}, skipping\")\n",
    "            continue\n",
    "    # Find two reference tiles that don't have controlled releases\n",
    "    clean_refs = []\n",
    "\n",
    "    # Check if we have manual reference selections for this date\n",
    "    if plume_date_str in manual_reference_selections:\n",
    "        before_date, earlier_date = manual_reference_selections[plume_date_str]\n",
    "        print(f\"Using manually specified reference dates: {before_date} and {earlier_date}\")\n",
    "\n",
    "        # Find the reference tiles matching the manual selections\n",
    "        for ref in reference_data:\n",
    "            ref_date = ref[\"tile_item\"].time.date().isoformat()\n",
    "            if ref_date in (before_date, earlier_date):\n",
    "                clean_refs.append(ref)\n",
    "    else:\n",
    "        # Use the automatic selection logic\n",
    "        for ref in reference_data:\n",
    "            ref_date = ref[\"tile_item\"].time.date().isoformat()\n",
    "            if ref_date in cloudy_dates:\n",
    "                continue\n",
    "            if ref_date in phase0_release_dates:\n",
    "                continue\n",
    "            if ref_date >= plume_date_str:\n",
    "                continue\n",
    "            clean_refs.append(ref)\n",
    "\n",
    "    if len(clean_refs) < 2:  # noqa: PLR2004\n",
    "        print(f\"Not enough clean reference tiles found for {plume_date_str}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Sort by date (newest first) and take the two most recent\n",
    "    clean_refs.sort(key=lambda x: x[\"tile_item\"].time.date().isoformat(), reverse=True)\n",
    "    reference_chips = clean_refs[:2]\n",
    "\n",
    "    before_data = reference_chips[0]\n",
    "    earlier_data = reference_chips[1]\n",
    "\n",
    "    print(\"Using reference tiles from:\")\n",
    "    print(f\"  Before: {before_data['tile_item'].time.date().isoformat()}\")\n",
    "    print(f\"  Earlier: {earlier_data['tile_item'].time.date().isoformat()}\")\n",
    "\n",
    "    # Generate predictions\n",
    "    def get_preds_for_model(model):\n",
    "        \"\"\"Generate predictions for a given model.\"\"\"\n",
    "        preds = generate_predictions(main_tile, reference_chips, model, device, band_concatenator, lossFn)  # noqa: B023\n",
    "        preds = add_retrieval_to_pred(preds, main_tile[\"tile_item\"])  # noqa: B023\n",
    "        # Add timestamp to predictions\n",
    "        preds[\"timestamp\"] = main_tile[\"tile_item\"].time  # noqa: B023\n",
    "        preds[\"ground_truth_kg_h\"] = ground_truth_kg_h  # noqa: B023\n",
    "        return preds\n",
    "\n",
    "    all_model_preds = [get_preds_for_model(model) for model in alternative_models]\n",
    "\n",
    "    alternative_predictions[plume_date_str] = all_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for date_str, alt_preds in alternative_predictions.items():\n",
    "    central_preds = predictions[date_str]\n",
    "\n",
    "    def get_quantification(preds):\n",
    "        \"\"\"Calculate the emission rate quantification for a plume.\"\"\"\n",
    "        binary_probability = preds[\"binary_probability\"].numpy()\n",
    "\n",
    "        pixel_width = 20  # Sentinel 2 resolution\n",
    "        sensing_time = preds[\"timestamp\"]\n",
    "        wind_data = get_wind_data_from_csv(wind_data_df, sensing_time.date().isoformat())\n",
    "        wind_speed, wind_direction = wind_data[\"geos_fp\"]\n",
    "\n",
    "        # just do strict watershed here\n",
    "        labelled_mask = label(\n",
    "            retrieval_mask_using_watershed_algo(\n",
    "                WatershedParameters(\n",
    "                    marker_distance=1, marker_threshold=0.1, watershed_floor_threshold=0.1, closing_footprint_size=0\n",
    "                ),\n",
    "                binary_probability,\n",
    "            )\n",
    "        )\n",
    "        central_plume_id = find_central_plume(labelled_mask, max_distance_pixels=10, pixel_width=20)\n",
    "        central_plume_mask = labelled_mask == central_plume_id\n",
    "\n",
    "        conditional_retrieval = preds[\"conditional_retrieval\"]\n",
    "        marginal_retrieval = conditional_retrieval * binary_probability\n",
    "        rescaled_retrieval = marginal_retrieval / binary_probability[central_plume_mask].max()\n",
    "\n",
    "        L_major, IME, Q_major = calculate_major_axis_quantification(\n",
    "            central_plume_mask, rescaled_retrieval, pixel_width, wind_speed\n",
    "        )\n",
    "        return Q_major\n",
    "\n",
    "    Q_ensemble = [get_quantification(pred) for pred in alt_preds]\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Ensemble of quantification on {date_str}: {Q_ensemble}\")\n",
    "    Q_central = get_quantification(central_preds)\n",
    "    print(f\"Central quantification on {date_str}: {Q_central}\")\n",
    "\n",
    "    Q_interval = quantification_interval(Q_central, Q_ensemble)\n",
    "    print(f\"Quantification interval for {date_str}: {Q_interval}\")\n",
    "\n",
    "    # Extract ground truth value\n",
    "    ground_truth = central_preds[\"ground_truth_kg_h\"]\n",
    "\n",
    "    # Plot the point with error bars\n",
    "    plt.errorbar(\n",
    "        x=ground_truth,\n",
    "        y=Q_central,\n",
    "        yerr=[[Q_central - Q_interval[0]], [Q_interval[1] - Q_central]],\n",
    "        fmt=\"o\",\n",
    "        capsize=5,\n",
    "        markersize=8,\n",
    "        color=\"blue\",\n",
    "        ecolor=\"gray\",\n",
    "    )\n",
    "    # add a date label next to the point, angled at 45 degrees\n",
    "    month_day = date_str.split(\"-\")[1] + \"-\" + date_str.split(\"-\")[2]\n",
    "    plt.text(ground_truth, Q_central, month_day, fontsize=10, ha=\"left\", va=\"bottom\", color=\"grey\", rotation=45)\n",
    "# Add a diagonal line representing perfect estimation (y=x)\n",
    "plt.plot([0, 1500], [0, 1500], \"k--\", alpha=0.7, label=\"Perfect Estimation\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Ground Truth (kg/h)\", fontsize=12)\n",
    "plt.ylabel(\"Estimated Emission Rate (kg/h)\", fontsize=12)\n",
    "plt.title(\"Emission Rate Estimation with Uncertainty\", fontsize=14)\n",
    "\n",
    "# Add grid and legend\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the error bars if we use ground truth wind speed\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for date_str, alt_preds in alternative_predictions.items():\n",
    "    central_preds = predictions[date_str]\n",
    "\n",
    "    def get_quantification(preds):\n",
    "        \"\"\"Calculate the emission rate quantification for a plume.\"\"\"\n",
    "        binary_probability = preds[\"binary_probability\"].numpy()\n",
    "\n",
    "        pixel_width = 20  # Sentinel 2 resolution\n",
    "        sensing_time = preds[\"timestamp\"]\n",
    "        wind_data = get_wind_data_from_csv(wind_data_df, sensing_time.date().isoformat())\n",
    "        wind_speed, wind_direction = wind_data[\"ground_truth\"]\n",
    "\n",
    "        # just do strict watershed here\n",
    "        labelled_mask = label(\n",
    "            retrieval_mask_using_watershed_algo(\n",
    "                WatershedParameters(\n",
    "                    marker_distance=1, marker_threshold=0.1, watershed_floor_threshold=0.05, closing_footprint_size=0\n",
    "                ),\n",
    "                binary_probability,\n",
    "            )\n",
    "        )\n",
    "        central_plume_id = find_central_plume(labelled_mask, max_distance_pixels=10, pixel_width=20)\n",
    "        central_plume_mask = labelled_mask == central_plume_id\n",
    "\n",
    "        conditional_retrieval = preds[\"conditional_retrieval\"]\n",
    "        marginal_retrieval = conditional_retrieval * binary_probability\n",
    "        rescaled_retrieval = marginal_retrieval / binary_probability[central_plume_mask].max()\n",
    "\n",
    "        L_major, IME, Q_major = calculate_major_axis_quantification(\n",
    "            central_plume_mask,\n",
    "            rescaled_retrieval,\n",
    "            pixel_width,\n",
    "            wind_speed,\n",
    "        )\n",
    "        return Q_major\n",
    "\n",
    "    Q_ensemble = [get_quantification(pred) for pred in alt_preds]\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Ensemble of quantification on {date_str}: {Q_ensemble}\")\n",
    "    Q_central = get_quantification(central_preds)\n",
    "    print(f\"Central quantification on {date_str}: {Q_central}\")\n",
    "\n",
    "    Q_interval = quantification_interval(Q_central, Q_ensemble, wind_MdALE=0.0)  # using ground truth wind speed\n",
    "    print(f\"Quantification interval for {date_str}: {Q_interval}\")\n",
    "\n",
    "    # Extract ground truth value\n",
    "    ground_truth = central_preds[\"ground_truth_kg_h\"]\n",
    "\n",
    "    # Plot the point with error bars\n",
    "    plt.errorbar(\n",
    "        x=ground_truth,\n",
    "        y=Q_central,\n",
    "        yerr=[[Q_central - Q_interval[0]], [Q_interval[1] - Q_central]],\n",
    "        fmt=\"o\",\n",
    "        capsize=5,\n",
    "        markersize=8,\n",
    "        color=\"blue\",\n",
    "        ecolor=\"gray\",\n",
    "    )\n",
    "    # add a date label next to the point, angled at 45 degrees\n",
    "    month_day = date_str.split(\"-\")[1] + \"-\" + date_str.split(\"-\")[2]\n",
    "    plt.text(ground_truth, Q_central, month_day, fontsize=10, ha=\"left\", va=\"bottom\", color=\"grey\", rotation=45)\n",
    "# Add a diagonal line representing perfect estimation (y=x)\n",
    "plt.plot([0, 1500], [0, 1500], \"k--\", alpha=0.7, label=\"Perfect Estimation\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Ground Truth (kg/h)\", fontsize=12)\n",
    "plt.ylabel(\"Estimated Emission Rate (kg/h)\", fontsize=12)\n",
    "plt.title(\"Emission Rate Estimation with Ground Truth Wind Speed\", fontsize=14)\n",
    "\n",
    "# Add grid and legend\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_day = wind_data_df[wind_data_df[\"date\"] == \"2024-11-22\"].iloc[0]\n",
    "print(f\"GEOS-FP wind speed: {np.sqrt(wind_day['geos_ux']**2 + wind_day['geos_uy']**2):.1f}\")\n",
    "print(f\"ERA5 wind speed: {np.sqrt(wind_day['era5_ux']**2 + wind_day['era5_uy']**2):.1f}\")\n",
    "print(f\"Ground truth wind speed: {np.sqrt(wind_day['gt_ux']**2 + wind_day['gt_uy']**2):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_data_df[\"geos_fp\"] = np.sqrt(wind_data_df[\"geos_ux\"] ** 2 + wind_data_df[\"geos_uy\"] ** 2)\n",
    "wind_data_df[\"era5\"] = np.sqrt(wind_data_df[\"era5_ux\"] ** 2 + wind_data_df[\"era5_uy\"] ** 2)\n",
    "wind_data_df[\"gt\"] = np.sqrt(wind_data_df[\"gt_ux\"] ** 2 + wind_data_df[\"gt_uy\"] ** 2)\n",
    "print(f\"GEOS-FP MdALE: {np.nanmedian(np.abs(np.log(wind_data_df['geos_fp']) - np.log(wind_data_df['gt']))):.2f}\")\n",
    "print(f\"ERA5 MdALE: {np.nanmedian(np.abs(np.log(wind_data_df['era5']) - np.log(wind_data_df['gt']))):.2f}\")\n",
    "plt.plot(wind_data_df[\"gt\"], wind_data_df[\"era5\"], \"o\")\n",
    "plt.plot(wind_data_df[\"gt\"], wind_data_df[\"geos_fp\"], \"p\")\n",
    "plt.xlabel(\"Ground truth wind speed (m/s)\")\n",
    "plt.ylabel(\"Estimated wind speed (m/s)\")\n",
    "plt.grid(True)\n",
    "plt.plot([0, 10], [0, 10], \"k--\", alpha=0.7, label=\"1:1 line\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
