{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "from lib.models.schemas import WatershedParameters\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sbr_2025 import BANDS_LANDSAT\n",
    "from sbr_2025.utils import intersects_center, select_reference_tiles_from_dates_str\n",
    "from sbr_2025.utils.plotting import (\n",
    "    Colorbar,\n",
    "    all_error_analysis_plots,\n",
    "    get_band_ratio_landsat,\n",
    "    get_rgb_bands_landsat,\n",
    "    plot_all_ratio,\n",
    "    plot_all_rgb,\n",
    "    plot_max_proba_center_buffer_heatmap,\n",
    "    plot_normal_and_avg_strategy,\n",
    "    plot_normal_and_avg_strategy_summary,\n",
    "    plot_ratio_diffs,\n",
    "    plot_rgb_ratio,\n",
    "    plot_wind,\n",
    "    validate_pred_retrievals,\n",
    ")\n",
    "from sbr_2025.utils.prediction import (\n",
    "    PlumeInfo,\n",
    "    TileInfo,\n",
    "    get_center_buffer,\n",
    "    get_reference_data,\n",
    "    get_reference_data_before,\n",
    "    predict,\n",
    "    predict_for_all_pairs,\n",
    ")\n",
    "from sbr_2025.utils.quantification import (\n",
    "    get_wind_components,\n",
    "    sbr_form_outputs,\n",
    ")\n",
    "from src.azure_wrap.ml_client_utils import initialize_blob_service_client\n",
    "from src.data.landsat_data import LandsatGranuleAccess\n",
    "from src.data.sentinel2 import Sentinel2Item\n",
    "from src.inference.inference_functions import (\n",
    "    crop_main_data_landsat,\n",
    "    crop_reference_data_landsat,\n",
    "    fetch_landsat_items_for_point,\n",
    "    query_landsat_catalog_for_point,\n",
    ")\n",
    "from src.inference.inference_target_location import (\n",
    "    quantify_retrieval,\n",
    ")\n",
    "from src.plotting.plotting_functions import grid16\n",
    "from src.training.loss_functions import TwoPartLoss\n",
    "from src.utils.parameters import LANDSAT_HAPI_DATA_PATH, S2_HAPI_DATA_PATH, SatelliteID\n",
    "from src.utils.quantification_utils import calc_effective_wind_speed, calc_wind_direction\n",
    "from src.utils.radtran_utils import RadTranLookupTable\n",
    "from src.utils.utils import initialize_clients, load_model_and_concatenator\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "ðŸ””ðŸ””ðŸ””ðŸ””\n",
    "\n",
    "**HOW TO UPDATE THIS NOTEBOOK**\n",
    "\n",
    "This is a template notebook that is under version control.\n",
    "When making changes, before staging and committing the changes, run `nbstripout` to remove the output from the notebook.\n",
    "\n",
    "```bash\n",
    "# conda install -c conda-forge nbstripout  # install nbstripout if not already installed\n",
    "nbstripout --drop-empty-cells --extra-keys=\"metadata.kernelspec.display_name metadata.kernelspec.name\" Landsat_SBR_exploration.ipynb\n",
    "git add -p Landsat_SBR_exploration.ipynb\n",
    "```\n",
    "\n",
    "ðŸ””ðŸ””ðŸ””ðŸ””"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client, _, _, _, s3_client = initialize_clients(False)\n",
    "abs_client = initialize_blob_service_client(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon = 32.82175, -111.78581  # release point for SBRs\n",
    "\n",
    "# can we predict on larger chips?\n",
    "crop_size = 256  # 128\n",
    "center_buffer = 12  # 5  # number of pixels to search from center\n",
    "\n",
    "# Date range of SBRs\n",
    "start_date = \"2025-01-01\"\n",
    "end_date = \"2025-04-30\"  # a month after the end of the SBRs to include reference images after\n",
    "\n",
    "# these dates have KNOWN releases during Phase 0\n",
    "phase0_release_dates = [\n",
    "    # \"2024-11-14\",\n",
    "    \"2024-11-15\",  # LS\n",
    "    \"2024-11-16\",  # LS\n",
    "    # \"2024-11-19\",\n",
    "    # \"2024-11-22\",\n",
    "    \"2024-11-24\",  # LS\n",
    "    \"2024-12-02\",  # LS\n",
    "    # \"2024-12-04\",\n",
    "    # \"2024-12-07\",\n",
    "    \"2024-12-09\",  # LS\n",
    "    # \"2024-12-17\",\n",
    "    # \"2024-12-19\",\n",
    "    # \"2024-12-22\",\n",
    "    \"2024-12-26\",  # LS\n",
    "    # \"2024-12-29\",\n",
    "]\n",
    "\n",
    "release_dates_2022 = [\n",
    "    \"2022-10-25\",\n",
    "    \"2022-10-26\",\n",
    "    \"2022-11-02\",\n",
    "    \"2022-11-03\",\n",
    "    \"2022-11-10\",\n",
    "    \"2022-11-11\",\n",
    "    \"2022-11-18\",\n",
    "]\n",
    "\n",
    "watershed_params = WatershedParameters(\n",
    "    marker_distance=1,\n",
    "    marker_threshold=0.1,\n",
    "    watershed_floor_threshold=0.075,\n",
    "    closing_footprint_size=0,\n",
    ")\n",
    "item_meta_dict = {}\n",
    "\n",
    "start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(end_date, \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Models\n",
    "- 37: Train for longer (Production model)\n",
    "  - [Ep60] mAvgRecall: 40.3% (Hassi: 71.3%, Marcellus:  6.8%, Permian: 42.8%)\n",
    "- 155: b3 encoder\n",
    "  - [Ep50] mAvgRecall: 41.2% (Hassi: 72.3%, Marcellus:  7.4%, Permian: 43.8%)\n",
    "- 55: Bigger encoder b2\n",
    "  - [Ep68] mAvgRecall: 40.7% (Hassi: 71.4%, Marcellus:  8.0%, Permian: 42.6%)\n",
    "- 168: Bigger encoder b2\n",
    "  - [Ep70] mAvgRecall: 40.6% (Hassi: 71.0%, Marcellus:  7.5%, Permian: 43.4%)\n",
    "- 57: More bands, b1\n",
    "  - [Ep72] mAvgRecall: 40.3% (Hassi: 73.0%, Marcellus:  6.1%, Permian: 41.8%)\n",
    "- 103: Only deserts model, Hassi val\n",
    "  - [Ep28] mAvgRecall: 71.8% (Hassi: 71.8%)\n",
    "- 154: No modulation schedule (should be slightly worse in picking up extremely faint plumes)\n",
    "  - [Ep56] mAvgRecall: 40.9% (Hassi: 68.8%, Marcellus: 11.3%, Permian: 42.6%)\n",
    "- 14: First model we trained\n",
    "  - [Ep68] mAvgRecall: 39.7% (Hassi: 70.0%, Marcellus:  6.7%, Permian: 42.4%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ids = [\"37\", \"155\", \"55\", \"57\", \"103\", \"168\", \"154\", \"14\"]\n",
    "models = []\n",
    "band_concatenators = []\n",
    "for model_id in model_ids:\n",
    "    model, band_concatenator, train_params = load_model_and_concatenator(\n",
    "        f\"models:/landsat/{model_id}\", device, SatelliteID.LANDSAT\n",
    "    )\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    band_concatenators.append(band_concatenator)\n",
    "\n",
    "lossFn = TwoPartLoss(train_params[\"binary_threshold\"], train_params[\"MSE_multiplier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Overpass Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_items = query_landsat_catalog_for_point(lat, lon, start_date, end_date)\n",
    "for item in stac_items:\n",
    "    cloud_cover = item.properties[\"eo:cloud_cover\"]\n",
    "    print(\n",
    "        f\"{item.datetime.date().isoformat()} with {cloud_cover:6.1f}% clouds - ({item.id})\"\n",
    "        f\"and CRS {item.properties['proj:code']}\"\n",
    "    )\n",
    "overpass_dates = [item.datetime.date().isoformat() for item in stac_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# choose a single date to do inference\n",
    "target_date = stac_items[-1].datetime.date().isoformat()\n",
    "target_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "\n",
    "items = fetch_landsat_items_for_point(lat=lat, lon=lon, query_datetime=target_date, how_many_days_back=180)\n",
    "\n",
    "main_data = crop_main_data_landsat(items, abs_client, s3_client, lat, lon, crop_size)\n",
    "main_item = main_data[\"tile_item\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tile info for all overpasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata into the item\n",
    "main_item.load_metadata(s3_client, abs_client)\n",
    "\n",
    "tile_properties = TileInfo(\n",
    "    instrument_name=main_item.instrument_name,\n",
    "    date_analysis=datetime.today().date().isoformat(),\n",
    "    observation_date=main_item.time.date().isoformat(),\n",
    "    observation_timestamp=main_item.time.time().isoformat(timespec=\"seconds\"),\n",
    "    start_time=None,\n",
    "    end_time=None,\n",
    "    imaging_mode=main_item.imaging_mode,\n",
    "    off_nadir_angle=main_item.off_nadir_angle,\n",
    "    viewing_azimuth=main_item.viewing_azimuth,\n",
    "    solar_zenith=main_item.solar_zenith,\n",
    "    solar_azimuth=main_item.solar_azimuth,\n",
    "    orbit_state=None,\n",
    ")\n",
    "\n",
    "pprint.pp(tile_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_infos = []\n",
    "for t in tqdm(sorted(overpass_dates)):\n",
    "    # print(f\"Date: {target_date}\")\n",
    "    target_date = datetime.strptime(t, \"%Y-%m-%d\")\n",
    "\n",
    "    items = fetch_landsat_items_for_point(lat=lat, lon=lon, query_datetime=target_date)\n",
    "\n",
    "    main_data = crop_main_data_landsat(items, abs_client, s3_client, lat, lon, crop_size)\n",
    "    main_item = main_data[\"tile_item\"]\n",
    "    main_item.load_metadata(s3_client, abs_client)\n",
    "\n",
    "    tile_properties = TileInfo(\n",
    "        instrument_name=main_item.item.properties[\"platform\"],\n",
    "        date_analysis=datetime.today().date().isoformat(),\n",
    "        observation_date=main_item.time.date().isoformat(),\n",
    "        observation_timestamp=main_item.time.time().isoformat(timespec=\"seconds\"),\n",
    "        start_time=None,\n",
    "        end_time=None,\n",
    "        imaging_mode=main_item.imaging_mode,\n",
    "        off_nadir_angle=main_item.off_nadir_angle,\n",
    "        viewing_azimuth=main_item.viewing_azimuth,\n",
    "        solar_zenith=main_item.solar_zenith,\n",
    "        solar_azimuth=main_item.solar_azimuth,\n",
    "    )\n",
    "\n",
    "    tile_infos.append(tile_properties.asdict())\n",
    "\n",
    "df = pd.DataFrame(tile_infos, index=overpass_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if date use conversion to mm/dd/yyy\n",
    "# pd.to_datetime(df.observation_date).sort_values().dt.strftime('%m/%d/%Y')\n",
    "\n",
    "for x in list(df.solar_azimuth):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Load in Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 25% clouds/shadows cutoff to include non-obvious reference images that may help\n",
    "max_bad_pixel_perc = 25  # max % sum of clouds/cloud shadows/nodata in reference chips\n",
    "num_snapshots = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the slowest part of the next function,\n",
    "# getting the raster metadata once here speeds it up by a lot\n",
    "for item in tqdm(items):\n",
    "    if item.id not in item_meta_dict:\n",
    "        # Get the Landsat metadata from the coastal band (arbitrary choice).\n",
    "        try:\n",
    "            item_meta_dict[item.id] = item.get_raster_meta(\"coastal\", abs_client=abs_client)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "reference_data = crop_reference_data_landsat(\n",
    "    items,\n",
    "    main_data,\n",
    "    abs_client,\n",
    "    s3_client,\n",
    "    lat,\n",
    "    lon,\n",
    "    crop_size,\n",
    "    required_num_previous_snapshots=num_snapshots,\n",
    "    max_bad_pixel_perc=max_bad_pixel_perc,\n",
    "    item_meta_dict=item_meta_dict,\n",
    ")\n",
    "\n",
    "print(\"\\n---: Summary\")\n",
    "print(f\"Main tile for {target_date}: USE {main_item.id}\")\n",
    "for reference in reference_data:\n",
    "    tile_id = reference[\"tile_item\"].id\n",
    "    print(f\"Reference tile for {target_date}: USE {tile_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## I. RGBs & Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show all of them together\n",
    "plot_all_rgb([main_data, *reference_data], SatelliteID.LANDSAT)\n",
    "plot_all_ratio([main_data, *reference_data], SatelliteID.LANDSAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show them individually\n",
    "data_items = [main_data, *reference_data]\n",
    "\n",
    "for i, data in enumerate(data_items):\n",
    "    date = data[\"tile_item\"].time.date().isoformat()\n",
    "    plot_rgb_ratio(data, Colorbar.INDIVIDUAL, i, SatelliteID.LANDSAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ratio Diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_ratio_diffs(data_items, Colorbar.INDIVIDUAL, SatelliteID.LANDSAT, mean_adjust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## II. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Predict for a specific date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Select Reference Chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "overpass_dates = [item.datetime.date().isoformat() for item in stac_items][::-1]\n",
    "overpass_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reference tile dates:\")\n",
    "for x in reference_data:\n",
    "    reference_date = x[\"tile_item\"].time.date().isoformat()\n",
    "    if reference_date in overpass_dates:\n",
    "        print(f\"{reference_date} - possible SBR release (date is in Phase 1)\")\n",
    "    elif reference_date in phase0_release_dates:\n",
    "        print(f\"{reference_date} - known SBR release (date is in Phase 0)\")\n",
    "    else:\n",
    "        print(f\"{reference_date} - not a Phase 1 overpass date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"55\"\n",
    "model_idx = model_id.index(model_id)\n",
    "\n",
    "before_date = \"2025-03-08\"\n",
    "earlier_date = \"2025-02-28\"\n",
    "\n",
    "reference_chips = select_reference_tiles_from_dates_str(\n",
    "    reference_data, before_date=before_date, earlier_date=earlier_date\n",
    ")\n",
    "before_data = reference_chips[0]\n",
    "earlier_data = reference_chips[1]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"tile\": [\"main\", \"before\", \"earlier\"],\n",
    "        \"date\": [\n",
    "            main_data[\"tile_item\"].time.date().isoformat(),\n",
    "            before_data[\"tile_item\"].time.date().isoformat(),\n",
    "            earlier_data[\"tile_item\"].time.date().isoformat(),\n",
    "        ],\n",
    "        \"time\": [\n",
    "            main_data[\"tile_item\"].time.time().isoformat(),\n",
    "            before_data[\"tile_item\"].time.time().isoformat(),\n",
    "            earlier_data[\"tile_item\"].time.time().isoformat(),\n",
    "        ],\n",
    "        \"datetime\": [main_data[\"tile_item\"].time, before_data[\"tile_item\"].time, earlier_data[\"tile_item\"].time],\n",
    "        \"observation_angle\": [\n",
    "            main_data[\"tile_item\"].observation_angle,\n",
    "            before_data[\"tile_item\"].observation_angle,\n",
    "            earlier_data[\"tile_item\"].observation_angle,\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "watershed_params = WatershedParameters(\n",
    "    marker_distance=1,\n",
    "    marker_threshold=0.1,\n",
    "    watershed_floor_threshold=0.075,\n",
    "    closing_footprint_size=0,\n",
    ")\n",
    "prediction = predict(\n",
    "    main_data,\n",
    "    reference_chips,\n",
    "    watershed_params,\n",
    "    models[model_idx],\n",
    "    device,\n",
    "    band_concatenators[model_idx],\n",
    "    lossFn,\n",
    "    create_lookup_table=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_main = get_rgb_bands_landsat(main_data[\"crop_arrays\"], BANDS_LANDSAT)\n",
    "rgb_before = get_rgb_bands_landsat(before_data[\"crop_arrays\"], BANDS_LANDSAT)\n",
    "rgb_earlier = get_rgb_bands_landsat(earlier_data[\"crop_arrays\"], BANDS_LANDSAT)\n",
    "\n",
    "ratio_main = get_band_ratio_landsat(main_data[\"crop_arrays\"], BANDS_LANDSAT)\n",
    "ratio_before = get_band_ratio_landsat(before_data[\"crop_arrays\"], BANDS_LANDSAT)\n",
    "ratio_earlier = get_band_ratio_landsat(earlier_data[\"crop_arrays\"], BANDS_LANDSAT)\n",
    "\n",
    "date_main = main_data[\"tile_item\"].time.date().isoformat()\n",
    "date_before = before_data[\"tile_item\"].time.date().isoformat()\n",
    "date_earlier = earlier_data[\"tile_item\"].time.date().isoformat()\n",
    "\n",
    "plot = all_error_analysis_plots(\n",
    "    rgb_main=rgb_main,\n",
    "    rgb_before=rgb_before,\n",
    "    rgb_earlier=rgb_earlier,\n",
    "    ratio_main=ratio_main,\n",
    "    ratio_before=ratio_before,\n",
    "    ratio_earlier=ratio_earlier,\n",
    "    predicted_frac=prediction.marginal,\n",
    "    predicted_mask=prediction.mask,\n",
    "    conditional_pred=prediction.conditional,\n",
    "    binary_probability=prediction.binary_probability,\n",
    "    conditional_retrieval=prediction.conditional_retrieval,\n",
    "    masked_conditional_retrieval=prediction.masked_conditional_retrieval,\n",
    "    rescaled_retrieval=None,  # We will plot this in the Retrieval and quantification section\n",
    "    marginal_retrieval=prediction.marginal_retrieval,\n",
    "    watershed_segmentation_params=watershed_params,\n",
    "    dates=(date_main, date_before, date_earlier),\n",
    "    ratio_colorbar=Colorbar.SHARE,  # (0.65, 1.0),\n",
    "    ratio_diff_colorbar=Colorbar.SHARE,  # (-0.1, 0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Retrieval for the biggest plume in the center\n",
    "\n",
    "crop_x = main_data[\"crop_params\"][\"swir22\"][\"crop_start_x\"]\n",
    "crop_y = main_data[\"crop_params\"][\"swir22\"][\"crop_start_y\"]\n",
    "prediction.crop_x = crop_x\n",
    "prediction.crop_y = crop_y\n",
    "pred_info = prediction.asdict()\n",
    "\n",
    "timestamp = main_item.time\n",
    "raster_meta = main_item.get_raster_meta(\"swir22\", abs_client=abs_client)\n",
    "crop_crs = raster_meta[\"crs\"]\n",
    "\n",
    "window = rasterio.windows.Window(crop_x, crop_y, crop_size, crop_size)\n",
    "crop_transform = rasterio.windows.transform(window, raster_meta[\"transform\"])\n",
    "\n",
    "# Quantify plumes in retrieval\n",
    "plume_list = quantify_retrieval(\n",
    "    prediction.conditional_retrieval,\n",
    "    crop_transform,\n",
    "    crop_crs,\n",
    "    prediction.binary_probability,\n",
    "    timestamp.date().isoformat(),\n",
    "    floor_t=watershed_params.watershed_floor_threshold,\n",
    "    marker_t=watershed_params.marker_threshold,\n",
    "    spatial_resolution=30.0,\n",
    ")\n",
    "\n",
    "if len(plume_list) == 0:\n",
    "    print(\"No plumes detected\")\n",
    "else:\n",
    "    print(f\"Found {len(plume_list)} plumes\")\n",
    "\n",
    "    # Get Retrieval for the biggest plume in the center\n",
    "    plume_list = sorted(plume_list, key=lambda x: x[\"properties\"][\"Q\"])[::-1]\n",
    "    center_plumes: PlumeInfo = []\n",
    "    for plume in plume_list:\n",
    "        plume_info = PlumeInfo(**plume[\"properties\"])\n",
    "\n",
    "        intersects = intersects_center(*plume_info.bbox, buffer=center_buffer)\n",
    "        if intersects:\n",
    "            pprint.pp(plume_info)\n",
    "            center_plumes.append(plume)\n",
    "\n",
    "    print(f\"{len(center_plumes)} plumes intersect the center with {center_buffer} pixels of buffer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Heatmap of Max Probability + Binary Plot for Each Reference Chip Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"55\"\n",
    "model_idx = model_ids.index(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_for_all_pairs(\n",
    "    main_data,\n",
    "    reference_data,\n",
    "    models[model_idx],\n",
    "    device,\n",
    "    band_concatenators[model_idx],\n",
    "    lossFn,\n",
    "    watershed_params,\n",
    "    skip_retrieval=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [d[\"tile_item\"].time for d in reference_data]\n",
    "\n",
    "plot_max_proba_center_buffer_heatmap(\n",
    "    predictions=predictions,\n",
    "    dates=dates,\n",
    "    center_buffer=center_buffer,\n",
    "    title=f'Model {model_id} for main={target_date.isoformat().split(\"T\")[0]}',\n",
    "    satellite_id=SatelliteID.LANDSAT,\n",
    "    show_topk=5,\n",
    "    main_date=target_date.isoformat().split(\"T\")[0],\n",
    "    plot_binary_grid=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Predict for multiple dates with \"Normal\" and Average Reference images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Prepare data for all dates we want to predict for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main_data_all = {}\n",
    "reference_data_all = {}\n",
    "for item in stac_items:\n",
    "    target_date = item.datetime.date().isoformat()\n",
    "    target_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n",
    "\n",
    "    items = fetch_landsat_items_for_point(lat=lat, lon=lon, query_datetime=target_date)\n",
    "\n",
    "    main_data = crop_main_data_landsat(items, abs_client, s3_client, lat, lon, crop_size)\n",
    "    main_data_all[target_date] = main_data\n",
    "\n",
    "    main_item = main_data[\"tile_item\"]\n",
    "    print(f\"Main tile for {target_date}: USE {main_item.id}\")\n",
    "\n",
    "    # This is the slowest part of the next function,\n",
    "    # getting the raster metadata once here speeds it up by a lot\n",
    "    for item_ in items:\n",
    "        if item_.id not in item_meta_dict:\n",
    "            item_.prefetch_l1(s3_client, abs_client)\n",
    "            # Get the Landsat metadata from the coastal band (arbitrary choice).\n",
    "            item_meta_dict[item_.id] = item_.get_raster_meta(\"coastal\", abs_client=abs_client)\n",
    "\n",
    "    reference_data = crop_reference_data_landsat(\n",
    "        items,\n",
    "        main_data,\n",
    "        abs_client,\n",
    "        s3_client,\n",
    "        lat,\n",
    "        lon,\n",
    "        crop_size,\n",
    "        required_num_previous_snapshots=num_snapshots,\n",
    "        max_bad_pixel_perc=max_bad_pixel_perc,\n",
    "        item_meta_dict=item_meta_dict,\n",
    "    )\n",
    "    reference_data_all[target_date] = reference_data\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs, sums, avg_ref_counts = plot_normal_and_avg_strategy(\n",
    "    stac_items,\n",
    "    main_data_all,\n",
    "    reference_data_all,\n",
    "    phase0_release_dates,\n",
    "    models,\n",
    "    model_ids,\n",
    "    band_concatenators,\n",
    "    device,\n",
    "    lossFn,\n",
    "    watershed_params,\n",
    "    center_buffer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_and_avg_strategy_summary(sums, avg_ref_counts, model_ids, ylim=[-2, 30], buffer_width=center_buffer * 2 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "# In detail: Inspection and Submission of individual dates (Q1 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare wind data once\n",
    "wind_data = pd.read_csv(\"../../src/data/ancillary/wind_vectors_gt_vs_models_2025_sbr_sites.csv\")\n",
    "wind_data = wind_data[wind_data[\"sensor\"] == \"LS\"]\n",
    "wind_data[\"sensing_time\"] = pd.to_datetime(\n",
    "    wind_data[\"date\"] + \" \" + wind_data[\"overpass_time_utc\"], utc=True\n",
    ").dt.strftime(\"%Y-%m-%dT%H:%M:%S+0000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client, _, _, _, s3_client = initialize_clients(False)\n",
    "abs_client = initialize_blob_service_client(ml_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# choose a single date to do inference\n",
    "stac_idx = 19\n",
    "target_date = items[stac_idx].time\n",
    "print(target_date.isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudy_hazy_dates = [\n",
    "    \"2025-01-03\",  # hazy\n",
    "    \"2025-03-16\",  # hazy\n",
    "    \"2025-02-12\",  # cloudy\n",
    "    \"2025-03-23\",  # cloudy\n",
    "    \"2025-03-15\",  # cloudy\n",
    "    \"2025-03-07\",  # cloudy\n",
    "    \"2025-02-04\",  # cloudy\n",
    "]\n",
    "\n",
    "dark_ratio_dates = [\n",
    "    \"2025-01-27\",  # dark ratio, mean 0.792 (normal is ~0.84)\n",
    "    \"2025-03-08\",  # dark ratio, mean 0.785 (normal is ~0.84)\n",
    "]\n",
    "\n",
    "# Following dates we determined there had been an emission\n",
    "phase1_believed_release_dates = [\n",
    "    \"2025-01-26\",\n",
    "    \"2025-02-28\",\n",
    "    \"2025-03-16\",\n",
    "    \"2025-03-24\",\n",
    "]\n",
    "\n",
    "exclude_dates = [\n",
    "    # \"2024-12-02\",\n",
    "    *phase0_release_dates,\n",
    "    *cloudy_hazy_dates,\n",
    "    *dark_ratio_dates,\n",
    "    *phase1_believed_release_dates,\n",
    "]\n",
    "\n",
    "print(f\"excluding {len(exclude_dates)} dates\")\n",
    "print(\"\\n\".join(exclude_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare main data\n",
    "main_data = crop_main_data_landsat(items, abs_client, s3_client, lat, lon, crop_size, main_idx=stac_idx)\n",
    "main_item = main_data[\"tile_item\"]\n",
    "print(f\"Main tile      {main_item.id}\")\n",
    "\n",
    "# Use reference images before/after the main date\n",
    "max_days_difference = 60  # 75\n",
    "reference_data_ = []\n",
    "for reference in reference_data:\n",
    "    tile_id = reference[\"tile_item\"].id\n",
    "    ref_date = datetime.strptime(tile_id.split(\"_\")[3], \"%Y%m%d\")\n",
    "    ref_date_short = ref_date.isoformat().split(\"T\")[0]\n",
    "    if ref_date_short in exclude_dates:\n",
    "        print(f\"Reference img on {ref_date_short} is excluded         --> DONT USE\")\n",
    "        continue\n",
    "\n",
    "    # Calculate the absolute difference in time\n",
    "    time_difference = abs(target_date.date() - ref_date.date())\n",
    "\n",
    "    # Check if the difference in days is within the threshold\n",
    "    # timedelta.days gives the difference purely in days (ignoring hours etc.)\n",
    "    if time_difference.days <= max_days_difference and time_difference.days > 0:\n",
    "        reference_data_.append(reference)\n",
    "        print(\n",
    "            f'Reference img on {ref_date.isoformat().split(\"T\")[0]} is '\n",
    "            f'{time_difference.days:3} days distant -->      USE'\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f'Reference img on {ref_date.isoformat().split(\"T\")[0]} is '\n",
    "            f'{time_difference.days:3} days distant --> DONT USE'\n",
    "        )\n",
    "len(reference_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Wind for Main Date\n",
    "sensing_time = main_item.time  # .isoformat()\n",
    "print(sensing_time)\n",
    "wind_components = get_wind_components(wind_data, sensing_time)\n",
    "\n",
    "u_wind_component, v_wind_component = wind_components[\"geos\"]\n",
    "wind_speed_geos = calc_effective_wind_speed(u_wind_component, v_wind_component)\n",
    "wind_direction_geos = calc_wind_direction(u_wind_component, v_wind_component)\n",
    "\n",
    "u_wind_component, v_wind_component = wind_components[\"era5\"]\n",
    "wind_speed_era5 = calc_effective_wind_speed(u_wind_component, v_wind_component)\n",
    "wind_direction_era5 = calc_wind_direction(u_wind_component, v_wind_component)\n",
    "\n",
    "print(\"GEOS Wind\")\n",
    "plot_wind(\n",
    "    lon,\n",
    "    lat,\n",
    "    main_item,\n",
    "    abs_client,\n",
    "    wind_speed_geos,\n",
    "    wind_direction_geos,\n",
    "    satellite_id=SatelliteID.LANDSAT,\n",
    "    arrow_scale_factor=500,\n",
    ")\n",
    "print(\"ERA5 Wind\")\n",
    "plot_wind(\n",
    "    lon,\n",
    "    lat,\n",
    "    main_item,\n",
    "    abs_client,\n",
    "    wind_speed_era5,\n",
    "    wind_direction_era5,\n",
    "    satellite_id=SatelliteID.LANDSAT,\n",
    "    arrow_scale_factor=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Before/Earlier dates for \"Normal\" prediction\n",
    "print(f\"Main    date = {main_item.date}\")\n",
    "before_date = None\n",
    "earlier_date = None\n",
    "for reference in reference_data:\n",
    "    tile_id = reference[\"tile_item\"].id\n",
    "    ref_date = datetime.strptime(tile_id.split(\"_\")[3], \"%Y%m%d\")\n",
    "    ref_date_short = ref_date.isoformat().split(\"T\")[0]\n",
    "    # Calculate the absolute difference in time\n",
    "    time_difference = target_date.date() - ref_date.date()\n",
    "    if time_difference.days <= 0:\n",
    "        continue\n",
    "    if ref_date_short in exclude_dates:\n",
    "        print(f\"Reference img on {ref_date_short} is excluded         --> DONT USE\")\n",
    "        continue\n",
    "    if before_date is None:  # Closest is Before (t-1)\n",
    "        before_date = ref_date.isoformat().split(\"T\")[0]\n",
    "    elif earlier_date is None:  # Next closest is Earlier (t-2)\n",
    "        earlier_date = ref_date.isoformat().split(\"T\")[0]\n",
    "        break\n",
    "\n",
    "# # Optionally overwrite them\n",
    "# before_date = \"2025-01-19\"\n",
    "# earlier_date = \"2025-01-18\"\n",
    "\n",
    "print(f\"Before  date = {before_date}\")\n",
    "print(f\"Earlier date = {earlier_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = crop_size // 2, crop_size // 2\n",
    "\n",
    "half_crop = 64  # we want middle 128x128\n",
    "xmin = row - half_crop\n",
    "xmax = row + half_crop\n",
    "ymin = col - half_crop\n",
    "ymax = col + half_crop\n",
    "extent = [xmin, xmax, ymin, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reference_days_diffs = [40, 25, 15]\n",
    "\n",
    "# set up UI\n",
    "widget_outputs = [widgets.Output(layout={\"height\": \"1000px\", \"width\": \"1200px\"}) for _ in model_ids]\n",
    "for output in widget_outputs:\n",
    "    with output:\n",
    "        print(\"Loading...\")\n",
    "tabs = widgets.Tab(children=widget_outputs, titles=model_ids)\n",
    "display(tabs)\n",
    "\n",
    "# Visualize predictions using all reference image combinations as inputs\n",
    "output_preds = {}\n",
    "dates = [d[\"tile_item\"].time for d in reference_data_]\n",
    "for model_idx, (model_id, output) in enumerate(zip(model_ids, widget_outputs, strict=True)):\n",
    "    main_date = target_date.isoformat().split(\"T\")[0]\n",
    "    avg_preds = {}\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        for avg_reference_days_diff in avg_reference_days_diffs:\n",
    "            # Use reference images before/after the main date\n",
    "            for timeframe in [\"before/after\", \"before\"]:\n",
    "                if timeframe == \"before\":\n",
    "                    avg_reference_days_diff_ = int(avg_reference_days_diff * 2)\n",
    "                    reference_data_for_avg = get_reference_data_before(\n",
    "                        reference_data, target_date, max_days_difference=avg_reference_days_diff_\n",
    "                    )\n",
    "                else:\n",
    "                    avg_reference_days_diff_ = avg_reference_days_diff\n",
    "                    reference_data_for_avg = get_reference_data(\n",
    "                        reference_data, target_date, max_days_difference=avg_reference_days_diff_\n",
    "                    )\n",
    "                # Data preparation for predicting with average last 10 reference images\n",
    "                valid_refs = [ref for ref in reference_data_for_avg if ref[\"tile_item\"].time not in exclude_dates]\n",
    "                print(\n",
    "                    f\"Ref images {avg_reference_days_diff_} {timeframe}: \"\n",
    "                    f\"Filtered {len(reference_data_for_avg)} refs to \"\n",
    "                    f\"{len(valid_refs)} after excluding dates\"\n",
    "                )\n",
    "                avg_ref_count = len(valid_refs)\n",
    "                if avg_ref_count == 0:\n",
    "                    continue\n",
    "                data_avg = copy.copy(reference_data_for_avg[0])\n",
    "                data_avg[\"crop_arrays\"] = np.mean([ref[\"crop_arrays\"] for ref in valid_refs], axis=0)\n",
    "                center_y = data_avg[\"crop_arrays\"][:, xmin:xmax, ymin:ymax].shape[1] // 2\n",
    "                center_x = data_avg[\"crop_arrays\"][:, xmin:xmax, ymin:ymax].shape[2] // 2\n",
    "\n",
    "                f, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "                preds_avg = predict(\n",
    "                    main_data,\n",
    "                    [data_avg, data_avg],\n",
    "                    watershed_params,\n",
    "                    models[model_idx],\n",
    "                    device,\n",
    "                    band_concatenators[model_idx],\n",
    "                    lossFn,\n",
    "                )\n",
    "                ratio_main = get_band_ratio_landsat(preds_avg.x_dict[\"crop_main\"][0], BANDS_LANDSAT)[\n",
    "                    xmin:xmax, ymin:ymax\n",
    "                ]  # type:ignore\n",
    "                vmin = np.percentile(ratio_main, 0.5)\n",
    "                vmax = np.percentile(ratio_main, 99.5)\n",
    "\n",
    "                ratio_before = get_band_ratio_landsat(preds_avg.x_dict[\"crop_before\"][0], BANDS_LANDSAT)[\n",
    "                    xmin:xmax, ymin:ymax\n",
    "                ]  # type:ignore\n",
    "                ratio_earlier = get_band_ratio_landsat(preds_avg.x_dict[\"crop_earlier\"][0], BANDS_LANDSAT)[\n",
    "                    xmin:xmax, ymin:ymax\n",
    "                ]  # type:ignore\n",
    "\n",
    "                ax[0].imshow(ratio_earlier, vmin=vmin, vmax=vmax)\n",
    "                ax[0].set_title(\n",
    "                    f\"Avg({avg_reference_days_diff_} days {timeframe} main) = {avg_ref_count} refs\\n\"\n",
    "                    f\"Min {ratio_before.min():.2f} Max {ratio_before.max():.2f} \"\n",
    "                    f\"Mean {ratio_earlier.mean():.3f}\",\n",
    "                    fontsize=15,\n",
    "                )\n",
    "                ax[1].imshow(ratio_before, vmin=vmin, vmax=vmax)\n",
    "                ax[1].set_title(\n",
    "                    f\"Avg({avg_reference_days_diff_} days {timeframe} main) = {avg_ref_count} refs\\n\"\n",
    "                    f\"Min {ratio_before.min():.2f} Max {ratio_before.max():.2f} \"\n",
    "                    f\"Mean {ratio_before.mean():.3f}\",\n",
    "                    fontsize=15,\n",
    "                )\n",
    "                ax[2].imshow(ratio_main, vmin=vmin, vmax=vmax)\n",
    "                ax[2].set_title(\n",
    "                    f\"Main Ratio {main_date}\\nMin {ratio_main.min():.2f} Max {ratio_main.max():.2f} \"\n",
    "                    f\"Mean {ratio_main.mean():.3f}\",\n",
    "                    fontsize=15,\n",
    "                )\n",
    "                ax[3].imshow(preds_avg.binary_probability[xmin:xmax, ymin:ymax], vmin=0.0, vmax=1.0, cmap=\"hot_r\")\n",
    "\n",
    "                center = get_center_buffer(preds_avg.binary_probability, center_buffer)\n",
    "                avg_preds[f\"{avg_reference_days_diff_}_{timeframe}\"] = {\n",
    "                    \"avg_ref_count\": avg_ref_count,\n",
    "                    \"max_prob\": 100 * center.max(),\n",
    "                    \"sum_prob\": 100 * center.sum() / (center.shape[0] * center.shape[1]),\n",
    "                }\n",
    "                ax[3].set_title(\n",
    "                    f\"Avg({avg_reference_days_diff_} days {timeframe} main)\\nCenter sum(Prob): \"\n",
    "                    f\"{100 * center.sum() / (center.shape[0] * center.shape[1]):.1f}%, \"\n",
    "                    f\"Max: {100 * center.max():.0f}%\",\n",
    "                    fontsize=16,\n",
    "                )\n",
    "                ax[0].scatter(center_x, center_y, color=\"green\", marker=\"x\")\n",
    "                ax[1].scatter(center_x, center_y, color=\"green\", marker=\"x\")\n",
    "                ax[2].scatter(center_x, center_y, color=\"green\", marker=\"x\")\n",
    "                ax[3].scatter(center_x, center_y, color=\"green\", marker=\"x\")\n",
    "                grid16(ax[0])\n",
    "                grid16(ax[1])\n",
    "                grid16(ax[2])\n",
    "                grid16(ax[3])\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "        print(f\"Model {model_id}\")\n",
    "        for k, v in avg_preds.items():\n",
    "            print(\n",
    "                f\"Avg{k:15}   Center sum(Prob): {v['sum_prob']:4.1f}%, \"\n",
    "                f\"Max: {v['max_prob']:3.0f}% (using avg of {v['avg_ref_count']} ref images)\"\n",
    "            )\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        predictions = predict_for_all_pairs(\n",
    "            main_data,\n",
    "            reference_data_,\n",
    "            models[model_idx],\n",
    "            device,\n",
    "            band_concatenators[model_idx],\n",
    "            lossFn,\n",
    "            watershed_params,\n",
    "            skip_retrieval=True,\n",
    "        )\n",
    "\n",
    "        print(f\"Model {model_id}\")\n",
    "        output_preds[model_id] = plot_max_proba_center_buffer_heatmap(\n",
    "            predictions=predictions,\n",
    "            dates=dates,\n",
    "            center_buffer=center_buffer,\n",
    "            title=f'Model {model_id} for main={target_date.isoformat().split(\"T\")[0]}',\n",
    "            show_topk=6,\n",
    "            topk_based_on=\"marg_sum\",  # \"sum\", #\"max\",  # or \"sum\" to use the sum of center probabilities\n",
    "            satellite_id=SatelliteID.LANDSAT,\n",
    "            main_date=target_date.isoformat().split(\"T\")[0],\n",
    "            before_date=before_date,\n",
    "            earlier_date=earlier_date,\n",
    "            plot_topk=True,  # True,\n",
    "            plot_max_grid=True,  # True,\n",
    "            plot_binary_grid=False,\n",
    "            dates_to_exclude=[],\n",
    "            extent=extent,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "1. Validate and visualize the plumes and emission rates of \"normal\" predictions, topk predictions or any other combination. Build the combination list `combinations_to_visualize` you want to test and call `validate_pred_retrievals(combinations_to_visualize)`.\n",
    "2. Manually select the combination of model id + before/earlier dates we will use for L/IME/Emission Rate Calculation + Unmasked Retrieval and plume mask .tif outputs. Put this into the decision_dict for the main date, e.g. `'selected_retrieval': {'model_id': '37', 'date_before': '2024-12-25', 'date_earlier': '2024-12-18'}`\n",
    "    1. Set `feasible` to True if we are able to predict for this main date, i.e. its not cloudy/cloud shadowy\n",
    "    2. Set `note` to \"no_detection\" or \"detection\" and any other notes you have.\n",
    "    3. Set `watershed_marker_t` and `watershed_floor_t` to your selected watershed parameters if you want different parameters from the default `watershed_marker_t=0.2`,`watershed_floor_t=0.15`\n",
    "    4. Set `selected_retrieval` as described above\n",
    "    5. If you have a detection: Set `wind_source` to \"era5\" or \"geos\"\n",
    "    6. If you have a detection: Set `emission_ensemble_selections` to a list of model id + before/earlier dates whose emission rates will be used to calculate our emission rate uncertainty. __Don't put the selected retrieval in here.__\n",
    "4. Run `sbr_form_outputs()` to get the values ready for copying into the form fillout. This will also export the needed unmasked retrieval and the binary plume mask .tifs from your `selected_retrieval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once to initialize the RadTranLookup for the main date\n",
    "granule_item = main_data[\"tile_item\"]\n",
    "if isinstance(granule_item, LandsatGranuleAccess):\n",
    "    hapi_data_path = LANDSAT_HAPI_DATA_PATH\n",
    "elif isinstance(granule_item, Sentinel2Item):\n",
    "    hapi_data_path = S2_HAPI_DATA_PATH\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported granule access type: {type(granule_item)}\")\n",
    "lookup_table = RadTranLookupTable.from_params(\n",
    "    instrument=granule_item.instrument,\n",
    "    solar_angle=granule_item.solar_angle,\n",
    "    observation_angle=granule_item.observation_angle,\n",
    "    hapi_data_path=hapi_data_path,\n",
    "    min_ch4=0.0,\n",
    "    max_ch4=21.0,  # this value was selected based on the common value ranges of the sim plume datasets\n",
    "    spacing_resolution=40000,\n",
    "    ref_band=granule_item.swir16_band_name,\n",
    "    band=granule_item.swir22_band_name,\n",
    "    full_sensor_name=granule_item.sensor_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the combinations you want to validate/visualize\n",
    "combinations_to_visualize = []\n",
    "for model_id, v in output_preds.items():\n",
    "    # if model_id in [\"55\"]: # skip certain models\n",
    "    #     continue\n",
    "    for j in range(1, 4):  # 6): # use topX dates\n",
    "        combinations_to_visualize.append(\n",
    "            {\n",
    "                \"model_id\": model_id,\n",
    "                \"date_before\": v[f\"top_{j}\"][\"date_before\"],\n",
    "                \"date_earlier\": v[f\"top_{j}\"][\"date_earlier\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # visualize the \"normal\" predictions\n",
    "    combinations_to_visualize.append(\n",
    "        {\"model_id\": model_id, \"date_before\": v[\"normal\"][\"date_before\"], \"date_earlier\": v[\"normal\"][\"date_earlier\"]}\n",
    "    )\n",
    "\n",
    "# format output for copy/paste\n",
    "for d in combinations_to_visualize:\n",
    "    print(12 * \" \" + f\"{json.dumps(d)},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_speed_era5, wind_speed_geos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "- Model 154 for B=2025-01-19 and E=2025-01-11: L= 454.0, IME= 2824.3, Emission Rate:  395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_params = WatershedParameters(\n",
    "    marker_distance=1,\n",
    "    marker_threshold=0.1,\n",
    "    watershed_floor_threshold=0.075,\n",
    "    closing_footprint_size=0,\n",
    ")\n",
    "max_distance_pixels = 10\n",
    "pixel_width = 30\n",
    "wind_speed = wind_speed_geos  # (wind_speed_era5 + wind_speed_geos) / 2  # or wind_speed_geos\n",
    "# print(\"[WARN] using average windspeed\")\n",
    "\n",
    "validate_pred_retrievals(\n",
    "    combinations_to_visualize,\n",
    "    main_data,\n",
    "    reference_data,\n",
    "    model_ids,\n",
    "    watershed_params,\n",
    "    models,\n",
    "    device,\n",
    "    band_concatenators,\n",
    "    lossFn,\n",
    "    lookup_table,\n",
    "    wind_speed,\n",
    "    SatelliteID.LANDSAT,\n",
    "    max_distance_pixels,\n",
    "    pixel_width,\n",
    "    show_plots=False,\n",
    "    extent=extent,\n",
    ")\n",
    "validate_pred_retrievals(\n",
    "    combinations_to_visualize,\n",
    "    main_data,\n",
    "    reference_data,\n",
    "    model_ids,\n",
    "    watershed_params,\n",
    "    models,\n",
    "    device,\n",
    "    band_concatenators,\n",
    "    lossFn,\n",
    "    lookup_table,\n",
    "    wind_speed,\n",
    "    SatelliteID.LANDSAT,\n",
    "    max_distance_pixels,\n",
    "    pixel_width,\n",
    "    show_plots=True,\n",
    "    extent=extent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./landsat_phase1_decision_dict.json\") as fs:\n",
    "    decisions_dict = json.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbr_form_outputs(\n",
    "    decisions_dict[\"2025-02-19\"],\n",
    "    wind_speed_geos,\n",
    "    wind_direction_geos,\n",
    "    wind_speed_era5,\n",
    "    wind_direction_era5,\n",
    "    model_ids,\n",
    "    models,\n",
    "    device,\n",
    "    band_concatenators,\n",
    "    main_data,\n",
    "    reference_data,\n",
    "    lossFn,\n",
    "    lookup_table,\n",
    "    max_distance_pixels,\n",
    "    pixel_width,\n",
    "    main_item,\n",
    "    target_date,\n",
    "    SatelliteID.LANDSAT,\n",
    "    abs_client,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
